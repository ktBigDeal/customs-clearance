"""
Similar Law Retriever Module

ê´€ì„¸ë²• ë¬¸ì„œì—ì„œ ìœ ì‚¬í•œ ì¡°ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë‚´ë¶€ ì°¸ì¡°ë¥¼ í™œìš©í•œ í™•ì¥ ê²€ìƒ‰ ê¸°ëŠ¥
"""

import logging
from typing import List, Dict, Any, Optional, Set, Tuple
import re
from .embeddings import LangChainEmbedder
from .vector_store import ChromaVectorStore
from .query_normalizer import QueryNormalizer, AdvancedQueryProcessor

logger = logging.getLogger(__name__)


class SimilarLawRetriever:
    """ìœ ì‚¬í•œ ë²•ë¥  ì¡°ë¬¸ ê²€ìƒ‰ ë° ë‚´ë¶€ ì°¸ì¡° ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self,
                 embedder: LangChainEmbedder,
                 vector_store: ChromaVectorStore,
                 query_normalizer: QueryNormalizer):
        """
        ì´ˆê¸°í™”
        
        Args:
            embedder (LangChainEmbedder): ì„ë² ë”© ìƒì„±ê¸°
            vector_store (ChromaVectorStore): ë²¡í„° ì €ì¥ì†Œ
            query_normalizer (QueryNormalizer): ì¿¼ë¦¬ ì •ê·œí™”ê¸°
        """
        self.embedder = embedder
        self.vector_store = vector_store
        self.query_normalizer = query_normalizer
        self.query_processor = AdvancedQueryProcessor(query_normalizer)
        
        # ë‚´ë¶€ ë¬¸ì„œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._document_cache = {}
        
        logger.info("SimilarLawRetriever initialized")
    
    def search_similar_laws(self, 
                           raw_query: str, 
                           top_k: int = 5,
                           include_references: bool = True,
                           expand_with_synonyms: bool = True,
                           similarity_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ ë²•ë¥  ì¡°ë¬¸ ê²€ìƒ‰
        
        Args:
            raw_query (str): ì›ë³¸ ì‚¬ìš©ì ì§ˆì˜
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            include_references (bool): ë‚´ë¶€ ì°¸ì¡° ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            expand_with_synonyms (bool): ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. ë³µí•© ì¿¼ë¦¬ ì²˜ë¦¬
            processed_query = self.query_processor.process_complex_query(raw_query)
            
            # 2. ì‚¬ìš©í•  ì¿¼ë¦¬ ê²°ì •
            if expand_with_synonyms:
                search_query = processed_query["expanded_query"]
            else:
                search_query = processed_query["normalized_query"]
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            # 3. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedder.embed_text(search_query)
            
            # 4. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            logger.info(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘ (top_k: {top_k})")
            primary_results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k
            )
            logger.info(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(primary_results)}ê°œ")
            
            # 5. ë‚´ë¶€ ì°¸ì¡° í™•ì¥ ê²€ìƒ‰
            if include_references:
                expanded_results = self._expand_with_references(primary_results, top_k)
            else:
                expanded_results = primary_results
            
            # 6. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì •ë ¬
            final_results = self._post_process_results(expanded_results, processed_query)
            
            # 7. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            if similarity_threshold > 0.0:
                filtered_results = [result for result in final_results 
                                  if result.get("similarity", 0) >= similarity_threshold]
                logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold}ë¡œ í•„í„°ë§: {len(final_results)} â†’ {len(filtered_results)}ê°œ")
                final_results = filtered_results
            
            logger.info(f"âœ… {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìš”ì²­ëœ top_k: {top_k})")
            return final_results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_article_reference(self, article_ref: str) -> Optional[Dict[str, Any]]:
        """
        ì¡°ë¬¸ ì°¸ì¡°ë¥¼ í†µí•œ ì§ì ‘ ê²€ìƒ‰
        
        Args:
            article_ref (str): ì¡°ë¬¸ ì°¸ì¡° (ì˜ˆ: "ì œ1ì¡°", "ì œ5ì¡°ì œ2í•­")
            
        Returns:
            Optional[Dict[str, Any]]: í•´ë‹¹ ì¡°ë¬¸ ì •ë³´
        """
        try:
            # ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰ (where ì ˆ ì—†ì´)
            results = self.vector_store.search_similar(
                query_embedding=[0.0] * self.embedder.embedding_dim,  # ë”ë¯¸ ì„ë² ë”©
                top_k=200  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            )
            
            # ì •í™•í•œ ë§¤ì¹˜ ë˜ëŠ” ë¶€ë¶„ ë§¤ì¹˜ ì°¾ê¸°
            exact_matches = []
            partial_matches = []
            
            for result in results:
                result_index = result.get("index", "") or result.get("metadata", {}).get("index", "")
                
                if result_index == article_ref:
                    # ì •í™•í•œ ë§¤ì¹˜
                    exact_matches.append(result)
                elif article_ref in result_index or result_index in article_ref:
                    # ë¶€ë¶„ ë§¤ì¹˜ (ì˜ˆ: "ì œ1ì¡°" ê²€ìƒ‰ ì‹œ "ì œ1ì¡°ì œ1í•­" ë§¤ì¹˜)
                    partial_matches.append(result)
            
            # ì •í™•í•œ ë§¤ì¹˜ ìš°ì„  ë°˜í™˜
            if exact_matches:
                return exact_matches[0]
            elif partial_matches:
                return partial_matches[0]
            
            return None
            
        except Exception as e:
            logger.error(f"ì¡°ë¬¸ ì°¸ì¡° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def get_related_articles(self, document: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ë¬¸ì„œì™€ ê´€ë ¨ëœ ì¡°ë¬¸ë“¤ ì¡°íšŒ (ë‚´ë¶€ ì°¸ì¡° ê¸°ë°˜)
        
        Args:
            document (Dict[str, Any]): ê¸°ì¤€ ë¬¸ì„œ
            
        Returns:
            List[Dict[str, Any]]: ê´€ë ¨ ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        related_articles = []
        
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚´ë¶€ ì°¸ì¡° ì •ë³´ ì¶”ì¶œ
            metadata = document.get("metadata", {})
            
            # JSON ë¬¸ìì—´ë¡œ ì €ì¥ëœ ë‚´ë¶€ ì°¸ì¡° ì •ë³´ íŒŒì‹±
            internal_refs_raw = metadata.get("internal_law_references", "{}")
            if isinstance(internal_refs_raw, str):
                import json
                internal_refs = json.loads(internal_refs_raw)
            else:
                internal_refs = internal_refs_raw
            
            # ê° ì°¸ì¡° ìœ í˜•ë³„ë¡œ ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰
            for ref_type, ref_list in internal_refs.items():
                if not ref_list:
                    continue
                
                for ref in ref_list:  # ëª¨ë“  ì°¸ì¡° ì²˜ë¦¬
                    related_doc = self.search_by_article_reference(ref)
                    if related_doc and related_doc not in related_articles:
                        related_doc["reference_type"] = ref_type
                        related_articles.append(related_doc)
            
            logger.debug(f"ë°œê²¬ëœ ê´€ë ¨ ì¡°ë¬¸: {len(related_articles)}ê°œ")
            return related_articles
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ì¡°ë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def search_with_context_expansion(self, 
                                    query: str, 
                                    context_documents: List[Dict[str, Any]],
                                    top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ í™•ì¥ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            context_documents (List[Dict[str, Any]]): ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤
            top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # 1. ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
            basic_results = self.search_similar_laws(query, top_k)
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ ì¡°ë¬¸ë“¤ ìˆ˜ì§‘
            context_related = []
            for context_doc in context_documents:
                related = self.get_related_articles(context_doc)
                context_related.extend(related)
            
            # 3. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
            all_results = basic_results + context_related
            unique_results = self._remove_duplicates(all_results)
            
            # 4. ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±ì— ë”°ë¥¸ ì ìˆ˜ ë¶€ì—¬
            scored_results = self._score_with_context(unique_results, context_documents)
            
            # 5. ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            sorted_results = sorted(scored_results, 
                                  key=lambda x: x.get("context_score", 0), 
                                  reverse=True)
            
            return sorted_results[:top_k]
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self.search_similar_laws(query, top_k)
    
    def _expand_with_references(self, 
                              primary_results: List[Dict[str, Any]], 
                              max_total: int) -> List[Dict[str, Any]]:
        """
        ë‚´ë¶€ ì°¸ì¡°ë¥¼ í™œìš©í•œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥
        
        Args:
            primary_results (List[Dict[str, Any]]): ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼
            max_total (int): ìµœëŒ€ ë°˜í™˜ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: í™•ì¥ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        expanded_results = primary_results.copy()
        seen_ids = {result["id"] for result in primary_results}
        
        # ìƒìœ„ ê²°ê³¼ë“¤ì˜ ë‚´ë¶€ ì°¸ì¡° ë”°ë¼ê°€ê¸° (ëª¨ë“  ê²°ê³¼ í™•ì¥)
        for result in primary_results:
            try:
                related_articles = self.get_related_articles(result)
                
                for related in related_articles:
                    if related["id"] not in seen_ids and len(expanded_results) < max_total:
                        # ì°¸ì¡° ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€
                        related["reference_boost"] = 0.1
                        related["referenced_from"] = result["id"]
                        expanded_results.append(related)
                        seen_ids.add(related["id"])
                        
            except Exception as e:
                logger.warning(f"ì°¸ì¡° í™•ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return expanded_results
    
    def _post_process_results(self, 
                            results: List[Dict[str, Any]], 
                            processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ë° ìµœì í™”
        
        Args:
            results (List[Dict[str, Any]]): ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼
            processed_query (Dict[str, Any]): ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            List[Dict[str, Any]]: í›„ì²˜ë¦¬ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        # 1. ì ìˆ˜ ì¬ê³„ì‚°
        for result in results:
            base_score = result.get("similarity", 0)
            
            # ì°¸ì¡° ë¶€ìŠ¤íŠ¸ ì ìš©
            reference_boost = result.get("reference_boost", 0)
            
            # ì˜ë„ ë§¤ì¹­ ì ìˆ˜
            intent_score = self._calculate_intent_score(result, processed_query["intent"])
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = base_score + reference_boost + intent_score * 0.1
            result["final_score"] = final_score
        
        # 2. ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x.get("final_score", 0), reverse=True)
        
        # 3. ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in sorted_results:
            # ë‹¤ì¤‘ ê²½ë¡œì—ì„œ indexì™€ subtitle ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: ìµœìƒìœ„ > metadata > ê¸°ë³¸ê°’)
            index = (result.get("index") or 
                    result.get("metadata", {}).get("index") or "")
            subtitle = (result.get("subtitle") or 
                       result.get("metadata", {}).get("subtitle") or "")
            
            formatted_result = {
                "id": result["id"],
                "content": result["content"],
                "metadata": result["metadata"],
                "index": index,  # ìµœìƒìœ„ ë ˆë²¨ë¡œ ì¶”ê°€
                "subtitle": subtitle,  # ìµœìƒìœ„ ë ˆë²¨ë¡œ ì¶”ê°€
                "similarity": result.get("similarity", 0),
                "final_score": result.get("final_score", 0),
                "reference_info": {
                    "is_referenced": result.get("reference_boost", 0) > 0,
                    "referenced_from": result.get("referenced_from"),
                    "reference_type": result.get("reference_type")
                }
            }
            formatted_results.append(formatted_result)
        
        return formatted_results
    
    def _calculate_intent_score(self, result: Dict[str, Any], intent: Dict[str, Any]) -> float:
        """
        ì˜ë„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        
        Args:
            result (Dict[str, Any]): ê²€ìƒ‰ ê²°ê³¼
            intent (Dict[str, Any]): ì¶”ì¶œëœ ì˜ë„
            
        Returns:
            float: ì˜ë„ ë§¤ì¹­ ì ìˆ˜
        """
        score = 0.0
        content = result.get("content", "").lower()
        
        # í•µì‹¬ ê°œë… ë§¤ì¹­
        key_concepts = intent.get("key_concepts", [])
        for concept in key_concepts:
            if concept.lower() in content:
                score += 0.2
        
        # ë²•ë ¹ ì˜ì—­ ë§¤ì¹­
        law_area = intent.get("law_area", "")
        if law_area != "ì¼ë°˜":
            area_keywords = {
                "ìˆ˜ì…": ["ìˆ˜ì…", "ë°˜ì…", "ë“¤ì—¬ì˜¤"],
                "ìˆ˜ì¶œ": ["ìˆ˜ì¶œ", "ë°˜ì¶œ", "ë‚´ë³´ë‚´"],
                "í†µê´€": ["í†µê´€", "ì„¸ê´€"],
                "ê´€ì„¸": ["ê´€ì„¸", "ì„¸ê¸ˆ", "ë¶€ê³¼"],
                "ê²€ì‚¬": ["ê²€ì‚¬", "ê²€ì¦", "í™•ì¸"]
            }
            
            if law_area in area_keywords:
                for keyword in area_keywords[law_area]:
                    if keyword in content:
                        score += 0.3
                        break
        
        return min(score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì¤‘ë³µ ê²°ê³¼ ì œê±°
        
        Args:
            results (List[Dict[str, Any]]): ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict[str, Any]]: ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼
        """
        seen_ids = set()
        unique_results = []
        
        for result in results:
            result_id = result.get("id")
            if result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)
        
        return unique_results
    
    def _score_with_context(self, 
                          results: List[Dict[str, Any]], 
                          context_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤í•œ ì ìˆ˜ ë¶€ì—¬
        
        Args:
            results (List[Dict[str, Any]]): ê²€ìƒ‰ ê²°ê³¼
            context_docs (List[Dict[str, Any]]): ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤
            
        Returns:
            List[Dict[str, Any]]: ì ìˆ˜ê°€ ë¶€ì—¬ëœ ê²°ê³¼
        """
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        context_keywords = set()
        for doc in context_docs:
            content = doc.get("content", "")
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥)
            words = re.findall(r'\w+', content)
            context_keywords.update(words[:10])  # ìƒìœ„ 10ê°œ ë‹¨ì–´
        
        # ê° ê²°ê³¼ì— ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜ ë¶€ì—¬
        for result in results:
            content = result.get("content", "")
            words = set(re.findall(r'\w+', content))
            
            # ì»¨í…ìŠ¤íŠ¸ì™€ì˜ í‚¤ì›Œë“œ overlap ê³„ì‚°
            overlap = len(context_keywords & words)
            context_score = overlap / max(len(context_keywords), 1)
            
            result["context_score"] = context_score
        
        return results