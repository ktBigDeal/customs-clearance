"""
Trade Information Retriever Module

ë¬´ì—­ ì •ë³´ CSV ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  HSì½”ë“œ, êµ­ê°€ë³„ ê·œì œ ë“±ì„ í™œìš©í•œ í™•ì¥ ê²€ìƒ‰ ê¸°ëŠ¥
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
import re
from .embeddings import LangChainEmbedder
from .vector_store import LangChainVectorStore
from .query_normalizer import QueryNormalizer, AdvancedQueryProcessor, TradeQueryNormalizer

logger = logging.getLogger(__name__)


class TradeInfoRetriever:
    """ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ ë° ê´€ë ¨ ë°ì´í„° ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self,
                 embedder: Optional[LangChainEmbedder] = None,
                 vector_store: Optional[LangChainVectorStore] = None,
                 query_normalizer: Optional[QueryNormalizer] = None,
                 collection_name: str = "trade_info_collection"):
        """
        ì´ˆê¸°í™” (LangChain í‘œì¤€ ì‚¬ìš©)
        
        Args:
            embedder (Optional[LangChainEmbedder]): LangChain ì„ë² ë”© ìƒì„±ê¸°
            vector_store (Optional[LangChainVectorStore]): LangChain ë²¡í„° ì €ì¥ì†Œ
            query_normalizer (Optional[QueryNormalizer]): ì¿¼ë¦¬ ì •ê·œí™”ê¸°
            collection_name (str): ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„
        """
        # ê¸°ë³¸ê°’ ì„¤ì • (ìë™ ì´ˆê¸°í™”)
        if embedder is None:
            self.embedder = LangChainEmbedder()  # ê¸°ë³¸ text-embedding-3-small ì‚¬ìš©
        else:
            self.embedder = embedder
            
        if vector_store is None:
            self.vector_store = LangChainVectorStore(
                collection_name=collection_name,
                embedding_function=self.embedder.embeddings
            )
        else:
            self.vector_store = vector_store
            
        if query_normalizer is None:
            # TradeQueryNormalizer ì‚¬ìš©ìœ¼ë¡œ ë™ì‹ë¬¼ ìˆ˜ì… ì˜ë„ ê°ì§€ ê°œì„ 
            self.query_normalizer = TradeQueryNormalizer()
        else:
            self.query_normalizer = query_normalizer
            
        self.query_processor = AdvancedQueryProcessor(self.query_normalizer)
        self.collection_name = collection_name
        
        # ë‚´ë¶€ ë¬¸ì„œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._document_cache = {}
        
        # ğŸš€ LLM ë¶„ë¥˜ ê²°ê³¼ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)
        self._llm_classification_cache = {}  # query â†’ classification ê²°ê³¼
        self._cache_max_size = 100  # ìµœëŒ€ ìºì‹œ í¬ê¸°
        self._cache_ttl = 3600  # 1ì‹œê°„ TTL
        
        # HSì½”ë“œ íŒ¨í„´ ë§¤ì¹­ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        self.hs_code_pattern = re.compile(r'\b\d{4,10}\b')  # 4-10ìë¦¬ ìˆ«ìë¥¼ HSì½”ë“œë¡œ ì¸ì‹
        
        logger.info(f"LangChain TradeInfoRetriever initialized with collection: {collection_name}")
    
    def search_trade_info(self, 
                         raw_query: str, 
                         top_k: int = 5,
                         include_related: bool = True,
                         expand_with_synonyms: bool = True,
                         similarity_threshold: float = 0.0,
                         filter_by: Optional[Dict[str, Any]] = None,
                         search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ë¬´ì—­ ì •ë³´ ê²€ìƒ‰
        
        Args:
            raw_query (str): ì›ë³¸ ì‚¬ìš©ì ì§ˆì˜
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            include_related (bool): ê´€ë ¨ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            expand_with_synonyms (bool): ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
            filter_by (Optional[Dict[str, Any]]): í•„í„°ë§ ì¡°ê±´
            search_context (Optional[Dict[str, Any]]): ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ğŸ¯ ì…ë ¥ ì¿¼ë¦¬ ë¡œê¹… ë° ì •ê·œí™”
            logger.info(f"\nğŸ” ===== ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ ì‹œì‘ ===== ")
            logger.info(f"ğŸ“ ì›ë³¸ ì¿¼ë¦¬: '{raw_query}'")
            logger.info(f"âš™ï¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: top_k={top_k}, threshold={similarity_threshold}")
            if filter_by:
                logger.info(f"ğŸ¯ ì‚¬ìš©ì ì œê³µ í•„í„°: {filter_by}")
            if search_context:
                logger.info(f"ğŸ¨ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸: {search_context}")
            
            # 1. LLM ê¸°ë°˜ ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± (í•µì‹¬ ê°œì„  ê¸°ëŠ¥)
            logger.info(f"ğŸ¤– LLM ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± ì¤‘...")
            llm_filters = self._generate_llm_metadata_filters(raw_query, search_context)
            
            # 2. ê¸°ì¡´ í•„í„°ì™€ LLM í•„í„° ë³‘í•©
            combined_filters = self._merge_filters(filter_by, llm_filters)
            
            # ë³‘í•©ëœ í•„í„°ë¥¼ ì‹¤ì œ ê²€ìƒ‰ì— ì‚¬ìš©
            if combined_filters:
                if not filter_by:
                    filter_by = {}
                filter_by.update(combined_filters)
            
            # 3. ì¿¼ë¦¬ ì •ê·œí™” ë° í™•ì¥ (TradeQueryNormalizer ì‚¬ìš©)
            normalized_query = self.query_normalizer.normalize(raw_query, search_context)
            if expand_with_synonyms:
                expanded_query = self.query_normalizer.expand_query_with_synonyms(normalized_query)
            else:
                expanded_query = normalized_query
            
            logger.info(f"ğŸ” Query: '{raw_query}' â†’ '{expanded_query}'")
            logger.info(f"ğŸ§  LLM í•„í„° ê²°ê³¼: {llm_filters}")
            logger.info(f"ğŸ¯ ìµœì¢… ì ìš© í•„í„°: {filter_by}")
            
            # 4. HSì½”ë“œ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            hs_codes = self._extract_hs_codes(raw_query)
            if hs_codes:
                logger.info(f"ğŸ”¢ ê°ì§€ëœ HSì½”ë“œ: {hs_codes}")
                return self._search_by_hs_code(hs_codes, top_k, include_related)
            
            # 2.5. ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ë³„ ì²˜ë¦¬
            if search_context and search_context.get("domain_hints"):
                logger.info(f"ğŸ¯ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì ìš©: {search_context.get('domain_hints')}")
                # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ë³´ê°•
                context_keywords = search_context.get("boost_keywords", [])
                if context_keywords:
                    enhanced_query = f"{raw_query} {' '.join(context_keywords)}"
                    processed_query = self.query_processor.process_complex_query(enhanced_query)
                else:
                    processed_query = self.query_processor.process_complex_query(raw_query)
            else:
                processed_query = self.query_processor.process_complex_query(raw_query)
            
            # 3. ì‚¬ìš©í•  ì¿¼ë¦¬ ê²°ì • (ì´ë¯¸ ìœ„ì—ì„œ processed_query ìƒì„±ë¨)
            if expand_with_synonyms:
                search_query = processed_query["expanded_query"]
            else:
                search_query = processed_query["normalized_query"]
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            # 4. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedder.embed_text(search_query)
            
            # 5. ë™ì  ë²¡í„° ê²€ìƒ‰ (ìŠ¤ë§ˆíŠ¸ top_k ì¡°ì •)
            optimized_top_k, search_strategy = self._optimize_search_parameters(top_k, filter_by, processed_query)
            logger.info(f"ğŸ” ìµœì í™”ëœ ê²€ìƒ‰ ì‹œì‘ (top_k: {optimized_top_k}, ì „ëµ: {search_strategy})")
            
            # í•„í„°ë§ ì¡°ê±´ ì ìš©
            where_condition = self._build_where_condition(filter_by)
            
            logger.info(f"ğŸš€ ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰...")
            primary_results = self._execute_smart_search(
                query_embedding=query_embedding,
                top_k=optimized_top_k,
                where_condition=where_condition,
                search_strategy=search_strategy,
                original_query=raw_query
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            logger.info(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(primary_results)}ê°œ ë¬¸ì„œ")
            if primary_results:
                # ë°ì´í„° ì†ŒìŠ¤ë³„ ë¶„í¬ ë¡œê¹…
                source_distribution = {}
                for result in primary_results:
                    source = result.get('metadata', {}).get('data_source', 'unknown')
                    source_distribution[source] = source_distribution.get(source, 0) + 1
                logger.info(f"ğŸ“ˆ ë°ì´í„° ì†ŒìŠ¤ ë¶„í¬: {source_distribution}")
                
                # ìƒìœ„ 3ê°œ ê²°ê³¼ì˜ similarity ì ìˆ˜ ë¡œê¹…
                top_scores = [r.get('similarity', 0.0) for r in primary_results[:3]]
                logger.info(f"ğŸ¯ ìƒìœ„ ìœ ì‚¬ë„ ì ìˆ˜: {top_scores}")
            else:
                logger.warning(f"âš ï¸ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - fallback í•„ìš”")
            
            # 6. ê´€ë ¨ ë°ì´í„° í™•ì¥ ê²€ìƒ‰
            if include_related:
                expanded_results = self._expand_with_related_info(primary_results, top_k)
            else:
                expanded_results = primary_results
            
            # 7. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì •ë ¬ (search_context í¬í•¨)
            final_results = self._post_process_results(expanded_results, processed_query, search_context)
            
            # 8. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            if similarity_threshold > 0.0:
                filtered_results = [result for result in final_results 
                                  if result.get("similarity", 0) >= similarity_threshold]
                logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold}ë¡œ í•„í„°ë§: {len(final_results)} â†’ {len(filtered_results)}ê°œ")
                final_results = filtered_results
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…
            logger.info(f"ğŸ“Š ===== ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼ ===== ")
            if final_results:
                # ìµœì¢… ê²°ê³¼ í’ˆì§ˆ í‰ê°€
                avg_similarity = sum(r.get('similarity', 0.0) for r in final_results) / len(final_results)
                logger.info(f"ğŸ“ˆ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}")
                
                # regulation_type ë¶„í¬
                reg_types = {}
                for result in final_results:
                    reg_type = result.get('metadata', {}).get('regulation_type', 'unknown')
                    reg_types[reg_type] = reg_types.get(reg_type, 0) + 1
                logger.info(f"ğŸ·ï¸ ê·œì œ íƒ€ì… ë¶„í¬: {reg_types}")
            
            logger.info(f"===== ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ===== \n")
            logger.info(f"âœ… {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìš”ì²­ëœ top_k: {top_k})")
            
            # 7.5. ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° ì ì‘í˜• ê²€ìƒ‰ í™•ì¥
            quality_score = self._evaluate_result_quality(final_results, processed_query)
            logger.info(f"ğŸ“Š ê²°ê³¼ í’ˆì§ˆ í‰ê°€: {quality_score:.2f} (ê²°ê³¼ ìˆ˜: {len(final_results)})")
            
            # í’ˆì§ˆ ê¸°ë°˜ í™•ì¥ ê²€ìƒ‰ ê²°ì •
            needs_expansion = (
                len(final_results) < max(3, top_k // 2) or  # ê²°ê³¼ ìˆ˜ ë¶€ì¡±
                quality_score < 0.6 or  # í’ˆì§ˆ ì ìˆ˜ ë‚®ìŒ
                (len(final_results) < top_k and quality_score < 0.8)  # ìš”ì²­ ìˆ˜ëŸ‰ ë¯¸ë‹¬ì´ë©´ì„œ í’ˆì§ˆ ë³´í†µ
            )
            
            if needs_expansion and search_strategy != "expanded":
                logger.info(f"ğŸ”„ í™•ì¥ ê²€ìƒ‰ í•„ìš” (ê²°ê³¼: {len(final_results)}ê°œ, í’ˆì§ˆ: {quality_score:.2f})")
                expanded_results = self._execute_comprehensive_fallback_search(
                    raw_query, top_k, filter_by, processed_query, quality_score
                )
                
                # í™•ì¥ëœ ê²°ê³¼ê°€ ë” ë‚˜ì€ì§€ í‰ê°€
                if len(expanded_results) > 0:
                    expanded_quality = self._evaluate_result_quality(expanded_results, processed_query)
                    if len(expanded_results) > len(final_results) or expanded_quality > quality_score:
                        logger.info(f"ğŸ“ˆ í™•ì¥ ê²€ìƒ‰ ì„±ê³µ: {len(expanded_results)}ê°œ ê²°ê³¼, í’ˆì§ˆ: {expanded_quality:.2f}")
                        return expanded_results[:top_k]
            
            return final_results
            
        except Exception as e:
            logger.error(f"ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_country(self, country: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • êµ­ê°€ì˜ ë¬´ì—­ ê·œì œ ì •ë³´ ê²€ìƒ‰
        
        Args:
            country (str): êµ­ê°€ëª…
            top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # êµ­ê°€ëª… ì •ê·œí™”
            normalized_country = self._normalize_country_name(country)
            
            # ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰
            where_condition = {"country": {"$eq": normalized_country}}
            
            # êµ­ê°€ ê´€ë ¨ ì¿¼ë¦¬ë¡œ ì„ë² ë”© ê²€ìƒ‰
            query = f"{normalized_country} ë¬´ì—­ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì… ì œí•œ"
            query_embedding = self.embedder.embed_text(query)
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k,
                where=where_condition
            )
            
            logger.info(f"ğŸŒ {country} ê´€ë ¨ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"êµ­ê°€ë³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_product_category(self, category: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ ê²€ìƒ‰
        
        Args:
            category (str): ì œí’ˆ ì¹´í…Œê³ ë¦¬
            top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ì¿¼ë¦¬
            query = f"{category} ì œí’ˆ ë¬´ì—­ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì…"
            query_embedding = self.embedder.embed_text(query)
            
            # ì œí’ˆ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            where_condition = {"product_category": {"$eq": category}}
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k,
                where=where_condition
            )
            
            logger.info(f"ğŸ“¦ {category} ì¹´í…Œê³ ë¦¬ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_hs_codes(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ HSì½”ë“œ ì¶”ì¶œ"""
        matches = self.hs_code_pattern.findall(query)
        # 4ìë¦¬ ì´ìƒì˜ ìˆ«ìë§Œ HSì½”ë“œë¡œ ê°„ì£¼
        hs_codes = [match for match in matches if len(match) >= 4]
        return hs_codes
    
    def _search_by_hs_code(self, hs_codes: List[str], top_k: int, include_related: bool) -> List[Dict[str, Any]]:
        """HSì½”ë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        all_results = []
        
        for hs_code in hs_codes:
            try:
                # ì •í™•í•œ HSì½”ë“œ ë§¤ì¹­
                where_condition = {"hs_code": {"$eq": hs_code}}
                
                # HSì½”ë“œë¡œ ì¿¼ë¦¬ ìƒì„±
                query = f"HSì½”ë“œ {hs_code} ì œí’ˆ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì…"
                query_embedding = self.embedder.embed_text(query)
                
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    where=where_condition
                )
                
                # HSì½”ë“œ ë§¤ì¹­ ì •ë³´ ì¶”ê°€
                for result in results:
                    result["hs_code_match"] = hs_code
                    result["match_type"] = "exact_hs_code"
                
                all_results.extend(results)
                
                # ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ (ì• 6ìë¦¬ê°€ ê°™ì€ ê²½ìš°)
                if include_related and len(hs_code) >= 6:
                    related_results = self._search_related_hs_codes(hs_code[:6], top_k // 2)
                    all_results.extend(related_results)
                    
            except Exception as e:
                logger.error(f"HSì½”ë“œ {hs_code} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_results = self._deduplicate_results(all_results)
        return unique_results[:top_k]
    
    def _search_related_hs_codes(self, hs_prefix: str, top_k: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ (ì•ìë¦¬ê°€ ê°™ì€ ì œí’ˆë“¤)"""
        try:
            # HSì½”ë“œ ì•ìë¦¬ê°€ ê°™ì€ ì œí’ˆë“¤ ê²€ìƒ‰
            query = f"HSì½”ë“œ {hs_prefix} ê´€ë ¨ ì œí’ˆ"
            query_embedding = self.embedder.embed_text(query)
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k
            )
            
            # ê´€ë ¨ ì œí’ˆ ì •ë³´ ì¶”ê°€
            for result in results:
                result["match_type"] = "related_hs_code"
                result["hs_prefix_match"] = hs_prefix
            
            return results
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _normalize_country_name(self, country: str) -> str:
        """êµ­ê°€ëª… ì •ê·œí™”"""
        # ì¼ë°˜ì ì¸ êµ­ê°€ëª… ë§¤í•‘
        country_mapping = {
            "ë¯¸êµ­": "ë¯¸êµ­", "USA": "ë¯¸êµ­", "United States": "ë¯¸êµ­",
            "ì¤‘êµ­": "ì¤‘êµ­", "China": "ì¤‘êµ­",
            "ì¼ë³¸": "ì¼ë³¸", "Japan": "ì¼ë³¸",
            "ë…ì¼": "ë…ì¼", "Germany": "ë…ì¼",
            "ì˜êµ­": "ì˜êµ­", "UK": "ì˜êµ­", "United Kingdom": "ì˜êµ­",
            "í”„ë‘ìŠ¤": "í”„ë‘ìŠ¤", "France": "í”„ë‘ìŠ¤",
            "ì´íƒˆë¦¬ì•„": "ì´íƒˆë¦¬ì•„", "Italy": "ì´íƒˆë¦¬ì•„",
            "ìºë‚˜ë‹¤": "ìºë‚˜ë‹¤", "Canada": "ìºë‚˜ë‹¤",
            "í˜¸ì£¼": "í˜¸ì£¼", "Australia": "í˜¸ì£¼",
            "ì¸ë„": "ì¸ë„", "India": "ì¸ë„"
        }
        
        return country_mapping.get(country, country)
    
    def _build_where_condition(self, filter_by: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """í•„í„°ë§ ì¡°ê±´ êµ¬ì„± (data_type ì§€ì› ê°•í™”)"""
        if not filter_by:
            return None
        
        # ì§€ì›ë˜ëŠ” ë©”íƒ€ë°ì´í„° í•„ë“œ í™•ì¥
        supported_fields = [
            "category", "country", "regulation_type", "status", "hs_code", "product_category", 
            "data_type", "data_source", "consultation_type", "case_number", "product_name",
            "affected_country", "regulating_country", "animal_plant_type", "management_number"
        ]
        
        conditions = []
        for key, value in filter_by.items():
            if key in supported_fields:
                conditions.append({key: {"$eq": value}})
        
        if not conditions:
            return None
        elif len(conditions) == 1:
            return conditions[0]
        else:
            # ì—¬ëŸ¬ ì¡°ê±´ì„ $andë¡œ ê²°í•©
            return {"$and": conditions}
    
    def _expand_with_related_info(self, results: List[Dict[str, Any]], max_total: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ì •ë³´ë¡œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥"""
        expanded_results = results.copy()
        
        # ê¸°ì¡´ ê²°ê³¼ì—ì„œ HSì½”ë“œë‚˜ êµ­ê°€ ì •ë³´ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        seen_ids = {result.get("id", "") for result in results}
        
        for result in results[:3]:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ í™•ì¥
            metadata = result.get("metadata", {})
            
            # HSì½”ë“œ ê¸°ë°˜ í™•ì¥
            hs_code = metadata.get("hs_code", "")
            if hs_code and len(hs_code) >= 6:
                related_hs = self._search_related_hs_codes(hs_code[:6], 2)
                for related in related_hs:
                    if related.get("id", "") not in seen_ids:
                        related["reference_info"] = {"is_related": True, "related_to": result.get("id", "")}
                        expanded_results.append(related)
                        seen_ids.add(related.get("id", ""))
            
            # êµ­ê°€ ê¸°ë°˜ í™•ì¥
            country = metadata.get("country", "")
            if country:
                related_country = self.search_by_country(country, 2)
                for related in related_country:
                    if related.get("id", "") not in seen_ids:
                        related["reference_info"] = {"is_related": True, "related_to": result.get("id", "")}
                        expanded_results.append(related)
                        seen_ids.add(related.get("id", ""))
        
        return expanded_results[:max_total]
    
    def _post_process_results(self, results: List[Dict[str, Any]], processed_query: Dict[str, Any], search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ (search_context ê¸°ë°˜ ë¶€ìŠ¤íŒ… í¬í•¨)"""
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        for result in results:
            importance_score = self._calculate_importance_score(result, processed_query)
            result["importance_score"] = importance_score
            
            # search_context ê¸°ë°˜ ë¶€ìŠ¤íŒ…
            if search_context:
                context_boost = self._apply_search_context_boost(result, search_context)
                result["context_boost"] = context_boost
                result["importance_score"] += context_boost
        
        # ì¤‘ìš”ë„ì™€ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: (
            x.get("importance_score", 0) * 0.3 + 
            x.get("similarity", 0) * 0.7
        ), reverse=True)
        
        return results
    
    def _apply_search_context_boost(self, result: Dict[str, Any], search_context: Dict[str, Any]) -> float:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìˆ˜ ê³„ì‚°"""
        boost_score = 0.0
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        
        # ìš°ì„ ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ ë¶€ìŠ¤íŒ…
        priority_sources = search_context.get("priority_data_sources", [])
        data_source = metadata.get("data_source", "")
        if data_source in priority_sources:
            boost_score += 0.3
            result["boosted"] = True
            result["boost_reason"] = f"ìš°ì„ ìˆœìœ„ ë°ì´í„°ì†ŒìŠ¤: {data_source}"
        
        # ë„ë©”ì¸ íŒíŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
        domain_hints = search_context.get("domain_hints", [])
        for hint in domain_hints:
            if hint in content.lower() or hint in str(metadata).lower():
                boost_score += 0.2
                result["boosted"] = True
                result["boost_reason"] = result.get("boost_reason", "") + f" ë„ë©”ì¸ë§¤ì¹­: {hint}"
        
        # ë¶€ìŠ¤íŒ… í‚¤ì›Œë“œ ë§¤ì¹­
        boost_keywords = search_context.get("boost_keywords", [])
        for keyword in boost_keywords:
            if keyword in content or keyword in str(metadata):
                boost_score += 0.1
        
        return min(boost_score, 0.5)  # ìµœëŒ€ 0.5ë¡œ ì œí•œ
    
    def _calculate_importance_score(self, result: Dict[str, Any], processed_query: Dict[str, Any]) -> float:
        """ë¬¸ì„œ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        metadata = result.get("metadata", {})
        
        # ë™ì‹ë¬¼ ìˆ˜ì… ê·œì œ ë°ì´í„° ìµœìš°ì„  ì²˜ë¦¬
        data_source = metadata.get("data_source", "")
        if data_source == "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­":
            score += 0.5  # ë™ì‹ë¬¼ ê·œì œëŠ” ìµœê³  ì¤‘ìš”ë„
            
            # ë™ì‹ë¬¼ ê´€ë ¨ ì¶”ê°€ ê°€ì¤‘ì¹˜
            priority = metadata.get("priority", 0)
            regulation_intensity = metadata.get("regulation_intensity", "")
            
            # ğŸ”§ íƒ€ì… ì•ˆì „ì„±: priorityë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            try:
                priority_int = int(priority) if priority else 0
            except (ValueError, TypeError):
                priority_int = 0
            
            if priority_int >= 2:  # ë†’ì€ ìš°ì„ ìˆœìœ„
                score += 0.2
            if regulation_intensity == "high":  # ë†’ì€ ê·œì œ ê°•ë„
                score += 0.15
            if metadata.get("has_global_prohibition", False):  # ì „ì—­ ê¸ˆì§€
                score += 0.1
                
        elif "ê·œì œ" in data_source:
            score += 0.3  # ì¼ë°˜ ê·œì œ ì •ë³´
        
        # ê·œì œ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        regulation_type = metadata.get("regulation_type", "")
        if regulation_type == "import_regulations":
            score += 0.25  # ìˆ˜ì… ê·œì œëŠ” ë†’ì€ ì¤‘ìš”ë„
        elif "restrictions" in regulation_type:
            score += 0.2
        elif "prohibitions" in regulation_type:
            score += 0.3  # ê¸ˆì§€ í’ˆëª©ì€ ë§¤ìš° ì¤‘ìš”
        
        # HSì½”ë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜
        if result.get("match_type") == "exact_hs_code":
            score += 0.4
        elif result.get("match_type") == "related_hs_code":
            score += 0.2
        
        # ìƒíƒœë³„ ê°€ì¤‘ì¹˜
        is_active = metadata.get("is_active", False)
        if is_active:
            score += 0.15  # í™œì„± ê·œì œëŠ” ì¤‘ìš”ë„ ë†’ìŒ
            
        status = metadata.get("status", "")
        if "ê·œì œì¤‘" in status:
            score += 0.2  # ì‹¤ì œ ì ìš© ì¤‘ì¸ ê·œì œ
        elif "ì¡°ì‚¬ì¤‘" in status:
            score += 0.1
        
        # ë°ì´í„° íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        data_type = metadata.get("data_type", "")
        if data_type == "trade_regulation":
            score += 0.1  # ë¬´ì—­ ê·œì œ ë°ì´í„°
        
        return min(score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            result_id = result.get("id", "")
            if result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)
        
        return unique_results
    
    def get_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            stats = self.vector_store.get_collection_stats()
            stats["retriever_type"] = "LangChain_TradeInfoRetriever"
            return stats
        except Exception as e:
            logger.error(f"í†µê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _generate_llm_metadata_filters(self, query: str, search_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        LLMì„ ì‚¬ìš©í•´ì„œ ì¿¼ë¦¬ì— ë§ëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ìƒì„± (ê°•í™”ëœ ë²„ì „)
        
        Args:
            query (str): ì‚¬ìš©ì ì§ˆì˜
            search_context (Optional[Dict[str, Any]]): ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìƒì„±ëœ ë©”íƒ€ë°ì´í„° í•„í„°
        """
        try:
            # 1. LLM ì˜ë„ ë¶„ì„ìœ¼ë¡œ í•„í„° íŒíŠ¸ ì¶”ì¶œ
            intent_info = self.query_normalizer.extract_intent(query)
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í•„í„° (search_contextê°€ ì œê³µí•œ íŒíŠ¸ ìš°ì„  ì ìš©)
            filters = self._apply_context_based_filters(search_context)
            
            # 3. LLM ì˜ë„ ë¶„ì„ì„ í†µí•œ ìŠ¤ë§ˆíŠ¸ í•„í„° ë§¤í•‘
            smart_filters = self._generate_smart_filter_mapping(intent_info, query)
            
            # 4. í•„í„° ë³‘í•© (ì»¨í…ìŠ¤íŠ¸ > ìŠ¤ë§ˆíŠ¸ ë§¤í•‘ > ê¸°ë³¸ê°’ ìˆœ)
            final_filters = self._merge_filter_priorities(filters, smart_filters, search_context)
            
            # 5. ì œí’ˆë³„ íŠ¹í™” í•„í„° ì¶”ê°€
            product_filters = self._extract_product_specific_filters(intent_info)
            final_filters.update(product_filters)
            
            logger.info(f"ğŸ¤– ìŠ¤ë§ˆíŠ¸ í•„í„° ìƒì„±: {final_filters}")
            return final_filters
            
        except Exception as e:
            logger.error(f"LLM ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ê·œì œ ë°ì´í„°ë¡œ fallback
            return {"data_type": "trade_regulation"}
    
    def _apply_context_based_filters(self, search_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ í•„í„° ì ìš©
        
        Args:
            search_context: ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°
        """
        filters = {}
        
        if not search_context:
            return filters
        
        # ì—ì´ì „íŠ¸ íƒ€ì…ë³„ ê¸°ë³¸ í•„í„°
        agent_type = search_context.get("agent_type")
        if agent_type == "regulation_agent":
            filters["data_type"] = "trade_regulation"
        elif agent_type == "consultation_agent":
            filters["data_type"] = "consultation_case"
        
        # ìš°ì„ ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ ì ìš©
        priority_sources = search_context.get("priority_data_sources", [])
        if priority_sources:
            filters["data_source"] = priority_sources[0]
        
        # ê·œì œ íƒ€ì… íŒíŠ¸ ì ìš© (ìµœìš°ì„ )
        regulation_type_hint = search_context.get("regulation_type_hint")
        if regulation_type_hint:
            filters["regulation_type"] = regulation_type_hint
            logger.info(f"ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ê·œì œ íƒ€ì…: {regulation_type_hint}")
        
        return filters
    
    def _generate_smart_filter_mapping(self, intent_info: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        ğŸš€ í†µí•© LLM ê¸°ë°˜ ì§€ëŠ¥í˜• í•„í„° ë§¤í•‘ (í•˜ë“œì½”ë”© ë§¤í•‘ ì™„ì „ ì œê±°)
        
        Args:
            intent_info: LLMì´ ì¶”ì¶œí•œ ì˜ë„ ì •ë³´ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„± ìœ ì§€)
            query: ì›ë³¸ ì§ˆì˜
            
        Returns:
            Dict[str, Any]: ì™„ì „í•œ ì§€ëŠ¥í˜• í•„í„°
        """
        try:
            # ğŸ§  ëª¨ë“  ê·œì œ ìœ í˜•ì„ LLMì´ í•œ ë²ˆì— ë¶„ë¥˜
            llm_classification = self._classify_regulation_query_with_llm(query)
            
            # fallbackì´ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬
            if llm_classification.get("fallback_needed") or llm_classification.get("regulation_category") in ["error", "unclear"]:
                logger.warning(f"ğŸ”„ LLM ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ ê·œì œ ë°ì´í„°ë¡œ fallback: {llm_classification.get('reasoning', '')}")
                return self._apply_basic_fallback_filters(query)
            
            # ì§ì ‘ data_sourceì™€ regulation_type ë§¤í•‘
            data_source = llm_classification.get("data_source")
            regulation_type = llm_classification.get("regulation_type")
            confidence = llm_classification.get("confidence", 0.0)
            
            # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ì€ ê²½ìš° fallback
            if confidence < 0.3:
                logger.warning(f"ğŸ”„ LLM ë¶„ë¥˜ ì‹ ë¢°ë„ ë¶€ì¡± ({confidence:.2f}), fallback ì ìš©")
                return self._apply_basic_fallback_filters(query)
            
            if not data_source or not regulation_type:
                logger.warning(f"ğŸ¤– LLM ë¶„ë¥˜ ë¶ˆì™„ì „: data_source={data_source}, regulation_type={regulation_type}")
                return self._apply_basic_fallback_filters(query)
            
            # ê¸°ë³¸ í•„í„° êµ¬ì„±
            filters = {
                "data_type": "trade_regulation",
                "data_source": data_source,
                "regulation_type": regulation_type
            }
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©
            detected_country = llm_classification.get("detected_country")
            if detected_country and detected_country.lower() != "null":
                regulation_category = llm_classification.get("regulation_category", "")
                if "foreign" in regulation_category:
                    # ì™¸êµ­ ê·œì œì˜ ê²½ìš° regulating_country í•„í„° ì¶”ê°€
                    filters["regulating_country"] = detected_country
                elif regulation_category in ["export_restrictions", "export_prohibitions"]:
                    # ìˆ˜ì¶œì œí•œ/ê¸ˆì§€ëŠ” í’ˆëª© ê¸°ì¤€ ê·œì œì´ë¯€ë¡œ êµ­ê°€ í•„í„° ë¶ˆí•„ìš”
                    # affected_country ë©”íƒ€ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ í•„í„°ë§ ì‹œ 0ê°œ ê²°ê³¼ ë°œìƒ ë°©ì§€
                    pass
                
            # ë™ì‹ë¬¼ ì œí’ˆ íŠ¹ë³„ í•„í„°
            product_info = llm_classification.get("product_info", {})
            if product_info.get("is_animal_plant"):
                filters["animal_plant_type"] = "ë™ë¬¼|ì‹ë¬¼"  # ë™ì‹ë¬¼ êµ¬ë¶„
            
            # HSì½”ë“œ ì–¸ê¸‰ ì‹œ HSì½”ë“œ í•„í„° í™œì„±í™” (í–¥í›„ í™•ì¥ìš©)
            if product_info.get("hs_code_mentioned"):
                filters["has_hs_code"] = True
            
            category = llm_classification.get("regulation_category", "unknown")
            reasoning = llm_classification.get("reasoning", "")
            
            logger.info(f"ğŸš€ í†µí•© LLM ë¶„ë¥˜ ì„±ê³µ: {category} â†’ {data_source} (ì‹ ë¢°ë„: {confidence:.2f})")
            logger.info(f"ğŸ“Š ë¶„ë¥˜ ê·¼ê±°: {reasoning[:100]}..." if len(reasoning) > 100 else f"ğŸ“Š ë¶„ë¥˜ ê·¼ê±°: {reasoning}")
            
            return filters
            
        except Exception as e:
            logger.error(f"í†µí•© LLM í•„í„° ë§¤í•‘ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ fallback
            return self._apply_basic_fallback_filters(query)
    
    def _classify_regulation_query_with_llm(self, query: str) -> Dict[str, Any]:
        """
        ğŸš€ ì™„ì „ í†µí•© LLM ê¸°ë°˜ ë¬´ì—­ ê·œì œ ë¶„ë¥˜ ì‹œìŠ¤í…œ
        
        ëª¨ë“  ê·œì œ ìœ í˜•ì„ í•œ ë²ˆì— ë¶„ë¥˜í•˜ì—¬ í•˜ë“œì½”ë”© ë§¤í•‘ ì œê±°
        
        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            
        Returns:
            Dict[str, Any]: ì™„ì „í•œ ë¶„ë¥˜ ë° í•„í„° ì •ë³´
        """
        try:
            # ğŸš€ ìºì‹œ í™•ì¸ (ì„±ëŠ¥ ìµœì í™”)
            cache_key = query.lower().strip()
            if cache_key in self._llm_classification_cache:
                cached_result = self._llm_classification_cache[cache_key]
                # TTL ê²€ì‚¬
                import time
                if time.time() - cached_result.get("cached_at", 0) < self._cache_ttl:
                    logger.info(f"ğŸ’¾ LLM ë¶„ë¥˜ ìºì‹œ ì ì¤‘: {cache_key[:30]}...")
                    return cached_result["classification"]
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self._llm_classification_cache[cache_key]
            
            classification_prompt = f"""
ë‹¤ìŒ ë¬´ì—­ ê·œì œ ì§ˆì˜ë¥¼ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë°ì´í„° ì†ŒìŠ¤ì™€ í•„í„°ë¥¼ ê²°ì •í•´ì£¼ì„¸ìš”:

ì§ˆì˜: "{query}"

## ğŸ¯ ë¶„ë¥˜í•  ê·œì œ ìœ í˜• (data_source ë§¤í•‘):

1. **ì™¸êµ­ ê·œì œ** â†’ "ìˆ˜ì…ê·œì œDB_ì „ì²´"
   - ì™¸êµ­ì´ í•œêµ­ ì œí’ˆì— ê±°ëŠ” ê·œì œ: ë°˜ë¤í•‘, ì„¸ì´í”„ê°€ë“œ, ìˆ˜ì…ì œí•œ ë“±
   - í‚¤ì›Œë“œ: [êµ­ê°€ëª…] + "ê±°ëŠ”", "ê·œì œ", "ë°˜ë¤í•‘", "ì„¸ì´í”„ê°€ë“œ", "ìˆ˜ì…ì œí•œ"
   - ì˜ˆ: "ë² íŠ¸ë‚¨ì´ í•œêµ­ì— ê±°ëŠ” ê·œì œ", "ì¸ë„ ë°˜ë¤í•‘", "ë¯¸êµ­ ì„¸ì´í”„ê°€ë“œ"

2. **í•œêµ­ ìˆ˜ì¶œì œí•œ** â†’ "ìˆ˜ì¶œì œí•œí’ˆëª©"  
   - í•œêµ­ì´ íŠ¹ì • í’ˆëª©ì˜ ìˆ˜ì¶œì„ ì œí•œ
   - í‚¤ì›Œë“œ: "ìˆ˜ì¶œì œí•œ", "ìˆ˜ì¶œ ì œí•œ", "ë‚´ë³´ë‚¼ ìˆ˜ ì—†ëŠ”", "ìˆ˜ì¶œ ê·œì œ"
   - ì˜ˆ: "HSì½”ë“œ ìˆ˜ì¶œì œí•œ", "ê´‘ë¬¼ ìˆ˜ì¶œì œí•œ"

3. **í•œêµ­ ìˆ˜ì¶œê¸ˆì§€** â†’ "ìˆ˜ì¶œê¸ˆì§€í’ˆëª©"
   - í•œêµ­ì´ íŠ¹ì • í’ˆëª©ì˜ ìˆ˜ì¶œì„ ì™„ì „ ê¸ˆì§€  
   - í‚¤ì›Œë“œ: "ìˆ˜ì¶œê¸ˆì§€", "ìˆ˜ì¶œ ê¸ˆì§€", "ìˆ˜ì¶œ ë¶ˆê°€", "ìˆ˜ì¶œ ì°¨ë‹¨"
   - ì˜ˆ: "ê³ ë˜ê³ ê¸° ìˆ˜ì¶œê¸ˆì§€", "ë©¸ì¢…ìœ„ê¸°ì¢… ìˆ˜ì¶œê¸ˆì§€"

4. **í•œêµ­ ìˆ˜ì…ì œí•œ** â†’ "ìˆ˜ì…ì œí•œí’ˆëª©"
   - í•œêµ­ì´ íŠ¹ì • í’ˆëª©ì˜ ìˆ˜ì…ì„ ì œí•œ (ìŠ¹ì¸ í•„ìš”)
   - í‚¤ì›Œë“œ: "ìˆ˜ì…ì œí•œ", "ìˆ˜ì… ì œí•œ", "ìŠ¹ì¸ í•„ìš”", "ìˆ˜ì… ê·œì œ"
   - ì˜ˆ: "í•­ê³µê¸° ë¶€í’ˆ ìˆ˜ì…ì œí•œ", "í”Œë¼ìŠ¤í‹± ìˆ˜ì…ì œí•œ"

5. **ë™ì‹ë¬¼ ìˆ˜ì…ê·œì œ** â†’ "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"
   - ë™ë¬¼, ì‹ë¬¼, ë†ì¶•ì‚°í’ˆ ìˆ˜ì… í—ˆìš©/ê¸ˆì§€ êµ­ê°€ ì •ë³´
   - í‚¤ì›Œë“œ: ë™ë¬¼/ì‹ë¬¼ëª…, "ìˆ˜ì… í—ˆìš©", "ê²€ì—­", "ë†ì‚°ë¬¼", "ì¶•ì‚°ë¬¼"
   - ì˜ˆ: "ë”¸ê¸° ìˆ˜ì…", "ì†Œê³ ê¸° í—ˆìš©êµ­ê°€", "ì•„ë³´ì¹´ë„ ê²€ì—­"

## ğŸ¯ ë¶„ì„ ìš”ì†Œ:
- **ì£¼ì²´ êµ­ê°€**: ê·œì œí•˜ëŠ” êµ­ê°€ (ì™¸êµ­ vs í•œêµ­)
- **ëŒ€ìƒ í’ˆëª©**: êµ¬ì²´ì  í’ˆëª©ëª…, HSì½”ë“œ, ì¹´í…Œê³ ë¦¬
- **ê·œì œ ê°•ë„**: ì œí•œ vs ê¸ˆì§€ vs í—ˆìš© ì¡°ê±´
- **ë°©í–¥ì„±**: ìˆ˜ì¶œ vs ìˆ˜ì… + í•œêµ­â†”ì™¸êµ­ ë°©í–¥

ì‘ë‹µ JSON í˜•ì‹:
{{
    "regulation_category": "foreign_regulation|export_restrictions|export_prohibitions|import_restrictions|animal_plant_import",
    "data_source": "ìˆ˜ì…ê·œì œDB_ì „ì²´|ìˆ˜ì¶œì œí•œí’ˆëª©|ìˆ˜ì¶œê¸ˆì§€í’ˆëª©|ìˆ˜ì…ì œí•œí’ˆëª©|ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­", 
    "regulation_type": "export_destination_restrictions|export_restrictions|export_prohibitions|import_restrictions|import_regulations",
    "detected_country": "êµ­ê°€ëª… ë˜ëŠ” null",
    "product_info": {{
        "is_animal_plant": true/false,
        "hs_code_mentioned": true/false,
        "product_category": "ì¶”ì • ì¹´í…Œê³ ë¦¬ ë˜ëŠ” null"
    }},
    "regulation_direction": "foreign_to_korea|korea_export|korea_import|unclear",
    "confidence": 0.0-1.0,
    "reasoning": "ë¶„ë¥˜ ê·¼ê±° ì„¤ëª…"
}}

## ğŸ” ë¶„ë¥˜ ì˜ˆì‹œ:
- "ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´ê°€ í˜„ì¬ í–‰í•˜ëŠ” ê·œì œë“¤" 
  â†’ {{"regulation_category": "foreign_regulation", "data_source": "ìˆ˜ì…ê·œì œDB_ì „ì²´"}}
  
- "ë”¸ê¸° ìˆ˜ì…í•  ë•Œ ì£¼ì˜ì‚¬í•­"
  â†’ {{"regulation_category": "animal_plant_import", "data_source": "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"}}
  
- "HSì½”ë“œ 2505 ìˆ˜ì¶œì œí•œ ìˆë‚˜ìš”?"
  â†’ {{"regulation_category": "export_restrictions", "data_source": "ìˆ˜ì¶œì œí•œí’ˆëª©"}}

ì •í™•í•œ ë¶„ë¥˜ë¥¼ ìœ„í•´ í‚¤ì›Œë“œ, ë¬¸ë§¥, ì£¼ì²´-ê°ì²´ ê´€ê³„ë¥¼ ì¢…í•© ë¶„ì„í•˜ì„¸ìš”.
"""

            response = self.query_normalizer.client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": classification_prompt}],
                temperature=0.1,
                max_tokens=300
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # JSON ì¶”ì¶œ ì‹œë„
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result_json = json.loads(json_match.group())
                
                # ğŸš€ ìºì‹œì— ì €ì¥ (ì„±ëŠ¥ ìµœì í™”)
                import time
                cached_entry = {
                    "classification": result_json,
                    "cached_at": time.time()
                }
                
                # ìºì‹œ í¬ê¸° ì œí•œ ê´€ë¦¬
                if len(self._llm_classification_cache) >= self._cache_max_size:
                    # ê°€ì¥ ì˜¤ë˜ëœ ì—”íŠ¸ë¦¬ ì œê±° (FIFO)
                    oldest_key = min(self._llm_classification_cache.keys(), 
                                    key=lambda k: self._llm_classification_cache[k].get("cached_at", 0))
                    del self._llm_classification_cache[oldest_key]
                
                self._llm_classification_cache[cache_key] = cached_entry
                
                logger.info(f"ğŸ¤– LLM ê·œì œ ë¶„ë¥˜: {result_json.get('regulation_category', 'unknown')} â†’ {result_json.get('data_source', 'none')}")
                return result_json
            else:
                logger.warning(f"LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ì‹¤íŒ¨: {result_text}")
                # ë” ë‚˜ì€ ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    "regulation_category": "unclear",
                    "data_source": None,
                    "regulation_type": None,
                    "confidence": 0.0,
                    "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨"
                }
                
        except Exception as e:
            logger.error(f"LLM ê·œì œ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            # ë” ë‚˜ì€ fallback ì²˜ë¦¬
            return {
                "regulation_category": "error",
                "data_source": None,
                "regulation_type": None,
                "confidence": 0.0,
                "reasoning": f"LLM ë¶„ë¥˜ ì˜¤ë¥˜: {str(e)}",
                "fallback_needed": True
            }
    
    def _apply_basic_fallback_filters(self, query: str) -> Dict[str, Any]:
        """
        ğŸ”„ LLM ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ fallback í•„í„° ì ìš©
        
        ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ìµœì†Œí•œì˜ ë¶„ë¥˜ ìˆ˜í–‰
        
        Args:
            query: ì›ë³¸ ì§ˆì˜
            
        Returns:
            Dict[str, Any]: ê¸°ë³¸ fallback í•„í„°
        """
        query_lower = query.lower()
        
        # ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ë³¸ ë¶„ë¥˜ (LLM ëŒ€ë¹„ ì •í™•ë„ ë‚®ìŒ)
        if any(keyword in query_lower for keyword in ['ìˆ˜ì¶œê¸ˆì§€', 'ìˆ˜ì¶œ ê¸ˆì§€', 'ê¸ˆì§€í’ˆëª©']):
            logger.info(f"ğŸ“‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback: ìˆ˜ì¶œê¸ˆì§€í’ˆëª©")
            return {
                "data_type": "trade_regulation",
                "data_source": "ìˆ˜ì¶œê¸ˆì§€í’ˆëª©",
                "regulation_type": "export_prohibitions"
            }
        elif any(keyword in query_lower for keyword in ['ìˆ˜ì¶œì œí•œ', 'ìˆ˜ì¶œ ì œí•œ']):
            logger.info(f"ğŸ“‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback: ìˆ˜ì¶œì œí•œí’ˆëª©")
            return {
                "data_type": "trade_regulation",
                "data_source": "ìˆ˜ì¶œì œí•œí’ˆëª©",
                "regulation_type": "export_restrictions"
            }
        elif any(keyword in query_lower for keyword in ['ë™ì‹ë¬¼', 'ë†ì‚°ë¬¼', 'ì¶•ì‚°ë¬¼', 'ê²€ì—­', 'ì•„ë³´ì¹´ë„', 'ë”¸ê¸°', 'ì†Œê³ ê¸°']):
            logger.info(f"ğŸ“‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback: ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­")
            return {
                "data_type": "trade_regulation",
                "data_source": "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­",
                "regulation_type": "import_regulations"
            }
        elif any(keyword in query_lower for keyword in ['ìˆ˜ì…ì œí•œ', 'ìˆ˜ì… ì œí•œ']):
            logger.info(f"ğŸ“‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback: ìˆ˜ì…ì œí•œí’ˆëª©")
            return {
                "data_type": "trade_regulation",
                "data_source": "ìˆ˜ì…ì œí•œí’ˆëª©",
                "regulation_type": "import_restrictions"
            }
        elif any(keyword in query_lower for keyword in ['ë°˜ë¤í•‘', 'ì„¸ì´í”„ê°€ë“œ', 'ê·œì œ', 'ì™¸êµ­', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ë² íŠ¸ë‚¨']):
            logger.info(f"ğŸ“‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback: ìˆ˜ì…ê·œì œDB_ì „ì²´")
            return {
                "data_type": "trade_regulation",
                "data_source": "ìˆ˜ì…ê·œì œDB_ì „ì²´",
                "regulation_type": "export_destination_restrictions"
            }
        else:
            # ìµœì¢… ê¸°ë³¸ê°’
            logger.info(f"ğŸ“‹ ìµœì¢… ê¸°ë³¸ê°’ fallback: trade_regulation ì „ì²´")
            return {"data_type": "trade_regulation"}
    
    def _apply_enhanced_fallback_search(self, query: str, filters: Dict[str, Any]) -> List[Dict]:
        """
        ğŸ”„ ê°•í™”ëœ Fallback ê²€ìƒ‰ ì‹œìŠ¤í…œ - ë‹¨ê³„ì  í•„í„° ì™„í™”
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            filters: ì›ë³¸ í•„í„°
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # ë‹¨ê³„ì  fallback ì „ëµ
            fallback_filters = [
                filters,  # ì›ë³¸ í•„í„°
                {k: v for k, v in filters.items() if k in ["data_type", "data_source"]},  # ê¸°ë³¸ í•„í„°ë§Œ
                {"data_type": "trade_regulation"},  # ìµœì†Œ í•„í„°
            ]
            
            for i, fallback_filter in enumerate(fallback_filters, 1):
                try:
                    query_embedding = self.embedder.embed_text(query)
                    results = self.vector_store.search_similar(
                        query_embedding=query_embedding,
                        top_k=12,
                        where=fallback_filter
                    )
                    
                    if results:
                        logger.info(f"âœ… Fallback ê²€ìƒ‰ ì„±ê³µ (ë‹¨ê³„ {i}): {len(results)}ê°œ ê²°ê³¼")
                        logger.debug(f"ğŸ“Š ì ìš©ëœ í•„í„°: {fallback_filter}")
                        return results
                    else:
                        logger.warning(f"âš ï¸ Fallback ë‹¨ê³„ {i} ì‹¤íŒ¨: 0ê°œ ê²°ê³¼")
                        
                except Exception as step_error:
                    logger.error(f"âŒ Fallback ë‹¨ê³„ {i} ì˜¤ë¥˜: {step_error}")
                    continue
            
            logger.error(f"âŒ ëª¨ë“  Fallback ê²€ìƒ‰ ì‹¤íŒ¨")
            return []
            
        except Exception as e:
            logger.error(f"âŒ Fallback ê²€ìƒ‰ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_category_based_filters(self, trade_category: str) -> Dict[str, Any]:
        """âš ï¸ DEPRECATED: ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í•„í„° ë§¤í•‘ (LLM í†µí•© ë¶„ë¥˜ë¡œ ëŒ€ì²´ë¨)"""
        logger.warning(f"âš ï¸ DEPRECATED í•¨ìˆ˜ í˜¸ì¶œ: _get_category_based_filters({trade_category}) - LLM ë¶„ë¥˜ ì‹œìŠ¤í…œ ì‚¬ìš© ê¶Œì¥")
        
        # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ë˜ ìƒˆë¡œìš´ fallbackìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        return self._apply_basic_fallback_filters(trade_category)
    
    def _extract_product_specific_filters(self, intent_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        âš ï¸ DEPRECATED: ì œí’ˆë³„ íŠ¹í™” í•„í„° ì¶”ì¶œ (LLM í†µí•© ë¶„ë¥˜ë¡œ ëŒ€ì²´ë¨)
        
        LLMì´ ëª¨ë“  ì œí’ˆ ë¶„ë¥˜ë¥¼ ë‹´ë‹¹í•˜ë¯€ë¡œ ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ
        
        Args:
            intent_info: LLM ì˜ë„ ë¶„ì„ ê²°ê³¼ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            
        Returns:
            Dict[str, Any]: ë¹ˆ ë”•ì…”ë„ˆë¦¬ (LLMì´ ëª¨ë“  ë¶„ë¥˜ ì²˜ë¦¬)
        """
        logger.debug(f"âš ï¸ DEPRECATED í•¨ìˆ˜ í˜¸ì¶œ: _extract_product_specific_filters - LLM ë¶„ë¥˜ê°€ ëª¨ë“  ì œí’ˆ ë¶„ë¥˜ ì²˜ë¦¬")
        
        # LLMì´ ëª¨ë“  ì œí’ˆ ë¶„ë¥˜ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        return {}
    
    def _merge_filter_priorities(self, context_filters: Dict[str, Any], 
                               smart_filters: Dict[str, Any],
                               search_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ í•„í„°ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë³‘í•© (ê°œì„ ëœ ìš°ì„ ìˆœìœ„ ë¡œì§)
        
        ğŸ¯ NEW ìš°ì„ ìˆœìœ„: ìŠ¤ë§ˆíŠ¸ ë§¤í•‘(LLM) > ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸ > ê¸°ë³¸ê°’
        
        Args:
            context_filters: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°
            smart_filters: LLM ìŠ¤ë§ˆíŠ¸ ë§¤í•‘ í•„í„°
            search_context: ì›ë³¸ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìš°ì„ ìˆœìœ„ ì ìš©ëœ ìµœì¢… í•„í„°
        """
        # ğŸ¯ 1. ìŠ¤ë§ˆíŠ¸ ë§¤í•‘(LLM)ì„ ê¸°ë³¸ìœ¼ë¡œ ì‹œì‘ (ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„)
        merged = smart_filters.copy()
        
        # 2. ì»¨í…ìŠ¤íŠ¸ í•„í„°ëŠ” ìŠ¤ë§ˆíŠ¸ í•„í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì ìš©
        for key, value in context_filters.items():
            if key not in merged or not merged[key]:  # LLMì´ ë¶„ë¥˜í•˜ì§€ ëª»í•œ ê²½ìš°ë§Œ
                merged[key] = value
                logger.debug(f"ğŸ”§ ì»¨í…ìŠ¤íŠ¸ í•„í„° ë³´ì™„: {key} = {value}")
        
        # 3. ìµœì†Œí•œ data_typeì€ ë³´ì¥
        if "data_type" not in merged or not merged["data_type"]:
            agent_type = search_context.get("agent_type") if search_context else None
            if agent_type == "regulation_agent":
                merged["data_type"] = "trade_regulation"
            elif agent_type == "consultation_agent":
                merged["data_type"] = "consultation_case"
            else:
                merged["data_type"] = "trade_regulation"  # ê¸°ë³¸ê°’
        
        # 4. í•„í„° í’ˆì§ˆ ê²€ì¦
        if merged.get("data_source") and merged.get("regulation_type"):
            logger.info(f"âœ… ê³ í’ˆì§ˆ í•„í„° ë³‘í•© ì™„ë£Œ: {merged.get('data_source')} + {merged.get('regulation_type')}")
        else:
            logger.warning(f"âš ï¸ ë¶€ë¶„ì  í•„í„° ë³‘í•©: data_source={merged.get('data_source')}, regulation_type={merged.get('regulation_type')}")
        
        return merged
    
    def _merge_filters(self, user_filters: Optional[Dict[str, Any]], llm_filters: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì œê³µ í•„í„°ì™€ LLM ìƒì„± í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©
        
        Args:
            user_filters: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•œ í•„í„°
            llm_filters: LLMì´ ìƒì„±í•œ í•„í„°
            
        Returns:
            Dict[str, Any]: ë³‘í•©ëœ í•„í„°
        """
        if not user_filters:
            return llm_filters
        
        if not llm_filters:
            return user_filters or {}
        
        # ì‚¬ìš©ì í•„í„°ê°€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§€ë˜, LLM í•„í„°ë¡œ ë³´ì™„
        merged = llm_filters.copy()
        merged.update(user_filters)  # ì‚¬ìš©ì í•„í„°ë¡œ ë®ì–´ì“°ê¸°
        
        return merged
    
    def get_langchain_retriever(self, 
                               search_type: str = "similarity",
                               search_kwargs: Optional[Dict[str, Any]] = None):
        """
        LangChain í‘œì¤€ Retriever ê°ì²´ ë°˜í™˜ (ì²´ì¸ ì—°ê²°ìš©)
        
        Args:
            search_type (str): ê²€ìƒ‰ íƒ€ì… ("similarity", "mmr", "similarity_score_threshold")
            search_kwargs (Optional[Dict[str, Any]]): ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            
        Returns:
            Retriever: LangChain Retriever ê°ì²´
        """
        if search_kwargs is None:
            search_kwargs = {"k": 5}
        
        return self.vector_store.get_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )
    
    def _optimize_search_parameters(self, base_top_k: int, 
                                  filter_conditions: Optional[Dict[str, Any]], 
                                  processed_query: Dict[str, Any]) -> Tuple[int, str]:
        """
        ì¿¼ë¦¬ ë³µì¡ë„ì™€ í•„í„° ì¡°ê±´ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ë¥¼ ë™ì  ìµœì í™”
        
        Args:
            base_top_k: ê¸°ë³¸ top_k ê°’
            filter_conditions: ì ìš©ëœ í•„í„° ì¡°ê±´
            processed_query: ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            Tuple[int, str]: (ìµœì í™”ëœ top_k, ê²€ìƒ‰ ì „ëµ)
        """
        try:
            # 1. í•„í„° ì‹ ë¢°ë„ í‰ê°€
            filter_confidence = self._evaluate_filter_confidence(filter_conditions)
            
            # 2. ì¿¼ë¦¬ ë³µì¡ë„ í‰ê°€  
            query_complexity = self._evaluate_query_complexity(processed_query)
            
            # 3. ë™ì  top_k ê³„ì‚°
            if filter_confidence > 0.8:
                # í•„í„°ê°€ ë§¤ìš° ì •í™•í•œ ê²½ìš° - ì ì€ ìˆ˜ì˜ ì •í™•í•œ ê²°ê³¼
                optimized_top_k = max(base_top_k, 8)
                search_strategy = "precision"
            elif filter_confidence > 0.6:
                # í•„í„°ê°€ ì–´ëŠ ì •ë„ ì •í™•í•œ ê²½ìš° - ê¸°ë³¸ê°’
                optimized_top_k = base_top_k
                search_strategy = "balanced"
            else:
                # í•„í„° ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° - ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬
                optimized_top_k = min(base_top_k * 2, 20)
                search_strategy = "recall"
            
            # 4. ë³µì¡í•œ ì¿¼ë¦¬ëŠ” ë” ë§ì€ ê²°ê³¼ í•„ìš”
            if query_complexity > 0.7:
                optimized_top_k = min(optimized_top_k + 5, 25)
                
            logger.debug(f"ğŸ¯ ê²€ìƒ‰ ìµœì í™”: í•„í„°ì‹ ë¢°ë„={filter_confidence:.2f}, ì¿¼ë¦¬ë³µì¡ë„={query_complexity:.2f}, top_k={base_top_k}â†’{optimized_top_k}")
            
            return optimized_top_k, search_strategy
            
        except Exception as e:
            logger.warning(f"ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ ìµœì í™” ì‹¤íŒ¨: {e}")
            return base_top_k, "standard"
    
    def _evaluate_filter_confidence(self, filters: Optional[Dict[str, Any]]) -> float:
        """
        ì ìš©ëœ í•„í„°ì˜ ì‹ ë¢°ë„ í‰ê°€
        
        Args:
            filters: ì ìš©ëœ í•„í„° ì¡°ê±´ë“¤
            
        Returns:
            float: í•„í„° ì‹ ë¢°ë„ (0.0-1.0)
        """
        if not filters:
            return 0.0
        
        confidence = 0.0
        
        # data_sourceê°€ íŠ¹ì •ë˜ì–´ ìˆìœ¼ë©´ ë†’ì€ ì‹ ë¢°ë„
        if "data_source" in filters:
            confidence += 0.4
        
        # regulation_typeì´ íŠ¹ì •ë˜ì–´ ìˆìœ¼ë©´ ë†’ì€ ì‹ ë¢°ë„  
        if "regulation_type" in filters:
            confidence += 0.3
        
        # ì œí’ˆë³„ í•„í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì‹ ë¢°ë„
        if "animal_plant_type" in filters or "product_category" in filters:
            confidence += 0.2
        
        # data_typeì€ ê¸°ë³¸ í•„í„°ì´ë¯€ë¡œ ë‚®ì€ ì‹ ë¢°ë„
        if "data_type" in filters:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _evaluate_query_complexity(self, processed_query: Dict[str, Any]) -> float:
        """
        ì¿¼ë¦¬ ë³µì¡ë„ í‰ê°€
        
        Args:
            processed_query: ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            float: ì¿¼ë¦¬ ë³µì¡ë„ (0.0-1.0)
        """
        complexity = 0.0
        
        # ì˜ë„ ë³µì¡ë„
        intent = processed_query.get("intent", {})
        if intent.get("specificity") == "êµ¬ì²´ì ":
            complexity += 0.3
        
        # í‚¤ ê°œë… ìˆ˜
        key_concepts = len(intent.get("key_concepts", []))
        complexity += min(key_concepts * 0.1, 0.3)
        
        # ì¿¼ë¦¬ ê¸¸ì´
        query_length = len(processed_query.get("normalized_query", ""))
        if query_length > 20:
            complexity += 0.2
        
        # í™•ì¥ëœ ì¿¼ë¦¬ì™€ ì›ë³¸ ì¿¼ë¦¬ ì°¨ì´
        original_len = len(processed_query.get("normalized_query", ""))
        expanded_len = len(processed_query.get("expanded_query", ""))
        if expanded_len > original_len * 1.5:
            complexity += 0.2
        
        return min(complexity, 1.0)
    
    def _execute_smart_search(self, query_embedding: List[float], 
                             top_k: int, 
                             where_condition: Optional[Dict[str, Any]],
                             search_strategy: str,
                             original_query: str) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ì „ëµì— ë”°ë¥¸ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            top_k: ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜
            where_condition: í•„í„° ì¡°ê±´
            search_strategy: ê²€ìƒ‰ ì „ëµ
            original_query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # LangChainVectorStoreëŠ” search_type ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 
            # ì „ëµì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            
            if search_strategy == "precision":
                # ì •ë°€ë„ ìš°ì„  - ìœ ì‚¬ë„ ê²€ìƒ‰ í›„ ë†’ì€ ì„ê³„ê°’ í•„í„°ë§
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=min(top_k * 2, 20),  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                    where=where_condition
                )
                # ë†’ì€ ì ìˆ˜ë§Œ ìœ ì§€ (ì •ë°€ë„ ìš°ì„ )
                filtered_results = [r for r in results if r.get("score", 0) >= 0.7]
                return filtered_results[:top_k] if filtered_results else results[:top_k]
                
            elif search_strategy == "recall":
                # ì¬í˜„ìœ¨ ìš°ì„  - ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘ í›„ ë‹¤ì–‘ì„± í™•ë³´
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=min(top_k * 3, 30),  # ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘
                    where=where_condition
                )
                # ë‹¤ì–‘ì„± ë³´ì¥ì„ ìœ„í•´ ë‹¤ë¥¸ data_sourceì—ì„œ ê²°ê³¼ ë¶„ì‚° ì„ íƒ
                return self._ensure_result_diversity(results, top_k)
                
            else:
                # ê· í˜• ì¡íŒ ê²€ìƒ‰ - ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰
                return self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    where=where_condition
                )
            
        except Exception as e:
            logger.warning(f"ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback
            return self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k,
                where=where_condition
            )
    
    def _execute_fallback_search(self, query: str, 
                               expanded_top_k: int,
                               original_filters: Optional[Dict[str, Any]],
                               processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ ì‹¤í–‰í•˜ëŠ” í™•ì¥ ê²€ìƒ‰ (ì ì§„ì  í•„í„° ì™„í™”)
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            expanded_top_k: í™•ì¥ëœ top_k
            original_filters: ì›ë³¸ í•„í„°
            processed_query: ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            List[Dict[str, Any]]: í™•ì¥ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            logger.info("ğŸ”„ í™•ì¥ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰: ì ì§„ì  í•„í„° ì™„í™”")
            
            # 1ë‹¨ê³„: regulation_type ì œê±°í•˜ê³  ì¬ê²€ìƒ‰
            relaxed_filters = original_filters.copy() if original_filters else {}
            if "regulation_type" in relaxed_filters:
                del relaxed_filters["regulation_type"]
                logger.info("ğŸ“‰ 1ë‹¨ê³„: regulation_type í•„í„° ì™„í™”")
                
                query_embedding = self.embedder.embed_text(processed_query["normalized_query"])
                where_condition = self._build_where_condition(relaxed_filters)
                
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=expanded_top_k,
                    where=where_condition
                )
                
                if len(results) >= 3:
                    return results
            
            # 2ë‹¨ê³„: data_sourceë„ ì œê±°í•˜ê³  ì¬ê²€ìƒ‰
            if "data_source" in relaxed_filters:
                del relaxed_filters["data_source"]
                logger.info("ğŸ“‰ 2ë‹¨ê³„: data_source í•„í„° ì™„í™”")
                
                where_condition = self._build_where_condition(relaxed_filters)
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=expanded_top_k,
                    where=where_condition
                )
                
                if len(results) >= 3:
                    return results
            
            # 3ë‹¨ê³„: data_typeë§Œ ìœ ì§€í•˜ê³  ì „ì²´ ê²€ìƒ‰
            minimal_filters = {"data_type": "trade_regulation"}
            logger.info("ğŸ“‰ 3ë‹¨ê³„: ìµœì†Œ í•„í„°ë¡œ ì „ì²´ ê²€ìƒ‰")
            
            where_condition = self._build_where_condition(minimal_filters)
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=expanded_top_k,
                where=where_condition
            )
            
            return results
            
        except Exception as e:
            logger.error(f"í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _evaluate_result_quality(self, results: List[Dict[str, Any]], processed_query: Dict[str, Any]) -> float:
        """
        ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            processed_query: ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            float: í’ˆì§ˆ ì ìˆ˜ (0.0-1.0)
        """
        if not results:
            return 0.0
        
        total_score = 0.0
        
        for result in results:
            score = 0.0
            metadata = result.get("metadata", {})
            
            # 1. ì¤‘ìš”ë„ ì ìˆ˜ ë°˜ì˜ (40%)
            importance = result.get("importance_score", 0.0)
            score += importance * 0.4
            
            # 2. ìœ ì‚¬ë„ ì ìˆ˜ ë°˜ì˜ (30%) 
            similarity = result.get("score", 0.0)
            score += similarity * 0.3
            
            # 3. ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ í‰ê°€ (15%)
            metadata_completeness = self._evaluate_metadata_completeness(metadata)
            score += metadata_completeness * 0.15
            
            # 4. ì»¨í…ìŠ¤íŠ¸ ë¶€ìŠ¤íŒ… ë°˜ì˜ (15%)
            context_boost = result.get("context_boost", 0.0)
            score += min(context_boost, 0.15)
            
            total_score += score
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        average_quality = total_score / len(results)
        
        # ê²°ê³¼ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        diversity_bonus = self._evaluate_result_diversity(results)
        final_quality = min(average_quality + diversity_bonus, 1.0)
        
        return final_quality
    
    def _evaluate_metadata_completeness(self, metadata: Dict[str, Any]) -> float:
        """ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ í‰ê°€"""
        essential_fields = ["data_source", "data_type"]
        important_fields = ["regulation_type", "product_name", "country"]
        optional_fields = ["hs_code", "status", "priority"]
        
        score = 0.0
        
        # í•„ìˆ˜ í•„ë“œ ì²´í¬ (60%)
        for field in essential_fields:
            if metadata.get(field):
                score += 0.3
        
        # ì¤‘ìš” í•„ë“œ ì²´í¬ (30%)
        for field in important_fields:
            if metadata.get(field):
                score += 0.1
        
        # ì„ íƒ í•„ë“œ ì²´í¬ (10%)
        for field in optional_fields:
            if metadata.get(field):
                score += 0.033
        
        return min(score, 1.0)
    
    def _evaluate_result_diversity(self, results: List[Dict[str, Any]]) -> float:
        """ê²°ê³¼ ë‹¤ì–‘ì„± í‰ê°€"""
        if len(results) < 2:
            return 0.0
        
        unique_sources = set()
        unique_types = set()
        
        for result in results:
            metadata = result.get("metadata", {})
            unique_sources.add(metadata.get("data_source", ""))
            unique_types.add(metadata.get("regulation_type", ""))
        
        # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        source_diversity = min(len(unique_sources) / len(results), 1.0) * 0.05
        type_diversity = min(len(unique_types) / len(results), 1.0) * 0.05
        
        return source_diversity + type_diversity
    
    def _execute_comprehensive_fallback_search(self, query: str, 
                                             target_top_k: int,
                                             original_filters: Optional[Dict[str, Any]],
                                             processed_query: Dict[str, Any],
                                             current_quality: float) -> List[Dict[str, Any]]:
        """
        í¬ê´„ì ì¸ í™•ì¥ ê²€ìƒ‰ ì „ëµ (í’ˆì§ˆ ê¸°ë°˜ ì ì‘í˜• ì ‘ê·¼)
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            target_top_k: ëª©í‘œ ê²°ê³¼ ìˆ˜
            original_filters: ì›ë³¸ í•„í„°
            processed_query: ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì •ë³´
            current_quality: í˜„ì¬ ê²°ê³¼ í’ˆì§ˆ
            
        Returns:
            List[Dict[str, Any]]: í™•ì¥ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            logger.info(f"ğŸ”„ í¬ê´„ì  í™•ì¥ ê²€ìƒ‰ ì‹œì‘ (í˜„ì¬ í’ˆì§ˆ: {current_quality:.2f})")
            
            # í’ˆì§ˆì— ë”°ë¥¸ í™•ì¥ ì „ëµ ì„ íƒ
            if current_quality < 0.3:
                # í’ˆì§ˆì´ ë§¤ìš° ë‚®ìŒ - ì „ë©´ì  ì¬ê²€ìƒ‰
                return self._execute_comprehensive_search(query, target_top_k, processed_query)
            elif current_quality < 0.6:
                # í’ˆì§ˆì´ ë‚®ìŒ - ì ì§„ì  í™•ì¥ + ë™ì˜ì–´ ê°•í™”
                return self._execute_progressive_expansion(query, target_top_k * 2, original_filters, processed_query)
            else:
                # í’ˆì§ˆì´ ë³´í†µ - ê´€ë ¨ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
                return self._execute_related_search_expansion(query, target_top_k, original_filters, processed_query)
            
        except Exception as e:
            logger.error(f"í¬ê´„ì  í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _execute_comprehensive_search(self, query: str, target_top_k: int, processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì „ë©´ì  ì¬ê²€ìƒ‰ - í’ˆì§ˆì´ ë§¤ìš° ë‚®ì„ ë•Œ"""
        logger.info("ğŸ“¡ ì „ë©´ì  ì¬ê²€ìƒ‰ ì‹¤í–‰")
        
        # ë™ì˜ì–´ í™•ì¥ëœ ì¿¼ë¦¬ë¡œ í•„í„° ì—†ì´ ê²€ìƒ‰
        expanded_query = processed_query.get("expanded_query", query)
        query_embedding = self.embedder.embed_text(expanded_query)
        
        # data_typeë§Œ ìœ ì§€í•˜ê³  ê´‘ë²”ìœ„ ê²€ìƒ‰
        minimal_filters = {"data_type": "trade_regulation"}
        where_condition = self._build_where_condition(minimal_filters)
        
        results = self.vector_store.search_similar(
            query_embedding=query_embedding,
            top_k=target_top_k * 2,  # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬
            where=where_condition,
            # search_type="mmr"  # ë‹¤ì–‘ì„± í™•ë³´
        )
        
        return results
    
    def _execute_progressive_expansion(self, query: str, 
                                     expanded_top_k: int,
                                     original_filters: Optional[Dict[str, Any]],
                                     processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì ì§„ì  í™•ì¥ ê²€ìƒ‰ - ë‹¨ê³„ë³„ í•„í„° ì™„í™” + ì¿¼ë¦¬ í™•ì¥"""
        logger.info("ğŸ”„ ì ì§„ì  í™•ì¥ ê²€ìƒ‰ ì‹¤í–‰")
        
        best_results = []
        query_embedding = self.embedder.embed_text(processed_query.get("normalized_query", query))
        
        # 1ë‹¨ê³„: ì¿¼ë¦¬ í™•ì¥ + ì›ë³¸ í•„í„°
        if original_filters:
            logger.info("ğŸ“ˆ 1ë‹¨ê³„: ì¿¼ë¦¬ í™•ì¥ + ì›ë³¸ í•„í„°")
            expanded_query = processed_query.get("expanded_query", query)
            expanded_embedding = self.embedder.embed_text(expanded_query)
            
            where_condition = self._build_where_condition(original_filters)
            results = self.vector_store.search_similar(
                query_embedding=expanded_embedding,
                top_k=expanded_top_k,
                where=where_condition
            )
            
            if len(results) >= 3:
                return results
            best_results.extend(results)
        
        # 2ë‹¨ê³„: ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥ ê²€ìƒ‰
        logger.info("ğŸ“ˆ 2ë‹¨ê³„: ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥")
        related_results = self._search_with_related_keywords(query, expanded_top_k, processed_query)
        best_results.extend(related_results)
        
        # 3ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í™•ì¥
        logger.info("ğŸ“ˆ 3ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í™•ì¥")
        category_results = self._search_by_inferred_category(processed_query, expanded_top_k)
        best_results.extend(category_results)
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆìˆœ ì •ë ¬
        unique_results = self._deduplicate_results(best_results)
        return unique_results[:expanded_top_k]
    
    def _search_with_related_keywords(self, query: str, top_k: int, processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•œ í™•ì¥ ê²€ìƒ‰"""
        try:
            intent = processed_query.get("intent", {})
            key_concepts = intent.get("key_concepts", [])
            
            # í‚¤ ê°œë…ë“¤ë¡œ í™•ì¥ ì¿¼ë¦¬ ìƒì„±
            if key_concepts:
                extended_query = f"{query} {' '.join(key_concepts[:3])}"  # ìƒìœ„ 3ê°œ ê°œë…ë§Œ ì‚¬ìš©
                query_embedding = self.embedder.embed_text(extended_query)
                
                # ì œí•œì  í•„í„°ë¡œ ê²€ìƒ‰
                basic_filters = {"data_type": "trade_regulation"}
                where_condition = self._build_where_condition(basic_filters)
                
                return self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    where=where_condition
                )
            
            return []
            
        except Exception as e:
            logger.warning(f"ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _search_by_inferred_category(self, processed_query: Dict[str, Any], top_k: int) -> List[Dict[str, Any]]:
        """ì¶”ë¡ ëœ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            intent = processed_query.get("intent", {})
            trade_category = intent.get("trade_category", "")
            
            if not trade_category or trade_category == "ì¼ë°˜":
                return []
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì¼ë°˜í™”ëœ ì¿¼ë¦¬ ìƒì„±
            category_queries = {
                "ìˆ˜ì¶œì œí•œ": "ìˆ˜ì¶œ ì œí•œ í’ˆëª© ê·œì œ",
                "ìˆ˜ì…ê·œì œ": "ìˆ˜ì… ê·œì œ ì œí•œ í’ˆëª©",
                "ë™ì‹ë¬¼ìˆ˜ì…ê·œì œ": "ë™ì‹ë¬¼ ìˆ˜ì… í—ˆìš© êµ­ê°€ ê·œì œ",
                "ë°˜ë¤í•‘": "ë°˜ë¤í•‘ ê´€ì„¸ ê·œì œ",
                "ì„¸ì´í”„ê°€ë“œ": "ì„¸ì´í”„ê°€ë“œ ê¸´ê¸‰ìˆ˜ì…ì œí•œ"
            }
            
            category_query = category_queries.get(trade_category)
            if category_query:
                query_embedding = self.embedder.embed_text(category_query)
                
                return self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    where={"data_type": {"$eq": "trade_regulation"}}
                )
            
            return []
            
        except Exception as e:
            logger.warning(f"ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _execute_related_search_expansion(self, query: str,
                                        target_top_k: int,
                                        original_filters: Optional[Dict[str, Any]],
                                        processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ê²€ìƒ‰ìœ¼ë¡œ ê²°ê³¼ ë³´ì™„ - í’ˆì§ˆì´ ë³´í†µì¼ ë•Œ"""
        logger.info("ğŸ”— ê´€ë ¨ ê²€ìƒ‰ í™•ì¥ ì‹¤í–‰")
        
        results = []
        
        # í˜„ì¬ ì¿¼ë¦¬ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        key_concepts = processed_query.get("intent", {}).get("key_concepts", [])
        
        # ê° í‚¤ ê°œë…ë³„ë¡œ ê´€ë ¨ ê²€ìƒ‰ ìˆ˜í–‰
        for concept in key_concepts[:2]:  # ìƒìœ„ 2ê°œ ê°œë…ë§Œ
            concept_query = f"{concept} ê´€ë ¨ ê·œì œ ì •ë³´"
            query_embedding = self.embedder.embed_text(concept_query)
            
            # ì™„í™”ëœ í•„í„° ì‚¬ìš©
            relaxed_filters = original_filters.copy() if original_filters else {}
            if "regulation_type" in relaxed_filters:
                del relaxed_filters["regulation_type"]
            
            where_condition = self._build_where_condition(relaxed_filters)
            
            concept_results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=target_top_k // 2,
                where=where_condition
            )
            
            # ê´€ë ¨ì„± í‘œì‹œ
            for result in concept_results:
                result["expansion_type"] = "concept_related"
                result["related_concept"] = concept
            
            results.extend(concept_results)
        
        return self._deduplicate_results(results)
    
    
    def _ensure_result_diversity(self, results: List[Dict[str, Any]], target_count: int) -> List[Dict[str, Any]]:
        """
        ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ì„ ë³´ì¥í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ê²°ê³¼ ì„ íƒ
        
        Args:
            results: ì „ì²´ ê²€ìƒ‰ ê²°ê³¼
            target_count: ëª©í‘œ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ë‹¤ì–‘ì„±ì´ ë³´ì¥ëœ ê²°ê³¼
        """
        if not results or len(results) <= target_count:
            return results[:target_count]
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
        source_groups = {}
        for result in results:
            data_source = result.get("metadata", {}).get("data_source", "unknown")
            if data_source not in source_groups:
                source_groups[data_source] = []
            source_groups[data_source].append(result)
        
        # ê° ì†ŒìŠ¤ì—ì„œ ê· ë“±í•˜ê²Œ ì„ íƒ
        selected_results = []
        sources = list(source_groups.keys())
        
        # ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ì„ íƒ
        source_index = 0
        while len(selected_results) < target_count and any(source_groups.values()):
            current_source = sources[source_index % len(sources)]
            
            if source_groups[current_source]:
                selected_results.append(source_groups[current_source].pop(0))
            
            source_index += 1
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if source_index > target_count * len(sources):
                break
        
        return selected_results[:target_count]
    
