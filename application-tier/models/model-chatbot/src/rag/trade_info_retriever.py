"""
Trade Information Retriever Module

ë¬´ì—­ ì •ë³´ CSV ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  HSì½”ë“œ, êµ­ê°€ë³„ ê·œì œ ë“±ì„ í™œìš©í•œ í™•ì¥ ê²€ìƒ‰ ê¸°ëŠ¥
"""

import logging
from typing import List, Dict, Any, Optional
import re
from .embeddings import LangChainEmbedder
from .vector_store import LangChainVectorStore
from .query_normalizer import QueryNormalizer, AdvancedQueryProcessor, TradeQueryNormalizer

logger = logging.getLogger(__name__)


class TradeInfoRetriever:
    """ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ ë° ê´€ë ¨ ë°ì´í„° ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self,
                 embedder: Optional[LangChainEmbedder] = None,
                 vector_store: Optional[LangChainVectorStore] = None,
                 query_normalizer: Optional[QueryNormalizer] = None,
                 collection_name: str = "trade_info_collection"):
        """
        ì´ˆê¸°í™” (LangChain í‘œì¤€ ì‚¬ìš©)
        
        Args:
            embedder (Optional[LangChainEmbedder]): LangChain ì„ë² ë”© ìƒì„±ê¸°
            vector_store (Optional[LangChainVectorStore]): LangChain ë²¡í„° ì €ì¥ì†Œ
            query_normalizer (Optional[QueryNormalizer]): ì¿¼ë¦¬ ì •ê·œí™”ê¸°
            collection_name (str): ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„
        """
        # ê¸°ë³¸ê°’ ì„¤ì • (ìë™ ì´ˆê¸°í™”)
        if embedder is None:
            self.embedder = LangChainEmbedder()  # ê¸°ë³¸ text-embedding-3-small ì‚¬ìš©
        else:
            self.embedder = embedder
            
        if vector_store is None:
            self.vector_store = LangChainVectorStore(
                collection_name=collection_name,
                embedding_function=self.embedder.embeddings
            )
        else:
            self.vector_store = vector_store
            
        if query_normalizer is None:
            # TradeQueryNormalizer ì‚¬ìš©ìœ¼ë¡œ ë™ì‹ë¬¼ ìˆ˜ì… ì˜ë„ ê°ì§€ ê°œì„ 
            self.query_normalizer = TradeQueryNormalizer()
        else:
            self.query_normalizer = query_normalizer
            
        self.query_processor = AdvancedQueryProcessor(self.query_normalizer)
        self.collection_name = collection_name
        
        # ë‚´ë¶€ ë¬¸ì„œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._document_cache = {}
        
        # HSì½”ë“œ íŒ¨í„´ ë§¤ì¹­ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        self.hs_code_pattern = re.compile(r'\b\d{4,10}\b')  # 4-10ìë¦¬ ìˆ«ìë¥¼ HSì½”ë“œë¡œ ì¸ì‹
        
        logger.info(f"LangChain TradeInfoRetriever initialized with collection: {collection_name}")
    
    def search_trade_info(self, 
                         raw_query: str, 
                         top_k: int = 5,
                         include_related: bool = True,
                         expand_with_synonyms: bool = True,
                         similarity_threshold: float = 0.0,
                         filter_by: Optional[Dict[str, Any]] = None,
                         search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ë¬´ì—­ ì •ë³´ ê²€ìƒ‰
        
        Args:
            raw_query (str): ì›ë³¸ ì‚¬ìš©ì ì§ˆì˜
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            include_related (bool): ê´€ë ¨ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            expand_with_synonyms (bool): ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
            filter_by (Optional[Dict[str, Any]]): í•„í„°ë§ ì¡°ê±´
            search_context (Optional[Dict[str, Any]]): ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. LLM ê¸°ë°˜ ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± (í•µì‹¬ ê°œì„  ê¸°ëŠ¥)
            llm_filters = self._generate_llm_metadata_filters(raw_query, search_context)
            
            # 2. ê¸°ì¡´ í•„í„°ì™€ LLM í•„í„° ë³‘í•©
            combined_filters = self._merge_filters(filter_by, llm_filters)
            
            # ë³‘í•©ëœ í•„í„°ë¥¼ ì‹¤ì œ ê²€ìƒ‰ì— ì‚¬ìš©
            if combined_filters:
                if not filter_by:
                    filter_by = {}
                filter_by.update(combined_filters)
            
            # 3. ì¿¼ë¦¬ ì •ê·œí™” ë° í™•ì¥ (TradeQueryNormalizer ì‚¬ìš©)
            normalized_query = self.query_normalizer.normalize(raw_query, search_context)
            if expand_with_synonyms:
                expanded_query = self.query_normalizer.expand_query_with_synonyms(normalized_query)
            else:
                expanded_query = normalized_query
            
            logger.info(f"ğŸ” Query: '{raw_query}' â†’ '{expanded_query}'")
            logger.info(f"ğŸ·ï¸ LLM Filters: {llm_filters}")
            
            # 4. HSì½”ë“œ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            hs_codes = self._extract_hs_codes(raw_query)
            if hs_codes:
                logger.info(f"ğŸ”¢ ê°ì§€ëœ HSì½”ë“œ: {hs_codes}")
                return self._search_by_hs_code(hs_codes, top_k, include_related)
            
            # 2.5. ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ë³„ ì²˜ë¦¬
            if search_context and search_context.get("domain_hints"):
                logger.info(f"ğŸ¯ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì ìš©: {search_context.get('domain_hints')}")
                # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ë³´ê°•
                context_keywords = search_context.get("boost_keywords", [])
                if context_keywords:
                    enhanced_query = f"{raw_query} {' '.join(context_keywords)}"
                    processed_query = self.query_processor.process_complex_query(enhanced_query)
                else:
                    processed_query = self.query_processor.process_complex_query(raw_query)
            else:
                processed_query = self.query_processor.process_complex_query(raw_query)
            
            # 3. ì‚¬ìš©í•  ì¿¼ë¦¬ ê²°ì • (ì´ë¯¸ ìœ„ì—ì„œ processed_query ìƒì„±ë¨)
            if expand_with_synonyms:
                search_query = processed_query["expanded_query"]
            else:
                search_query = processed_query["normalized_query"]
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            # 4. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedder.embed_text(search_query)
            
            # 5. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            logger.info(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘ (top_k: {top_k})")
            
            # í•„í„°ë§ ì¡°ê±´ ì ìš©
            where_condition = self._build_where_condition(filter_by)
            
            primary_results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k,
                where=where_condition
            )
            logger.info(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(primary_results)}ê°œ")
            
            # 6. ê´€ë ¨ ë°ì´í„° í™•ì¥ ê²€ìƒ‰
            if include_related:
                expanded_results = self._expand_with_related_info(primary_results, top_k)
            else:
                expanded_results = primary_results
            
            # 7. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì •ë ¬ (search_context í¬í•¨)
            final_results = self._post_process_results(expanded_results, processed_query, search_context)
            
            # 8. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            if similarity_threshold > 0.0:
                filtered_results = [result for result in final_results 
                                  if result.get("similarity", 0) >= similarity_threshold]
                logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold}ë¡œ í•„í„°ë§: {len(final_results)} â†’ {len(filtered_results)}ê°œ")
                final_results = filtered_results
            
            logger.info(f"âœ… {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìš”ì²­ëœ top_k: {top_k})")
            return final_results
            
        except Exception as e:
            logger.error(f"ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_country(self, country: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • êµ­ê°€ì˜ ë¬´ì—­ ê·œì œ ì •ë³´ ê²€ìƒ‰
        
        Args:
            country (str): êµ­ê°€ëª…
            top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # êµ­ê°€ëª… ì •ê·œí™”
            normalized_country = self._normalize_country_name(country)
            
            # ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰
            where_condition = {"country": {"$eq": normalized_country}}
            
            # êµ­ê°€ ê´€ë ¨ ì¿¼ë¦¬ë¡œ ì„ë² ë”© ê²€ìƒ‰
            query = f"{normalized_country} ë¬´ì—­ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì… ì œí•œ"
            query_embedding = self.embedder.embed_text(query)
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k,
                where=where_condition
            )
            
            logger.info(f"ğŸŒ {country} ê´€ë ¨ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"êµ­ê°€ë³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_product_category(self, category: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ ê²€ìƒ‰
        
        Args:
            category (str): ì œí’ˆ ì¹´í…Œê³ ë¦¬
            top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ì¿¼ë¦¬
            query = f"{category} ì œí’ˆ ë¬´ì—­ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì…"
            query_embedding = self.embedder.embed_text(query)
            
            # ì œí’ˆ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            where_condition = {"product_category": {"$eq": category}}
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k,
                where=where_condition
            )
            
            logger.info(f"ğŸ“¦ {category} ì¹´í…Œê³ ë¦¬ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_hs_codes(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ HSì½”ë“œ ì¶”ì¶œ"""
        matches = self.hs_code_pattern.findall(query)
        # 4ìë¦¬ ì´ìƒì˜ ìˆ«ìë§Œ HSì½”ë“œë¡œ ê°„ì£¼
        hs_codes = [match for match in matches if len(match) >= 4]
        return hs_codes
    
    def _search_by_hs_code(self, hs_codes: List[str], top_k: int, include_related: bool) -> List[Dict[str, Any]]:
        """HSì½”ë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        all_results = []
        
        for hs_code in hs_codes:
            try:
                # ì •í™•í•œ HSì½”ë“œ ë§¤ì¹­
                where_condition = {"hs_code": {"$eq": hs_code}}
                
                # HSì½”ë“œë¡œ ì¿¼ë¦¬ ìƒì„±
                query = f"HSì½”ë“œ {hs_code} ì œí’ˆ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì…"
                query_embedding = self.embedder.embed_text(query)
                
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    where=where_condition
                )
                
                # HSì½”ë“œ ë§¤ì¹­ ì •ë³´ ì¶”ê°€
                for result in results:
                    result["hs_code_match"] = hs_code
                    result["match_type"] = "exact_hs_code"
                
                all_results.extend(results)
                
                # ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ (ì• 6ìë¦¬ê°€ ê°™ì€ ê²½ìš°)
                if include_related and len(hs_code) >= 6:
                    related_results = self._search_related_hs_codes(hs_code[:6], top_k // 2)
                    all_results.extend(related_results)
                    
            except Exception as e:
                logger.error(f"HSì½”ë“œ {hs_code} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_results = self._deduplicate_results(all_results)
        return unique_results[:top_k]
    
    def _search_related_hs_codes(self, hs_prefix: str, top_k: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ (ì•ìë¦¬ê°€ ê°™ì€ ì œí’ˆë“¤)"""
        try:
            # HSì½”ë“œ ì•ìë¦¬ê°€ ê°™ì€ ì œí’ˆë“¤ ê²€ìƒ‰
            query = f"HSì½”ë“œ {hs_prefix} ê´€ë ¨ ì œí’ˆ"
            query_embedding = self.embedder.embed_text(query)
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k
            )
            
            # ê´€ë ¨ ì œí’ˆ ì •ë³´ ì¶”ê°€
            for result in results:
                result["match_type"] = "related_hs_code"
                result["hs_prefix_match"] = hs_prefix
            
            return results
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _normalize_country_name(self, country: str) -> str:
        """êµ­ê°€ëª… ì •ê·œí™”"""
        # ì¼ë°˜ì ì¸ êµ­ê°€ëª… ë§¤í•‘
        country_mapping = {
            "ë¯¸êµ­": "ë¯¸êµ­", "USA": "ë¯¸êµ­", "United States": "ë¯¸êµ­",
            "ì¤‘êµ­": "ì¤‘êµ­", "China": "ì¤‘êµ­",
            "ì¼ë³¸": "ì¼ë³¸", "Japan": "ì¼ë³¸",
            "ë…ì¼": "ë…ì¼", "Germany": "ë…ì¼",
            "ì˜êµ­": "ì˜êµ­", "UK": "ì˜êµ­", "United Kingdom": "ì˜êµ­",
            "í”„ë‘ìŠ¤": "í”„ë‘ìŠ¤", "France": "í”„ë‘ìŠ¤",
            "ì´íƒˆë¦¬ì•„": "ì´íƒˆë¦¬ì•„", "Italy": "ì´íƒˆë¦¬ì•„",
            "ìºë‚˜ë‹¤": "ìºë‚˜ë‹¤", "Canada": "ìºë‚˜ë‹¤",
            "í˜¸ì£¼": "í˜¸ì£¼", "Australia": "í˜¸ì£¼",
            "ì¸ë„": "ì¸ë„", "India": "ì¸ë„"
        }
        
        return country_mapping.get(country, country)
    
    def _build_where_condition(self, filter_by: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """í•„í„°ë§ ì¡°ê±´ êµ¬ì„± (data_type ì§€ì› ê°•í™”)"""
        if not filter_by:
            return None
        
        # ì§€ì›ë˜ëŠ” ë©”íƒ€ë°ì´í„° í•„ë“œ í™•ì¥
        supported_fields = [
            "category", "country", "regulation_type", "status", "hs_code", "product_category", 
            "data_type", "data_source", "consultation_type", "case_number", "product_name",
            "affected_country", "regulating_country", "animal_plant_type", "management_number"
        ]
        
        conditions = []
        for key, value in filter_by.items():
            if key in supported_fields:
                conditions.append({key: {"$eq": value}})
        
        if not conditions:
            return None
        elif len(conditions) == 1:
            return conditions[0]
        else:
            # ì—¬ëŸ¬ ì¡°ê±´ì„ $andë¡œ ê²°í•©
            return {"$and": conditions}
    
    def _expand_with_related_info(self, results: List[Dict[str, Any]], max_total: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ì •ë³´ë¡œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥"""
        expanded_results = results.copy()
        
        # ê¸°ì¡´ ê²°ê³¼ì—ì„œ HSì½”ë“œë‚˜ êµ­ê°€ ì •ë³´ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        seen_ids = {result.get("id", "") for result in results}
        
        for result in results[:3]:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ í™•ì¥
            metadata = result.get("metadata", {})
            
            # HSì½”ë“œ ê¸°ë°˜ í™•ì¥
            hs_code = metadata.get("hs_code", "")
            if hs_code and len(hs_code) >= 6:
                related_hs = self._search_related_hs_codes(hs_code[:6], 2)
                for related in related_hs:
                    if related.get("id", "") not in seen_ids:
                        related["reference_info"] = {"is_related": True, "related_to": result.get("id", "")}
                        expanded_results.append(related)
                        seen_ids.add(related.get("id", ""))
            
            # êµ­ê°€ ê¸°ë°˜ í™•ì¥
            country = metadata.get("country", "")
            if country:
                related_country = self.search_by_country(country, 2)
                for related in related_country:
                    if related.get("id", "") not in seen_ids:
                        related["reference_info"] = {"is_related": True, "related_to": result.get("id", "")}
                        expanded_results.append(related)
                        seen_ids.add(related.get("id", ""))
        
        return expanded_results[:max_total]
    
    def _post_process_results(self, results: List[Dict[str, Any]], processed_query: Dict[str, Any], search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ (search_context ê¸°ë°˜ ë¶€ìŠ¤íŒ… í¬í•¨)"""
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        for result in results:
            importance_score = self._calculate_importance_score(result, processed_query)
            result["importance_score"] = importance_score
            
            # search_context ê¸°ë°˜ ë¶€ìŠ¤íŒ…
            if search_context:
                context_boost = self._apply_search_context_boost(result, search_context)
                result["context_boost"] = context_boost
                result["importance_score"] += context_boost
        
        # ì¤‘ìš”ë„ì™€ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: (
            x.get("importance_score", 0) * 0.3 + 
            x.get("similarity", 0) * 0.7
        ), reverse=True)
        
        return results
    
    def _apply_search_context_boost(self, result: Dict[str, Any], search_context: Dict[str, Any]) -> float:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìˆ˜ ê³„ì‚°"""
        boost_score = 0.0
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        
        # ìš°ì„ ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ ë¶€ìŠ¤íŒ…
        priority_sources = search_context.get("priority_data_sources", [])
        data_source = metadata.get("data_source", "")
        if data_source in priority_sources:
            boost_score += 0.3
            result["boosted"] = True
            result["boost_reason"] = f"ìš°ì„ ìˆœìœ„ ë°ì´í„°ì†ŒìŠ¤: {data_source}"
        
        # ë„ë©”ì¸ íŒíŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
        domain_hints = search_context.get("domain_hints", [])
        for hint in domain_hints:
            if hint in content.lower() or hint in str(metadata).lower():
                boost_score += 0.2
                result["boosted"] = True
                result["boost_reason"] = result.get("boost_reason", "") + f" ë„ë©”ì¸ë§¤ì¹­: {hint}"
        
        # ë¶€ìŠ¤íŒ… í‚¤ì›Œë“œ ë§¤ì¹­
        boost_keywords = search_context.get("boost_keywords", [])
        for keyword in boost_keywords:
            if keyword in content or keyword in str(metadata):
                boost_score += 0.1
        
        return min(boost_score, 0.5)  # ìµœëŒ€ 0.5ë¡œ ì œí•œ
    
    def _calculate_importance_score(self, result: Dict[str, Any], processed_query: Dict[str, Any]) -> float:
        """ë¬¸ì„œ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        metadata = result.get("metadata", {})
        
        # ë™ì‹ë¬¼ ìˆ˜ì… ê·œì œ ë°ì´í„° ìµœìš°ì„  ì²˜ë¦¬
        data_source = metadata.get("data_source", "")
        if data_source == "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­":
            score += 0.5  # ë™ì‹ë¬¼ ê·œì œëŠ” ìµœê³  ì¤‘ìš”ë„
            
            # ë™ì‹ë¬¼ ê´€ë ¨ ì¶”ê°€ ê°€ì¤‘ì¹˜
            priority = metadata.get("priority", 0)
            regulation_intensity = metadata.get("regulation_intensity", "")
            
            # ğŸ”§ íƒ€ì… ì•ˆì „ì„±: priorityë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            try:
                priority_int = int(priority) if priority else 0
            except (ValueError, TypeError):
                priority_int = 0
            
            if priority_int >= 2:  # ë†’ì€ ìš°ì„ ìˆœìœ„
                score += 0.2
            if regulation_intensity == "high":  # ë†’ì€ ê·œì œ ê°•ë„
                score += 0.15
            if metadata.get("has_global_prohibition", False):  # ì „ì—­ ê¸ˆì§€
                score += 0.1
                
        elif "ê·œì œ" in data_source:
            score += 0.3  # ì¼ë°˜ ê·œì œ ì •ë³´
        
        # ê·œì œ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        regulation_type = metadata.get("regulation_type", "")
        if regulation_type == "import_regulations":
            score += 0.25  # ìˆ˜ì… ê·œì œëŠ” ë†’ì€ ì¤‘ìš”ë„
        elif "restrictions" in regulation_type:
            score += 0.2
        elif "prohibitions" in regulation_type:
            score += 0.3  # ê¸ˆì§€ í’ˆëª©ì€ ë§¤ìš° ì¤‘ìš”
        
        # HSì½”ë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜
        if result.get("match_type") == "exact_hs_code":
            score += 0.4
        elif result.get("match_type") == "related_hs_code":
            score += 0.2
        
        # ìƒíƒœë³„ ê°€ì¤‘ì¹˜
        is_active = metadata.get("is_active", False)
        if is_active:
            score += 0.15  # í™œì„± ê·œì œëŠ” ì¤‘ìš”ë„ ë†’ìŒ
            
        status = metadata.get("status", "")
        if "ê·œì œì¤‘" in status:
            score += 0.2  # ì‹¤ì œ ì ìš© ì¤‘ì¸ ê·œì œ
        elif "ì¡°ì‚¬ì¤‘" in status:
            score += 0.1
        
        # ë°ì´í„° íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        data_type = metadata.get("data_type", "")
        if data_type == "trade_regulation":
            score += 0.1  # ë¬´ì—­ ê·œì œ ë°ì´í„°
        
        return min(score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            result_id = result.get("id", "")
            if result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)
        
        return unique_results
    
    def get_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            stats = self.vector_store.get_collection_stats()
            stats["retriever_type"] = "LangChain_TradeInfoRetriever"
            return stats
        except Exception as e:
            logger.error(f"í†µê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _generate_llm_metadata_filters(self, query: str, search_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        LLMì„ ì‚¬ìš©í•´ì„œ ì¿¼ë¦¬ì— ë§ëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ìƒì„±
        
        Args:
            query (str): ì‚¬ìš©ì ì§ˆì˜
            search_context (Optional[Dict[str, Any]]): ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìƒì„±ëœ ë©”íƒ€ë°ì´í„° í•„í„°
        """
        try:
            # 1. ì˜ë„ ë¶„ì„ìœ¼ë¡œ í•„í„° íŒíŠ¸ ì¶”ì¶œ
            intent_info = self.query_normalizer.extract_intent(query)
            
            filters = {}
            
            # 2. ë°ì´í„° íƒ€ì… ê²°ì •
            if intent_info.get("intent_type") == "ë™ì‹ë¬¼ê²€ì—­":
                filters["data_type"] = "trade_regulation"
                filters["data_source"] = "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"
            elif intent_info.get("trade_category") == "ë™ì‹ë¬¼ìˆ˜ì…ê·œì œ":
                filters["data_type"] = "trade_regulation"
                filters["data_source"] = "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"
            elif search_context and search_context.get("agent_type") == "regulation_agent":
                filters["data_type"] = "trade_regulation"
            elif search_context and search_context.get("agent_type") == "consultation_agent":
                filters["data_type"] = "consultation_case"
            
            # 3. ë™ë¬¼/ì‹ë¬¼ íƒ€ì… ê²°ì •
            key_concepts = intent_info.get("key_concepts", [])
            detected_items = []
            
            # ë†ì‚°ë¬¼/ì‹ë¬¼ í‚¤ì›Œë“œ ê°ì§€
            plant_keywords = ['ì•„ë³´ì¹´ë„', 'ë°”ë‚˜ë‚˜', 'ë”¸ê¸°', 'ì‚¬ê³¼', 'ë°°', 'í¬ë„', 'ì²´ë¦¬', 'ë³µìˆ­ì•„', 
                            'ì˜¤ë Œì§€', 'ë ˆëª¬', 'ë¼ì„', 'í‚¤ìœ„', 'ë§ê³ ', 'íŒŒì¸ì• í”Œ', 'ë©œë¡ ', 'ìˆ˜ë°•',
                            'ìŒ€', 'ë°€', 'ì˜¥ìˆ˜ìˆ˜', 'ì½©', 'íŒ¥', 'ë…¹ë‘', 'ê°ì', 'ê³ êµ¬ë§ˆ', 'ì–‘íŒŒ', 'ë§ˆëŠ˜']
            
            # ë™ë¬¼/ì¶•ì‚°ë¬¼ í‚¤ì›Œë“œ ê°ì§€  
            animal_keywords = ['ë¼ì§€ê³ ê¸°', 'ì†Œê³ ê¸°', 'ë‹­ê³ ê¸°', 'ì˜¤ë¦¬ê³ ê¸°', 'ì–‘ê³ ê¸°', 'ìƒì„ ', 'ìƒˆìš°', 'ê²Œ',
                             'ìš°ìœ ', 'ì¹˜ì¦ˆ', 'ë²„í„°', 'ìš”êµ¬ë¥´íŠ¸', 'ê³„ë€', 'ê¿€']
            
            for concept in key_concepts:
                if concept in plant_keywords:
                    detected_items.append(concept)
                    filters["animal_plant_type"] = "ì‹ë¬¼"
                    filters["product_category"] = "ì‹ë¬¼"
                elif concept in animal_keywords:
                    detected_items.append(concept)
                    filters["animal_plant_type"] = "ë™ë¬¼"
                    filters["product_category"] = "ë™ë¬¼"
            
            # 4. ì œí’ˆëª… ì •í™• ë§¤ì¹­ í•„í„°
            if detected_items:
                # ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì œí’ˆëª…ìœ¼ë¡œ ì •í™• ë§¤ì¹­ ì‹œë„
                primary_product = detected_items[0]
                logger.info(f"ğŸ¯ ì œí’ˆëª… ì •í™• ë§¤ì¹­ í•„í„°: {primary_product}")
            
            # 5. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ í•„í„°
            if search_context:
                priority_sources = search_context.get("priority_data_sources", [])
                if priority_sources:
                    # ì²« ë²ˆì§¸ ìš°ì„ ìˆœìœ„ ì†ŒìŠ¤ ì‚¬ìš©
                    filters["data_source"] = priority_sources[0]
                
                domain_hints = search_context.get("domain_hints", [])
                if "animal_plant_import" in domain_hints:
                    filters["data_type"] = "trade_regulation"
                    filters["regulation_type"] = "import_regulations"
            
            logger.debug(f"ğŸ¤– LLM ìƒì„± í•„í„°: {filters}")
            return filters
            
        except Exception as e:
            logger.error(f"LLM ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _merge_filters(self, user_filters: Optional[Dict[str, Any]], llm_filters: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì œê³µ í•„í„°ì™€ LLM ìƒì„± í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©
        
        Args:
            user_filters: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•œ í•„í„°
            llm_filters: LLMì´ ìƒì„±í•œ í•„í„°
            
        Returns:
            Dict[str, Any]: ë³‘í•©ëœ í•„í„°
        """
        if not user_filters:
            return llm_filters
        
        if not llm_filters:
            return user_filters or {}
        
        # ì‚¬ìš©ì í•„í„°ê°€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§€ë˜, LLM í•„í„°ë¡œ ë³´ì™„
        merged = llm_filters.copy()
        merged.update(user_filters)  # ì‚¬ìš©ì í•„í„°ë¡œ ë®ì–´ì“°ê¸°
        
        return merged
    
    def get_langchain_retriever(self, 
                               search_type: str = "similarity",
                               search_kwargs: Optional[Dict[str, Any]] = None):
        """
        LangChain í‘œì¤€ Retriever ê°ì²´ ë°˜í™˜ (ì²´ì¸ ì—°ê²°ìš©)
        
        Args:
            search_type (str): ê²€ìƒ‰ íƒ€ì… ("similarity", "mmr", "similarity_score_threshold")
            search_kwargs (Optional[Dict[str, Any]]): ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            
        Returns:
            Retriever: LangChain Retriever ê°ì²´
        """
        if search_kwargs is None:
            search_kwargs = {"k": 5}
        
        return self.vector_store.get_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )
    
