#!/usr/bin/env python3
"""
Document Processing Script

ê´€ì„¸ë²• ë¬¸ì„œ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
Chunking+Loader.ipynb ë…¸íŠ¸ë¶ì˜ ê¸°ëŠ¥ì„ ëª…ë ¹í–‰ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

Usage:
    python scripts/process_documents.py --all
    python scripts/process_documents.py --law ê´€ì„¸ë²•
    python scripts/process_documents.py --law ê´€ì„¸ë²• --output custom_output.json
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.config import (
    load_config, get_law_data_paths, get_output_paths, get_pdf_data_paths, get_pdf_output_paths, 
    get_csv_data_paths, get_csv_output_paths, get_consultation_case_paths, validate_environment
)
from src.utils.file_utils import load_json_data, save_processed_documents
from src.data_processing.law_document_loader import CustomsLawLoader
from src.data_processing.trade_info_csv_loader import CSVDocumentLoader
from src.data_processing.pdf_processor import PDFDocumentProcessor
from src.data_processing.law_chunking_utils import analyze_chunking_results, print_sample_chunks, validate_chunk_integrity
from src.data_processing.pdf_chunking_utils import validate_pdf_chunks, analyze_pdf_processing_results, get_pdf_chunk_statistics

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def process_single_law(law_name: str, law_data: Dict[str, Any], output_path: Optional[Path] = None) -> List[Dict[str, Any]]:
    """ë‹¨ì¼ ë²•ë ¹ ë¬¸ì„œ ì²˜ë¦¬
    
    Args:
        law_name (str): ë²•ë ¹ëª…
        law_data (Dict[str, Any]): ë²•ë ¹ JSON ë°ì´í„°
        output_path (Optional[Path]): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        List[Dict[str, Any]]: ì²˜ë¦¬ëœ ë¬¸ì„œ ì²­í¬ë“¤
    """
    print(f"\nğŸ“„ {law_name} ì²˜ë¦¬ ì‹œì‘...")
    
    # CustomsLawLoaderë¡œ ë¬¸ì„œ ì²˜ë¦¬
    loader = CustomsLawLoader(law_data)
    processed_documents = loader.load()
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š {law_name} ì²­í‚¹ ê²°ê³¼ ë¶„ì„:")
    analyze_chunking_results(processed_documents)
    
    # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
    integrity_issues = validate_chunk_integrity(processed_documents)
    if integrity_issues:
        print(f"\nâš ï¸ ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œ ë°œê²¬:")
        for issue in integrity_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ ì¶œë ¥
            print(f"  - {issue}")
        if len(integrity_issues) > 5:
            print(f"  ... ì´ {len(integrity_issues)}ê°œ ë¬¸ì œ ë°œê²¬")
    else:
        print("âœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
    
    # ê²°ê³¼ ì €ì¥
    if output_path:
        success = save_processed_documents(processed_documents, str(output_path))
        if not success:
            logger.error(f"Failed to save {law_name} processed documents")
            return []
    
    print(f"âœ… {law_name} ì²˜ë¦¬ ì™„ë£Œ: {len(processed_documents)}ê°œ ì²­í¬ ìƒì„±")
    return processed_documents


def process_all_laws(show_samples: bool = False) -> Dict[str, List[Dict[str, Any]]]:
    """ëª¨ë“  ë²•ë ¹ ë¬¸ì„œ ì²˜ë¦¬
    
    Args:
        show_samples (bool): ìƒ˜í”Œ ì²­í¬ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        Dict[str, List[Dict[str, Any]]]: ë²•ë ¹ë³„ ì²˜ë¦¬ëœ ë¬¸ì„œë“¤
    """
    print("ğŸš€ ì „ì²´ ê´€ì„¸ë²• ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë° ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    data_paths = get_law_data_paths()
    output_paths = get_output_paths()
    
    all_results = {}
    total_chunks = 0
    
    for law_name, data_path in data_paths.items():
        if not data_path.exists():
            print(f"âš ï¸ {law_name} ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
            continue
        
        # JSON ë°ì´í„° ë¡œë“œ
        law_data = load_json_data(str(data_path))
        if law_data is None:
            print(f"âŒ {law_name} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            continue
        
        # ë¬¸ì„œ ì²˜ë¦¬
        output_path = output_paths.get(law_name)
        processed_docs = process_single_law(law_name, law_data, output_path)
        
        if processed_docs:
            all_results[law_name] = processed_docs
            total_chunks += len(processed_docs)
            
            # ìƒ˜í”Œ ì¶œë ¥ (ìš”ì²­ì‹œ)
            if show_samples:
                print(f"\nğŸ“‹ {law_name} ìƒ˜í”Œ ì²­í¬:")
                print_sample_chunks(processed_docs, num_samples=1)
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì²˜ë¦¬ëœ ë²•ë ¹ ìˆ˜: {len(all_results)}")
    print(f"ì´ ìƒì„±ëœ ì²­í¬ ìˆ˜: {total_chunks}")
    
    # ë²•ë ¹ë³„ ì²­í¬ ìˆ˜ ì¶œë ¥
    for law_name, docs in all_results.items():
        print(f"  - {law_name}: {len(docs)}ê°œ ì²­í¬")
    
    return all_results


def process_single_pdf(pdf_name: str, pdf_path: Path, output_path: Optional[Path] = None) -> List[Dict[str, Any]]:
    """ë‹¨ì¼ PDF ë¬¸ì„œ ì²˜ë¦¬ (JSONL ë°©ì‹)
    
    Args:
        pdf_name (str): PDF ë¬¸ì„œëª…
        pdf_path (Path): PDF íŒŒì¼ ê²½ë¡œ
        output_path (Optional[Path]): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (.jsonl)
    
    Returns:
        List[Dict[str, Any]]: ì²˜ë¦¬ëœ ë¬¸ì„œ ì²­í¬ë“¤
    """
    print(f"\nğŸ“„ {pdf_name} PDF ì²˜ë¦¬ ì‹œì‘...")
    
    # PDFDocumentProcessorë¡œ ë¬¸ì„œ ì²˜ë¦¬ ë° JSONL ì €ì¥
    processor = PDFDocumentProcessor(pdf_path, pdf_name)
    
    if output_path:
        # ì²˜ë¦¬ í›„ ë°”ë¡œ JSONLë¡œ ì €ì¥
        processed_documents, save_success = processor.process_and_save_jsonl(output_path)
        if not save_success:
            logger.error(f"Failed to save {pdf_name} processed documents to JSONL")
            return []
    else:
        # ì €ì¥ ì—†ì´ ì²˜ë¦¬ë§Œ
        processed_documents = processor.process()
    
    if not processed_documents:
        print(f"âŒ {pdf_name} PDF ì²˜ë¦¬ ì‹¤íŒ¨")
        return []
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š {pdf_name} PDF ì²­í‚¹ ê²°ê³¼ ë¶„ì„:")
    analysis = analyze_pdf_processing_results(processed_documents)
    
    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    overview = analysis.get("overview", {})
    print(f"  ì´ ì²­í¬ ìˆ˜: {overview.get('total_chunks', 0)}")
    print(f"  í‰ê·  ì²­í¬ í¬ê¸°: {overview.get('average_chunk_size', 0)} ë¬¸ì")
    print(f"  ì†ŒìŠ¤ ë¬¸ì„œ ìˆ˜: {len(overview.get('source_documents', []))}")
    
    # ë¬¸ì„œ ìœ í˜•ë³„ í†µê³„
    doc_types = analysis.get("document_types", {})
    if doc_types:
        print("  ë¬¸ì„œ ìœ í˜•ë³„ ì²­í¬ ìˆ˜:")
        for doc_type, stats in doc_types.items():
            print(f"    - {doc_type}: {stats.get('count', 0)}ê°œ")
    
    # PDF ì²­í‚¹ ê²€ì¦
    validation_result = validate_pdf_chunks(processed_documents)
    if validation_result["is_valid"]:
        print("âœ… PDF ì²­í‚¹ ê²€ì¦ í†µê³¼")
    else:
        print("âš ï¸ PDF ì²­í‚¹ ê²€ì¦ ë¬¸ì œ ë°œê²¬:")
        for issue in validation_result["issues"][:3]:  # ìµœëŒ€ 3ê°œë§Œ ì¶œë ¥
            print(f"  - {issue}")
        if len(validation_result["issues"]) > 3:
            print(f"  ... ì´ {len(validation_result['issues'])}ê°œ ë¬¸ì œ ë°œê²¬")
    
    # ì²˜ë¦¬ ìš”ì•½ ì¶œë ¥
    summary = processor.get_processing_summary()
    print(f"\nğŸ“‹ {pdf_name} ì²˜ë¦¬ ìš”ì•½:")
    print(f"  ë¬¸ì„œ ìœ í˜•: {summary.get('document_type', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
    print(f"  ì¶”ì¶œ ë°©ë²•: {list(summary.get('extraction_methods', {}).keys())}")
    print(f"  HSì½”ë“œ ë°œê²¬: {summary.get('hs_codes_found', 0)}ê°œ")
    print(f"  ë²•ë ¹ ì°¸ì¡° ë°œê²¬: {summary.get('law_references_found', 0)}ê°œ")
    
    if output_path:
        print(f"ğŸ’¾ JSONL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    print(f"âœ… {pdf_name} PDF ì²˜ë¦¬ ì™„ë£Œ: {len(processed_documents)}ê°œ ì²­í¬ ìƒì„±")
    return processed_documents


def process_all_pdfs(show_samples: bool = False) -> Dict[str, List[Dict[str, Any]]]:
    """ëª¨ë“  PDF ë¬¸ì„œ ì²˜ë¦¬
    
    Args:
        show_samples (bool): ìƒ˜í”Œ ì²­í¬ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        Dict[str, List[Dict[str, Any]]]: PDFë³„ ì²˜ë¦¬ëœ ë¬¸ì„œë“¤
    """
    print("ğŸš€ ì „ì²´ PDF ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
    
    # PDF íŒŒì¼ ê²½ë¡œ ë° ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    pdf_paths = get_pdf_data_paths()
    output_paths = get_pdf_output_paths()
    
    all_results = {}
    total_chunks = 0
    
    for pdf_name, pdf_path in pdf_paths.items():
        if not pdf_path.exists():
            print(f"âš ï¸ {pdf_name} PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            continue
        
        # PDF ë¬¸ì„œ ì²˜ë¦¬
        output_path = output_paths.get(pdf_name)
        processed_docs = process_single_pdf(pdf_name, pdf_path, output_path)
        
        if processed_docs:
            all_results[pdf_name] = processed_docs
            total_chunks += len(processed_docs)
            
            # ìƒ˜í”Œ ì¶œë ¥ (ìš”ì²­ì‹œ)
            if show_samples:
                print(f"\nğŸ“‹ {pdf_name} ìƒ˜í”Œ ì²­í¬:")
                # PDF ì²­í¬ëŠ” êµ¬ì¡°ê°€ ë‹¤ë¥´ë¯€ë¡œ ê°„ë‹¨íˆ ì¶œë ¥
                for i, chunk in enumerate(processed_docs[:2]):  # ìµœëŒ€ 2ê°œ
                    print(f"  ì²­í¬ {i+1}:")
                    print(f"    ì¸ë±ìŠ¤: {chunk.get('index', 'N/A')}")
                    print(f"    ì œëª©: {chunk.get('title', 'N/A')}")
                    content = chunk.get('content', '')
                    print(f"    ë‚´ìš©: {content[:100]}{'...' if len(content) > 100 else ''}")
                    print()
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ‰ ì „ì²´ PDF ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì²˜ë¦¬ëœ PDF ìˆ˜: {len(all_results)}")
    print(f"ì´ ìƒì„±ëœ ì²­í¬ ìˆ˜: {total_chunks}")
    
    # PDFë³„ ì²­í¬ ìˆ˜ ì¶œë ¥
    for pdf_name, docs in all_results.items():
        print(f"  - {pdf_name}: {len(docs)}ê°œ ì²­í¬")
    
    return all_results


def process_single_csv(csv_name: str, csv_path: Path, output_path: Optional[Path] = None) -> List[Dict[str, Any]]:
    """ë‹¨ì¼ CSV íŒŒì¼ ì²˜ë¦¬ (ì¼ë°˜ ì •ë³´ìš©)
    
    Args:
        csv_name (str): CSV íŒŒì¼ëª…
        csv_path (Path): CSV íŒŒì¼ ê²½ë¡œ
        output_path (Optional[Path]): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        List[Dict[str, Any]]: ì²˜ë¦¬ëœ ë¬¸ì„œ ì²­í¬ë“¤
    """
    print(f"\nğŸ“„ {csv_name} CSV ì²˜ë¦¬ ì‹œì‘...")
    
    # CSVDocumentLoaderë¡œ ë¬¸ì„œ ì²˜ë¦¬
    try:
        loader = CSVDocumentLoader(str(csv_path))
        processed_documents = loader.load()
    except Exception as e:
        print(f"âŒ {csv_name} CSV ë¡œë”© ì‹¤íŒ¨: {e}")
        return []
    
    if not processed_documents:
        print(f"âŒ {csv_name} CSV ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š {csv_name} CSV ì²­í‚¹ ê²°ê³¼ ë¶„ì„:")
    stats = loader.get_statistics()
    print(f"  ì´ ì²­í¬ ìˆ˜: {stats.get('total_documents', 0)}")
    print(f"  í‰ê·  ë‚´ìš© ê¸¸ì´: {stats.get('average_content_length', 0):.1f} ë¬¸ì")
    print(f"  CSV ìœ í˜•: {stats.get('csv_type', 'Unknown')}")
    
    # ì¹´í…Œê³ ë¦¬ ë¶„í¬
    category_dist = stats.get('category_distribution', {})
    if category_dist:
        print("  ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for category, count in category_dist.items():
            print(f"    - {category}: {count}ê°œ")
    
    # ê²°ê³¼ ì €ì¥
    if output_path:
        success = save_processed_documents(processed_documents, str(output_path))
        if not success:
            logger.error(f"Failed to save {csv_name} processed documents")
            return []
    
    print(f"âœ… {csv_name} CSV ì²˜ë¦¬ ì™„ë£Œ: {len(processed_documents)}ê°œ ì²­í¬ ìƒì„±")
    return processed_documents


def process_all_csvs(show_samples: bool = False) -> Dict[str, List[Dict[str, Any]]]:
    """ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬ (ì¼ë°˜ ì •ë³´ìš©)
    
    Args:
        show_samples (bool): ìƒ˜í”Œ ì²­í¬ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        Dict[str, List[Dict[str, Any]]]: CSVë³„ ì²˜ë¦¬ëœ ë¬¸ì„œë“¤
    """
    print("ğŸš€ ì „ì²´ ë¬´ì—­ ì •ë³´ CSV ì²˜ë¦¬ ì‹œì‘...")
    
    # CSV íŒŒì¼ ê²½ë¡œ ë° ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    csv_paths = get_csv_data_paths()
    output_paths = get_csv_output_paths()
    
    all_results = {}
    total_chunks = 0
    
    for csv_name, csv_path in csv_paths.items():
        if not csv_path.exists():
            print(f"âš ï¸ {csv_name} CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            continue
        
        # CSV ë¬¸ì„œ ì²˜ë¦¬
        output_path = output_paths.get(csv_name)
        processed_docs = process_single_csv(csv_name, csv_path, output_path)
        
        if processed_docs:
            all_results[csv_name] = processed_docs
            total_chunks += len(processed_docs)
            
            # ìƒ˜í”Œ ì¶œë ¥ (ìš”ì²­ì‹œ)
            if show_samples:
                print(f"\nğŸ“‹ {csv_name} ìƒ˜í”Œ ì²­í¬:")
                for i, chunk in enumerate(processed_docs[:2]):  # ìµœëŒ€ 2ê°œ
                    print(f"  ì²­í¬ {i+1}:")
                    print(f"    ì¸ë±ìŠ¤: {chunk.get('index', 'N/A')}")
                    print(f"    ì œëª©: {chunk.get('title', 'N/A')}")
                    content = chunk.get('content', '')
                    print(f"    ë‚´ìš©: {content[:150]}{'...' if len(content) > 150 else ''}")
                    
                    # ë©”íƒ€ë°ì´í„° ì •ë³´
                    metadata = chunk.get('metadata', {})
                    if metadata.get('hs_code'):
                        print(f"    HSì½”ë“œ: {metadata.get('hs_code')}")
                    if metadata.get('country'):
                        print(f"    êµ­ê°€: {metadata.get('country')}")
                    if metadata.get('regulation_type'):
                        print(f"    ê·œì œìœ í˜•: {metadata.get('regulation_type')}")
                    print()
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ‰ ì „ì²´ CSV ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì²˜ë¦¬ëœ CSV ìˆ˜: {len(all_results)}")
    print(f"ì´ ìƒì„±ëœ ì²­í¬ ìˆ˜: {total_chunks}")
    
    # CSVë³„ ì²­í¬ ìˆ˜ ì¶œë ¥
    for csv_name, docs in all_results.items():
        print(f"  - {csv_name}: {len(docs)}ê°œ ì²­í¬")
    
    return all_results


def process_consultation_cases(output_path: Optional[Path] = None, show_samples: bool = False) -> List[Dict[str, Any]]:
    """ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ PDF ì²˜ë¦¬ (JSON í˜•ì‹, RAG í˜¸í™˜)
    
    Args:
        output_path (Optional[Path]): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        show_samples (bool): ìƒ˜í”Œ ì²­í¬ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        List[Dict[str, Any]]: ì²˜ë¦¬ëœ ìƒë‹´ ì‚¬ë¡€ ì²­í¬ë“¤
    """
    print("\nğŸ“„ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ PDF ì²˜ë¦¬ ì‹œì‘...")
    
    # ìƒë‹´ ì‚¬ë¡€ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    consultation_paths = get_consultation_case_paths()
    input_pdf = consultation_paths["input_pdf"]
    
    if not input_pdf.exists():
        print(f"âš ï¸ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_pdf}")
        return []
    
    # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    if not output_path:
        output_path = consultation_paths["output_json"]
    
    # PDFDocumentProcessorë¡œ ë¬¸ì„œ ì²˜ë¦¬ (JSON ë°©ì‹ ì‚¬ìš©)
    processor = PDFDocumentProcessor(input_pdf, "ê´€ì„¸í–‰ì •_ë¯¼ì›ìƒë‹´_ì‚¬ë¡€ì§‘")
    
    # ì²˜ë¦¬ í›„ ë°”ë¡œ JSONìœ¼ë¡œ ì €ì¥ (RAG í˜¸í™˜)
    processed_documents, save_success = processor.process_and_save_json(output_path)
    
    if not processed_documents:
        print("âŒ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬ ì‹¤íŒ¨")
        return []
    
    if not save_success:
        print("âš ï¸ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²­í‚¹ ê²°ê³¼ ë¶„ì„:")
    summary = processor.get_processing_summary()
    
    # ê¸°ë³¸ í†µê³„
    print(f"  ì´ ì‚¬ë¡€ ìˆ˜: {summary.get('total_chunks', 0)}")
    print(f"  í‰ê·  ì‚¬ë¡€ í¬ê¸°: {summary.get('average_chunk_size', 0)} ë¬¸ì")
    print(f"  ë¬¸ì„œ ìœ í˜•: {summary.get('document_type', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    categories = summary.get('categories', {})
    if categories:
        print("  ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ë¡€ ìˆ˜:")
        for category, count in categories.items():
            print(f"    - {category}: {count}ê°œ")
    
    # ìƒë‹´ ìœ í˜•ë³„ í†µê³„
    consultation_types = summary.get('consultation_types', {})
    if consultation_types:
        print("  ìƒë‹´ ìœ í˜•ë³„ ì‚¬ë¡€ ìˆ˜:")
        for c_type, count in consultation_types.items():
            print(f"    - {c_type}: {count}ê°œ")
    
    # ë‚´ìš© ì™„ì„±ë„
    completeness = summary.get('content_completeness', {})
    if completeness:
        print(f"  ë‚´ìš© ì™„ì„±ë„:")
        print(f"    - ì§ˆë¬¸ í¬í•¨: {completeness.get('cases_with_questions', 0)}ê°œ")
        print(f"    - ë‹µë³€ í¬í•¨: {completeness.get('cases_with_answers', 0)}ê°œ")
        print(f"    - ë²•ë ¹ ì°¸ì¡° í¬í•¨: {completeness.get('cases_with_law_references', 0)}ê°œ")
        print(f"    - ì™„ì„±ë„ ë¹„ìœ¨: {completeness.get('completeness_rate', 0):.1%}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    if show_samples and processed_documents:
        print(f"\nğŸ“‹ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ìƒ˜í”Œ ì²­í¬:")
        for i, chunk in enumerate(processed_documents[:2]):  # ìµœëŒ€ 2ê°œ
            print(f"  ì‚¬ë¡€ {i+1}:")
            print(f"    ì¸ë±ìŠ¤: {chunk.get('index', 'N/A')}")
            print(f"    ì œëª©: {chunk.get('title', 'N/A')}")
            content = chunk.get('content', '')
            print(f"    ë‚´ìš©: {content[:200]}{'...' if len(content) > 200 else ''}")
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´
            metadata = chunk.get('metadata', {})
            if metadata.get('category'):
                print(f"    ì¹´í…Œê³ ë¦¬: {metadata.get('category')}")
            if metadata.get('consultation_type'):
                print(f"    ìƒë‹´ ìœ í˜•: {metadata.get('consultation_type')}")
            if metadata.get('keywords'):
                keywords = metadata.get('keywords', [])[:5]  # ìµœëŒ€ 5ê°œ
                print(f"    í‚¤ì›Œë“œ: {', '.join(keywords)}")
            print()
    
    if save_success:
        print(f"ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    print(f"âœ… ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_documents)}ê°œ ì‚¬ë¡€ ìƒì„±")
    return processed_documents


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ê´€ì„¸ë²• ë¬¸ì„œ, PDF, CSV ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ë²•ë ¹ ì²˜ë¦¬
  python scripts/process_documents.py --all                    # ëª¨ë“  ë²•ë ¹ ì²˜ë¦¬
  python scripts/process_documents.py --law ê´€ì„¸ë²•              # íŠ¹ì • ë²•ë ¹ë§Œ ì²˜ë¦¬
  python scripts/process_documents.py --all --samples         # ìƒ˜í”Œ ì¶œë ¥ í¬í•¨
  python scripts/process_documents.py --law ê´€ì„¸ë²• --output custom.json  # ì»¤ìŠ¤í…€ ì¶œë ¥ íŒŒì¼
  
  # PDF ì²˜ë¦¬ (JSONL í˜•ì‹)
  python scripts/process_documents.py --pdf-all               # ëª¨ë“  PDF ì²˜ë¦¬
  python scripts/process_documents.py --pdf ìˆ˜ì…ì œí•œí’ˆëª©       # íŠ¹ì • PDFë§Œ ì²˜ë¦¬
  python scripts/process_documents.py --pdf-all --samples     # PDF ìƒ˜í”Œ ì¶œë ¥ í¬í•¨
  python scripts/process_documents.py --pdf ìˆ˜ì…ì œí•œí’ˆëª© --output custom.jsonl  # ì»¤ìŠ¤í…€ JSONL ì¶œë ¥ íŒŒì¼
  
  # ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬ (JSON í˜•ì‹, RAG í˜¸í™˜)
  python scripts/process_documents.py --consultation          # ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬
  python scripts/process_documents.py --consultation --samples # ìƒë‹´ì‚¬ë¡€ ìƒ˜í”Œ ì¶œë ¥ í¬í•¨
  python scripts/process_documents.py --consultation --output consultation_cases.json  # ì»¤ìŠ¤í…€ JSON ì¶œë ¥
  
  # CSV ì²˜ë¦¬ (ì¼ë°˜ ì •ë³´ìš©)
  python scripts/process_documents.py --csv-all               # ëª¨ë“  CSV ì²˜ë¦¬
  python scripts/process_documents.py --csv ìˆ˜ì…ê·œì œDB_ì „ì²´    # íŠ¹ì • CSVë§Œ ì²˜ë¦¬
  python scripts/process_documents.py --csv-all --samples     # CSV ìƒ˜í”Œ ì¶œë ¥ í¬í•¨
  python scripts/process_documents.py --csv ìˆ˜ì¶œì œí•œí’ˆëª© --output custom.json  # ì»¤ìŠ¤í…€ JSON ì¶œë ¥ íŒŒì¼
        """
    )
    
    parser.add_argument(
        "--all",
        action="store_true",
        help="ëª¨ë“  ê´€ì„¸ë²• ë¬¸ì„œ ì²˜ë¦¬"
    )
    
    parser.add_argument(
        "--law",
        type=str,
        choices=["ê´€ì„¸ë²•", "ê´€ì„¸ë²•ì‹œí–‰ë ¹", "ê´€ì„¸ë²•ì‹œí–‰ê·œì¹™"],
        help="ì²˜ë¦¬í•  íŠ¹ì • ë²•ë ¹ ì„ íƒ"
    )
    
    parser.add_argument(
        "--pdf-all",
        action="store_true",
        help="ëª¨ë“  PDF ë¬¸ì„œ ì²˜ë¦¬"
    )
    
    parser.add_argument(
        "--pdf",
        type=str,
        choices=["ìˆ˜ì…ì œí•œí’ˆëª©", "ìˆ˜ì¶œì œí•œí’ˆëª©", "ìˆ˜ì¶œê¸ˆì§€í’ˆëª©", "ë¬´ì—­í†µê³„ë¶€í˜¸", "ìˆ˜ì…ì‹ ê³ ì„œì‘ì„±ìš”ë ¹", "ìˆ˜ì¶œì‹ ê³ ì„œì‘ì„±ìš”ë ¹"],
        help="ì²˜ë¦¬í•  íŠ¹ì • PDF ë¬¸ì„œ ì„ íƒ"
    )
    
    parser.add_argument(
        "--consultation",
        action="store_true",
        help="ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ PDF ì²˜ë¦¬ (JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥, RAG ì‹œìŠ¤í…œ í˜¸í™˜)"
    )
    
    parser.add_argument(
        "--csv-all",
        action="store_true",
        help="ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬ (ì¼ë°˜ ì •ë³´ìš©)"
    )
    
    parser.add_argument(
        "--csv",
        type=str,
        choices=["ìˆ˜ì…ê·œì œDB_ì „ì²´", "ìˆ˜ì¶œì œí•œí’ˆëª©", "ìˆ˜ì…ì œí•œí’ˆëª©", "ìˆ˜ì¶œê¸ˆì§€í’ˆëª©", "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"],
        help="ì²˜ë¦¬í•  íŠ¹ì • CSV íŒŒì¼ ì„ íƒ"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (íŠ¹ì • ë²•ë ¹/PDF/CSV ì²˜ë¦¬ì‹œë§Œ ì‚¬ìš©, PDFëŠ” .jsonl, CSVëŠ” .json í™•ì¥ì ê¶Œì¥)"
    )
    
    parser.add_argument(
        "--samples",
        action="store_true",
        help="ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥"
    )
    
    parser.add_argument(
        "--validate-env",
        action="store_true",
        help="í™˜ê²½ ì„¤ì •ë§Œ ê²€ì¦í•˜ê³  ì¢…ë£Œ"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        print("ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸...")
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        config = load_config()
        
        # í™˜ê²½ ê²€ì¦
        if not validate_environment():
            print("âŒ í™˜ê²½ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return 1
        
        # í™˜ê²½ ê²€ì¦ë§Œ í•˜ê³  ì¢…ë£Œ
        if args.validate_env:
            print("âœ… í™˜ê²½ ì„¤ì • ê²€ì¦ ì™„ë£Œ")
            return 0
        
        # ì²˜ë¦¬ ì˜µì…˜ í™•ì¸
        if not any([args.all, args.law, args.pdf_all, args.pdf, args.consultation, args.csv_all, args.csv]):
            print("âŒ ì²˜ë¦¬ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”: --all, --law, --pdf-all, --pdf, --consultation, --csv-all, --csv")
            parser.print_help()
            return 1
        
        # ë™ì‹œ ì²˜ë¦¬ ë°©ì§€
        law_options = args.all or args.law
        pdf_options = args.pdf_all or args.pdf
        consultation_options = args.consultation
        csv_options = args.csv_all or args.csv
        
        active_options = sum([bool(law_options), bool(pdf_options), bool(consultation_options), bool(csv_options)])
        if active_options > 1:
            print("âŒ ë²•ë ¹, PDF, ë¯¼ì›ìƒë‹´, CSV ì²˜ë¦¬ëŠ” ë™ì‹œì— ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ë‚˜ì”© ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return 1
        
        # ëª¨ë“  ë²•ë ¹ ì²˜ë¦¬
        if args.all:
            results = process_all_laws(show_samples=args.samples)
            if not results:
                print("âŒ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 1
        
        # íŠ¹ì • ë²•ë ¹ ì²˜ë¦¬
        elif args.law:
            data_paths = get_law_data_paths()
            output_paths = get_output_paths()
            
            data_path = data_paths[args.law]
            if not data_path.exists():
                print(f"âŒ {args.law} ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
                return 1
            
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            if args.output:
                output_path = Path(args.output)
            else:
                output_path = output_paths[args.law]
            
            # JSON ë°ì´í„° ë¡œë“œ
            law_data = load_json_data(str(data_path))
            if law_data is None:
                print(f"âŒ {args.law} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
                return 1
            
            # ë¬¸ì„œ ì²˜ë¦¬
            processed_docs = process_single_law(args.law, law_data, output_path)
            if not processed_docs:
                print(f"âŒ {args.law} ì²˜ë¦¬ ì‹¤íŒ¨")
                return 1
            
            # ìƒ˜í”Œ ì¶œë ¥
            if args.samples:
                print(f"\nğŸ“‹ {args.law} ìƒ˜í”Œ ì²­í¬:")
                print_sample_chunks(processed_docs, num_samples=2)
        
        # ëª¨ë“  PDF ì²˜ë¦¬
        elif args.pdf_all:
            results = process_all_pdfs(show_samples=args.samples)
            if not results:
                print("âŒ ì²˜ë¦¬ëœ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 1
        
        # íŠ¹ì • PDF ì²˜ë¦¬
        elif args.pdf:
            pdf_paths = get_pdf_data_paths()
            output_paths = get_pdf_output_paths()
            
            pdf_path = pdf_paths[args.pdf]
            if not pdf_path.exists():
                print(f"âŒ {args.pdf} PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                return 1
            
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            if args.output:
                output_path = Path(args.output)
            else:
                output_path = output_paths[args.pdf]
            
            # PDF ë¬¸ì„œ ì²˜ë¦¬
            processed_docs = process_single_pdf(args.pdf, pdf_path, output_path)
            if not processed_docs:
                print(f"âŒ {args.pdf} PDF ì²˜ë¦¬ ì‹¤íŒ¨")
                return 1
            
            # ìƒ˜í”Œ ì¶œë ¥
            if args.samples:
                print(f"\nğŸ“‹ {args.pdf} ìƒ˜í”Œ ì²­í¬:")
                for i, chunk in enumerate(processed_docs[:2]):  # ìµœëŒ€ 2ê°œ
                    print(f"  ì²­í¬ {i+1}:")
                    print(f"    ì¸ë±ìŠ¤: {chunk.get('index', 'N/A')}")
                    print(f"    ì œëª©: {chunk.get('title', 'N/A')}")
                    content = chunk.get('content', '')
                    print(f"    ë‚´ìš©: {content[:200]}{'...' if len(content) > 200 else ''}")
                    print()
        
        # ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬
        elif args.consultation:
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            if args.output:
                output_path = Path(args.output)
            else:
                output_path = None  # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            
            # ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬
            processed_docs = process_consultation_cases(output_path, show_samples=args.samples)
            if not processed_docs:
                print("âŒ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ì§‘ ì²˜ë¦¬ ì‹¤íŒ¨")
                return 1
        
        # ëª¨ë“  CSV ì²˜ë¦¬
        elif args.csv_all:
            results = process_all_csvs(show_samples=args.samples)
            if not results:
                print("âŒ ì²˜ë¦¬ëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return 1
        
        # íŠ¹ì • CSV ì²˜ë¦¬
        elif args.csv:
            csv_paths = get_csv_data_paths()
            output_paths = get_csv_output_paths()
            
            csv_path = csv_paths[args.csv]
            if not csv_path.exists():
                print(f"âŒ {args.csv} CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
                return 1
            
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            if args.output:
                output_path = Path(args.output)
            else:
                output_path = output_paths[args.csv]
            
            # CSV ë¬¸ì„œ ì²˜ë¦¬
            processed_docs = process_single_csv(args.csv, csv_path, output_path)
            if not processed_docs:
                print(f"âŒ {args.csv} CSV ì²˜ë¦¬ ì‹¤íŒ¨")
                return 1
            
            # ìƒ˜í”Œ ì¶œë ¥
            if args.samples:
                print(f"\nğŸ“‹ {args.csv} ìƒ˜í”Œ ì²­í¬:")
                for i, chunk in enumerate(processed_docs[:2]):  # ìµœëŒ€ 2ê°œ
                    print(f"  ì²­í¬ {i+1}:")
                    print(f"    ì¸ë±ìŠ¤: {chunk.get('index', 'N/A')}")
                    print(f"    ì œëª©: {chunk.get('title', 'N/A')}")
                    content = chunk.get('content', '')
                    print(f"    ë‚´ìš©: {content[:200]}{'...' if len(content) > 200 else ''}")
                    
                    # ë©”íƒ€ë°ì´í„° ì •ë³´
                    metadata = chunk.get('metadata', {})
                    if metadata.get('hs_code'):
                        print(f"    HSì½”ë“œ: {metadata.get('hs_code')}")
                    if metadata.get('country'):
                        print(f"    êµ­ê°€: {metadata.get('country')}")
                    if metadata.get('regulation_type'):
                        print(f"    ê·œì œìœ í˜•: {metadata.get('regulation_type')}")
                    print()
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)