"""
Customs Law Agent Module

ê´€ì„¸ë²• ì¡°ë¬¸ ì „ë¬¸ ì—ì´ì „íŠ¸ - ë²•ë ¹ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í†µí•©
"""

import logging
import openai
from typing import List, Dict, Any, Optional, Tuple
import json
import os
import re
from datetime import datetime

logger = logging.getLogger(__name__)


class ConversationMemory:
    """ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, max_history: int = 10):
        """
        ì´ˆê¸°í™”
        
        Args:
            max_history (int): ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ìˆ˜
        """
        self.max_history = max_history
        self.messages = []
        self.context_documents = []  # ëŒ€í™”ì—ì„œ ì°¸ì¡°ëœ ë¬¸ì„œë“¤
        
    def add_user_message(self, message: str) -> None:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€"""
        self.messages.append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        self._trim_history()
    
    def add_assistant_message(self, message: str, source_documents: List[Dict] = None) -> None:
        """ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€"""
        self.messages.append({
            "role": "assistant", 
            "content": message,
            "timestamp": datetime.now().isoformat(),
            "source_documents": source_documents or []
        })
        
        # ì°¸ì¡°ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        if source_documents:
            for doc in source_documents:
                if doc not in self.context_documents:
                    self.context_documents.append(doc)
        
        self._trim_history()
    
    def get_conversation_history(self, include_timestamps: bool = False) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        if include_timestamps:
            return self.messages.copy()
        else:
            return [{"role": msg["role"], "content": msg["content"]} for msg in self.messages]
    
    def get_recent_context(self, num_turns: int = 3) -> List[Dict]:
        """ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        recent_messages = self.messages[-num_turns*2:] if self.messages else []
        return [{"role": msg["role"], "content": msg["content"]} for msg in recent_messages]
    
    def clear_history(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.messages.clear()
        self.context_documents.clear()
    
    def _trim_history(self) -> None:
        """ëŒ€í™” ê¸°ë¡ í¬ê¸° ì œí•œ"""
        if len(self.messages) > self.max_history:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ëŒ€í™”ë§Œ ì œê±°
            system_messages = [msg for msg in self.messages if msg["role"] == "system"]
            other_messages = [msg for msg in self.messages if msg["role"] != "system"]
            
            # ìµœê·¼ ëŒ€í™”ë§Œ ìœ ì§€
            trimmed_other = other_messages[-(self.max_history - len(system_messages)):]
            self.messages = system_messages + trimmed_other


class CustomsLawRetriever:
    """ê´€ì„¸ë²• ì¡°ë¬¸ ê²€ìƒ‰ ë° ë‚´ë¶€ ì°¸ì¡° ì¶”ì  í´ë˜ìŠ¤ (í†µí•©)"""
    
    def __init__(self,
                 embedder=None,
                 vector_store=None,
                 query_normalizer=None,
                 collection_name: str = "law_collection"):
        """
        ì´ˆê¸°í™”
        
        Args:
            embedder: ì„ë² ë”© ìƒì„±ê¸°
            vector_store: ë²¡í„° ì €ì¥ì†Œ
            query_normalizer: ì¿¼ë¦¬ ì •ê·œí™”ê¸°
            collection_name (str): ì»¬ë ‰ì…˜ ì´ë¦„
        """
        # í†µí•©ëœ import ì‹œìŠ¤í…œ ì‚¬ìš©
        from ..utils.embeddings import LangChainEmbedder
        from ..utils.db_connect import LangChainVectorStore
        # Query normalizer removed - using basic string processing
        
        # LangChain êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
        if embedder is None:
            self.embedder = LangChainEmbedder()
        else:
            self.embedder = embedder
            
        if vector_store is None:
            self.vector_store = LangChainVectorStore(collection_name=collection_name)
        else:
            self.vector_store = vector_store
            
        if query_normalizer is None:
            from ..utils.query_normalizer import get_query_normalizer
            self.query_normalizer = get_query_normalizer("law")
        else:
            self.query_normalizer = query_normalizer
            
        # ë‚´ë¶€ ë¬¸ì„œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._document_cache = {}
        
        logger.info("CustomsLawRetriever initialized")
    
    def search_similar_laws(self, 
                           raw_query: str, 
                           top_k: int = 5,
                           include_references: bool = True,
                           expand_with_synonyms: bool = True,
                           similarity_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ ë²•ë¥  ì¡°ë¬¸ ê²€ìƒ‰
        
        Args:
            raw_query (str): ì›ë³¸ ì‚¬ìš©ì ì§ˆì˜
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            include_references (bool): ë‚´ë¶€ ì°¸ì¡° ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            expand_with_synonyms (bool): ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # Simple query processing
            search_query = raw_query.strip()
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            # 3. LangChain ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            logger.info(f"ğŸ” LangChain ë²¡í„° ê²€ìƒ‰ ì‹œì‘ (top_k: {top_k})")
            primary_results = self.vector_store.search_similar(
                query_text=search_query,
                top_k=top_k
            )
            logger.info(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(primary_results)}ê°œ")
            
            # 4. ë‚´ë¶€ ì°¸ì¡° í™•ì¥ ê²€ìƒ‰
            if include_references:
                expanded_results = self._expand_with_references(primary_results, top_k)
            else:
                expanded_results = primary_results
            
            # 5. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì •ë ¬
            final_results = self._post_process_results(expanded_results, processed_query)
            
            # 6. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            if similarity_threshold > 0.0:
                filtered_results = [result for result in final_results 
                                  if result.get("similarity", 0) >= similarity_threshold]
                logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold}ë¡œ í•„í„°ë§: {len(final_results)} â†’ {len(filtered_results)}ê°œ")
                final_results = filtered_results
            
            logger.info(f"âœ… {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìš”ì²­ëœ top_k: {top_k})")
            return final_results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _expand_with_references(self, 
                              primary_results: List[Dict[str, Any]], 
                              max_total: int) -> List[Dict[str, Any]]:
        """ë‚´ë¶€ ì°¸ì¡°ë¥¼ í™œìš©í•œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        expanded_results = primary_results.copy()
        seen_ids = {result["id"] for result in primary_results}
        
        # ìƒìœ„ ê²°ê³¼ë“¤ì˜ ë‚´ë¶€ ì°¸ì¡° ë”°ë¼ê°€ê¸°
        for result in primary_results:
            try:
                related_articles = self._get_related_articles(result)
                
                for related in related_articles:
                    if related["id"] not in seen_ids and len(expanded_results) < max_total:
                        related["reference_boost"] = thresholds["reference_boost"]
                        related["referenced_from"] = result["id"]
                        expanded_results.append(related)
                        seen_ids.add(related["id"])
                        
            except Exception as e:
                logger.warning(f"ì°¸ì¡° í™•ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return expanded_results
    
    def _get_related_articles(self, document: Dict[str, Any]) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œì™€ ê´€ë ¨ëœ ì¡°ë¬¸ë“¤ ì¡°íšŒ (ë‚´ë¶€ ì°¸ì¡° ê¸°ë°˜)"""
        related_articles = []
        
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚´ë¶€ ì°¸ì¡° ì •ë³´ ì¶”ì¶œ
            metadata = document.get("metadata", {})
            
            # JSON ë¬¸ìì—´ë¡œ ì €ì¥ëœ ë‚´ë¶€ ì°¸ì¡° ì •ë³´ íŒŒì‹±
            internal_refs_raw = metadata.get("internal_law_references", "{}")
            if isinstance(internal_refs_raw, str):
                internal_refs = json.loads(internal_refs_raw)
            else:
                internal_refs = internal_refs_raw
            
            # ê° ì°¸ì¡° ìœ í˜•ë³„ë¡œ ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰
            for ref_type, ref_list in internal_refs.items():
                if not ref_list:
                    continue
                
                for ref in ref_list:
                    related_doc = self._search_by_article_reference(ref)
                    if related_doc and related_doc not in related_articles:
                        related_doc["reference_type"] = ref_type
                        related_articles.append(related_doc)
            
            logger.debug(f"ë°œê²¬ëœ ê´€ë ¨ ì¡°ë¬¸: {len(related_articles)}ê°œ")
            return related_articles
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ì¡°ë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _search_by_article_reference(self, article_ref: str) -> Optional[Dict[str, Any]]:
        """ì¡°ë¬¸ ì°¸ì¡°ë¥¼ í†µí•œ ì§ì ‘ ê²€ìƒ‰"""
        try:
            # ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰
            results = self.vector_store.search_similar(
                query_text="ë²•ë ¹ ì¡°ë¬¸",
                top_k=200
            )
            
            # ì •í™•í•œ ë§¤ì¹˜ ë˜ëŠ” ë¶€ë¶„ ë§¤ì¹˜ ì°¾ê¸°
            exact_matches = []
            partial_matches = []
            
            for result in results:
                result_index = result.get("index", "") or result.get("metadata", {}).get("index", "")
                
                if result_index == article_ref:
                    exact_matches.append(result)
                elif article_ref in result_index or result_index in article_ref:
                    partial_matches.append(result)
            
            if exact_matches:
                return exact_matches[0]
            elif partial_matches:
                return partial_matches[0]
            
            return None
            
        except Exception as e:
            logger.error(f"ì¡°ë¬¸ ì°¸ì¡° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _post_process_results(self, 
                            results: List[Dict[str, Any]], 
                            processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ë° ìµœì í™”"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        # 1. ì ìˆ˜ ì¬ê³„ì‚°
        for result in results:
            base_score = result.get("similarity", 0)
            reference_boost = result.get("reference_boost", 0)
            intent_score = self._calculate_intent_score(result, processed_query["intent"])
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = base_score + reference_boost + intent_score * thresholds["intent_weight"]
            result["final_score"] = final_score
        
        # 2. ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x.get("final_score", 0), reverse=True)
        
        # 3. ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in sorted_results:
            # ë‹¤ì¤‘ ê²½ë¡œì—ì„œ indexì™€ subtitle ì¶”ì¶œ
            index = (result.get("index") or 
                    result.get("metadata", {}).get("index") or "")
            subtitle = (result.get("subtitle") or 
                       result.get("metadata", {}).get("subtitle") or "")
            
            formatted_result = {
                "id": result["id"],
                "content": result["content"],
                "metadata": result["metadata"],
                "index": index,
                "subtitle": subtitle,
                "similarity": result.get("similarity", 0),
                "final_score": result.get("final_score", 0),
                "reference_info": {
                    "is_referenced": result.get("reference_boost", 0) > 0,
                    "referenced_from": result.get("referenced_from"),
                    "reference_type": result.get("reference_type")
                }
            }
            formatted_results.append(formatted_result)
        
        return formatted_results
    
    def _calculate_intent_score(self, result: Dict[str, Any], intent: Dict[str, Any]) -> float:
        """ì˜ë„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        score = 0.0
        content = result.get("content", "").lower()
        
        # í•µì‹¬ ê°œë… ë§¤ì¹­
        key_concepts = intent.get("key_concepts", [])
        for concept in key_concepts:
            if concept.lower() in content:
                score += thresholds["concept_score"]
        
        # ë²•ë ¹ ì˜ì—­ ë§¤ì¹­
        law_area = intent.get("law_area", "")
        if law_area != "ì¼ë°˜":
            area_keywords = {
                "ìˆ˜ì…": ["ìˆ˜ì…", "ë°˜ì…", "ë“¤ì—¬ì˜¤"],
                "ìˆ˜ì¶œ": ["ìˆ˜ì¶œ", "ë°˜ì¶œ", "ë‚´ë³´ë‚´"],
                "í†µê´€": ["í†µê´€", "ì„¸ê´€"],
                "ê´€ì„¸": ["ê´€ì„¸", "ì„¸ê¸ˆ", "ë¶€ê³¼"],
                "ê²€ì‚¬": ["ê²€ì‚¬", "ê²€ì¦", "í™•ì¸"]
            }
            
            if law_area in area_keywords:
                for keyword in area_keywords[law_area]:
                    if keyword in content:
                        score += thresholds["area_score"]
                        break
        
        return min(score, 1.0)
    
    def get_langchain_retriever(self, search_type: str = "similarity", search_kwargs: Optional[Dict[str, Any]] = None):
        """LangChain í‘œì¤€ Retriever ê°ì²´ ë°˜í™˜"""
        if search_kwargs is None:
            search_kwargs = {"k": 5}
        return self.vector_store.get_retriever(search_type=search_type, search_kwargs=search_kwargs)


class CustomsLawAgent:
    """ê´€ì„¸ë²• ì¡°ë¬¸ ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self,
                 embedder=None,
                 vector_store=None,
                 query_normalizer=None,
                 model_name: str = "gpt-4.1-mini",
                 temperature: float = None,
                 max_context_docs: int = 5,
                 similarity_threshold: float = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            embedder: ì„ë² ë”© ìƒì„±ê¸° 
            vector_store: ë²¡í„° ì €ì¥ì†Œ
            query_normalizer: ì¿¼ë¦¬ ì •ê·œí™”ê¸°
            model_name (str): GPT ëª¨ë¸ëª…
            temperature (float): ìƒì„± ì˜¨ë„
            max_context_docs (int): ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if temperature is None:
            temperature = thresholds["model_temperature"] 
        if similarity_threshold is None:
            similarity_threshold = thresholds["similarity_threshold"]
        
        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.retriever = CustomsLawRetriever(
            embedder=embedder,
            vector_store=vector_store, 
            query_normalizer=query_normalizer
        )
        
        # ëª¨ë¸ ì„¤ì •
        self.model_name = model_name
        self.temperature = temperature
        self.max_context_docs = max_context_docs
        self.similarity_threshold = similarity_threshold
        
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
        self.memory = ConversationMemory()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = openai.OpenAI(api_key=api_key)
        logger.info(f"CustomsLawAgent initialized with model: {model_name}")
    
    def query_law(self, user_query: str, include_context: bool = True) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ê´€ì„¸ë²• ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆì˜
            include_context (bool): ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
            
        Returns:
            Tuple[str, List[Dict[str, Any]]]: (ì‘ë‹µ, ì°¸ì¡° ë¬¸ì„œë“¤)
        """
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self.retriever.search_similar_laws(
                raw_query=user_query,
                top_k=self.max_context_docs,
                similarity_threshold=self.similarity_threshold
            )
            
            # 2. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤í•œ í™•ì¥ ê²€ìƒ‰
            if include_context and self.memory.context_documents:
                context_docs = self.memory.context_documents[-3:]  # ìµœê·¼ 3ê°œ ë¬¸ì„œ
                expanded_docs = self.retriever._expand_with_context(user_query, context_docs, self.max_context_docs)
                
                # ì¤‘ë³µ ì œê±°í•˜ë©° í†µí•©
                from ..utils.tools import deduplicate_by_id
                all_docs = relevant_docs + expanded_docs
                relevant_docs = deduplicate_by_id(all_docs, id_key="id")
            
            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‘ë‹µ ìƒì„±
            if relevant_docs:
                response = self._generate_response_with_context(user_query, relevant_docs)
            else:
                response = self._generate_fallback_response(user_query)
            
            # 4. ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.memory.add_user_message(user_query)
            self.memory.add_assistant_message(response, relevant_docs)
            
            return response, relevant_docs
            
        except Exception as e:
            logger.error(f"ê´€ì„¸ë²• ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            error_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            return error_response, []
    
    def _generate_response_with_context(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±"""
        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
            conversation_context = self.memory.get_recent_context(2)
            
            # ë¬¸ì„œ ì •ë³´ í¬ë§·íŒ…
            context_text = self._format_documents_for_prompt(documents)
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = f"""ë‹¹ì‹ ì€ ê´€ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê´€ì„¸ë²• ì¡°ë¬¸ë“¤ì„ ì°¸ì¡°í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ê´€ë ¨ ë²•ë ¹ ì¡°ë¬¸:
{context_text}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
1. êµ¬ì²´ì ì¸ ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œ
2. ë²•ë ¹ì˜ ë‚´ìš©ì„ ì •í™•íˆ í•´ì„í•˜ì—¬ ì„¤ëª…  
3. ì‹¤ë¬´ì ì¸ ì ìš© ë°©ë²•ë„ í•¨ê»˜ ì•ˆë‚´
4. ê´€ë ¨ëœ ë‹¤ë¥¸ ì¡°ë¬¸ì´ ìˆë‹¤ë©´ í•¨ê»˜ ì•ˆë‚´"""
            
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend(conversation_context)
            messages.append({"role": "user", "content": query})
            
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=1000
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_fallback_response(self, query: str) -> str:
        """ë¬¸ì„œ ì—†ì´ ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±"""
        return f"'{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê´€ì„¸ë²• ì¡°ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
    
    def _format_documents_for_prompt(self, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_docs = []
        
        for doc in documents:
            index = doc.get("index", "")
            subtitle = doc.get("subtitle", "")
            content = doc.get("content", "")
            
            if index and content:
                doc_text = f"[{index}] {subtitle}\n{content}"
                formatted_docs.append(doc_text)
        
        return "\n\n".join(formatted_docs)
    
    def reset_conversation(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear_history()
        logger.info("ê´€ì„¸ë²• ì—ì´ì „íŠ¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”ë¨")
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """ëŒ€í™” í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "total_messages": len(self.memory.messages),
            "context_documents": len(self.memory.context_documents),
            "agent_type": "customs_law",
            "model_name": self.model_name,
            "temperature": self.temperature
        }


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ConversationAgent = CustomsLawAgent
SimilarLawRetriever = CustomsLawRetriever