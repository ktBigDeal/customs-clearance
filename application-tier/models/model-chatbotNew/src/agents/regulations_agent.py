"""
Regulations Agent Module

ë¬´ì—­ ê·œì œ ì „ë¬¸ ì—ì´ì „íŠ¸ - ê·œì œ ì •ë³´ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í†µí•©
"""

import logging
import openai
from typing import List, Dict, Any, Optional, Tuple
import json
import os
import re
from datetime import datetime

logger = logging.getLogger(__name__)


class TradeRegulationRetriever:
    """ë¬´ì—­ ê·œì œ ì •ë³´ ê²€ìƒ‰ ë° ê´€ë ¨ ë°ì´í„° ì¶”ì  í´ë˜ìŠ¤ (í†µí•©)"""
    
    def __init__(self,
                 embedder=None,
                 vector_store=None,
                 query_normalizer=None,
                 collection_name: str = "trade_info_collection"):
        """
        ì´ˆê¸°í™” (LangChain í‘œì¤€ ì‚¬ìš©)
        
        Args:
            embedder: LangChain ì„ë² ë”© ìƒì„±ê¸°
            vector_store: LangChain ë²¡í„° ì €ì¥ì†Œ
            query_normalizer: ì¿¼ë¦¬ ì •ê·œí™”ê¸°
            collection_name (str): ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„
        """
        from ..utils.embeddings import LangChainEmbedder
        from ..utils.db_connect import LangChainVectorStore
        from ..utils.query_normalizer import get_query_normalizer
        
        # ê¸°ë³¸ê°’ ì„¤ì • (ìë™ ì´ˆê¸°í™”)
        if embedder is None:
            self.embedder = LangChainEmbedder()
        else:
            self.embedder = embedder
            
        if vector_store is None:
            self.vector_store = LangChainVectorStore(
                collection_name=collection_name,
                embedding_function=self.embedder.embeddings
            )
        else:
            self.vector_store = vector_store
            
        if query_normalizer is None:
            self.query_normalizer = get_query_normalizer("trade")
        else:
            self.query_normalizer = query_normalizer
            
        self.collection_name = collection_name
        
        # ë‚´ë¶€ ë¬¸ì„œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._document_cache = {}
        
        # HSì½”ë“œ íŒ¨í„´ ë§¤ì¹­ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        self.hs_code_pattern = re.compile(r'\\b\\d{4,10}\\b')
        
        logger.info(f"TradeRegulationRetriever initialized with collection: {collection_name}")
    
    def search_trade_info(self, 
                         raw_query: str, 
                         top_k: int = 5,
                         include_related: bool = True,
                         expand_with_synonyms: bool = True,
                         similarity_threshold: float = 0.0,
                         filter_by: Optional[Dict[str, Any]] = None,
                         search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ë¬´ì—­ ì •ë³´ ê²€ìƒ‰
        
        Args:
            raw_query (str): ì›ë³¸ ì‚¬ìš©ì ì§ˆì˜
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            include_related (bool): ê´€ë ¨ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            expand_with_synonyms (bool): ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
            filter_by (Optional[Dict[str, Any]]): í•„í„°ë§ ì¡°ê±´
            search_context (Optional[Dict[str, Any]]): ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            from ..utils.query_normalizer import AdvancedQueryProcessor
            
            # 1. LLM ê¸°ë°˜ ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„±
            llm_filters = self._generate_llm_metadata_filters(raw_query, search_context)
            
            # 2. ê¸°ì¡´ í•„í„°ì™€ LLM í•„í„° ë³‘í•©
            combined_filters = self._merge_filters(filter_by, llm_filters)
            
            if combined_filters:
                if not filter_by:
                    filter_by = {}
                filter_by.update(combined_filters)
            
            # 3. ì¿¼ë¦¬ ì •ê·œí™” ë° í™•ì¥
            normalized_query = self.query_normalizer.normalize(raw_query, search_context)
            if expand_with_synonyms:
                expanded_query = self.query_normalizer.expand_query_with_synonyms(normalized_query)
            else:
                expanded_query = normalized_query
            
            logger.info(f"ğŸ” Query: '{raw_query}' â†’ '{expanded_query}'")
            logger.info(f"ğŸ·ï¸ LLM Filters: {llm_filters}")
            
            # 4. HSì½”ë“œ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬
            hs_codes = self._extract_hs_codes(raw_query)
            if hs_codes:
                logger.info(f"ğŸ”¢ ê°ì§€ëœ HSì½”ë“œ: {hs_codes}")
                return self._search_by_hs_code(hs_codes, top_k, include_related)
            
            # 5. ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ë³„ ì²˜ë¦¬
            query_processor = AdvancedQueryProcessor(self.query_normalizer)
            if search_context and search_context.get("domain_hints"):
                logger.info(f"ğŸ¯ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì ìš©: {search_context.get('domain_hints')}")
                context_keywords = search_context.get("boost_keywords", [])
                if context_keywords:
                    enhanced_query = f"{raw_query} {' '.join(context_keywords)}"
                    processed_query = query_processor.process_complex_query(enhanced_query)
                else:
                    processed_query = query_processor.process_complex_query(raw_query)
            else:
                processed_query = query_processor.process_complex_query(raw_query)
            
            # 6. ì‚¬ìš©í•  ì¿¼ë¦¬ ê²°ì •
            if expand_with_synonyms:
                search_query = processed_query["expanded_query"]
            else:
                search_query = processed_query["normalized_query"]
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            # 7. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            where_condition = self._build_where_condition(filter_by)
            
            primary_results = self.vector_store.search_similar(
                query_text=search_query,
                top_k=top_k,
                where=where_condition
            )
            logger.info(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(primary_results)}ê°œ")
            
            # 8. ê´€ë ¨ ë°ì´í„° í™•ì¥ ê²€ìƒ‰
            if include_related:
                expanded_results = self._expand_with_related_info(primary_results, top_k)
            else:
                expanded_results = primary_results
            
            # 9. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì •ë ¬
            final_results = self._post_process_results(expanded_results, processed_query, search_context)
            
            # 10. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            if similarity_threshold > 0.0:
                filtered_results = [result for result in final_results 
                                  if result.get("similarity", 0) >= similarity_threshold]
                logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold}ë¡œ í•„í„°ë§: {len(final_results)} â†’ {len(filtered_results)}ê°œ")
                final_results = filtered_results
            
            logger.info(f"âœ… {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìš”ì²­ëœ top_k: {top_k})")
            return final_results
            
        except Exception as e:
            logger.error(f"ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_hs_codes(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ HSì½”ë“œ ì¶”ì¶œ"""
        matches = self.hs_code_pattern.findall(query)
        hs_codes = [match for match in matches if len(match) >= 4]
        return hs_codes
    
    def _search_by_hs_code(self, hs_codes: List[str], top_k: int, include_related: bool) -> List[Dict[str, Any]]:
        """HSì½”ë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        all_results = []
        
        for hs_code in hs_codes:
            try:
                where_condition = {"hs_code": {"$eq": hs_code}}
                query = f"HSì½”ë“œ {hs_code} ì œí’ˆ ê·œì œ ìˆ˜ì¶œ ìˆ˜ì…"
                query_embedding = self.embedder.embed_text(query)
                
                results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    where=where_condition
                )
                
                for result in results:
                    result["hs_code_match"] = hs_code
                    result["match_type"] = "exact_hs_code"
                
                all_results.extend(results)
                
                if include_related and len(hs_code) >= 6:
                    related_results = self._search_related_hs_codes(hs_code[:6], top_k // 2)
                    all_results.extend(related_results)
                    
            except Exception as e:
                logger.error(f"HSì½”ë“œ {hs_code} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        from ..utils.tools import smart_deduplicate
        unique_results = smart_deduplicate(all_results, id_key="id", preserve_higher_score=True)
        return unique_results[:top_k]
    
    def _search_related_hs_codes(self, hs_prefix: str, top_k: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰"""
        try:
            query = f"HSì½”ë“œ {hs_prefix} ê´€ë ¨ ì œí’ˆ"
            query_embedding = self.embedder.embed_text(query)
            
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                top_k=top_k
            )
            
            for result in results:
                result["match_type"] = "related_hs_code"
                result["hs_prefix_match"] = hs_prefix
            
            return results
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ HSì½”ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _build_where_condition(self, filter_by: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """í•„í„°ë§ ì¡°ê±´ êµ¬ì„±"""
        if not filter_by:
            return None
        
        supported_fields = [
            "category", "country", "regulation_type", "status", "hs_code", "product_category", 
            "data_type", "data_source", "consultation_type", "case_number", "product_name",
            "affected_country", "regulating_country", "animal_plant_type", "management_number"
        ]
        
        conditions = []
        for key, value in filter_by.items():
            if key in supported_fields:
                conditions.append({key: {"$eq": value}})
        
        if not conditions:
            return None
        elif len(conditions) == 1:
            return conditions[0]
        else:
            return {"$and": conditions}
    
    def _expand_with_related_info(self, results: List[Dict[str, Any]], max_total: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ì •ë³´ë¡œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥"""
        expanded_results = results.copy()
        seen_ids = {result.get("id", "") for result in results}
        
        for result in results[:3]:
            metadata = result.get("metadata", {})
            
            # HSì½”ë“œ ê¸°ë°˜ í™•ì¥
            hs_code = metadata.get("hs_code", "")
            if hs_code and len(hs_code) >= 6:
                related_hs = self._search_related_hs_codes(hs_code[:6], 2)
                for related in related_hs:
                    if related.get("id", "") not in seen_ids:
                        related["reference_info"] = {"is_related": True, "related_to": result.get("id", "")}
                        expanded_results.append(related)
                        seen_ids.add(related.get("id", ""))
        
        return expanded_results[:max_total]
    
    def _post_process_results(self, results: List[Dict[str, Any]], processed_query: Dict[str, Any], search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        for result in results:
            importance_score = self._calculate_importance_score(result, processed_query)
            result["importance_score"] = importance_score
            
            # search_context ê¸°ë°˜ ë¶€ìŠ¤íŒ…
            if search_context:
                context_boost = self._apply_search_context_boost(result, search_context)
                result["context_boost"] = context_boost
                result["importance_score"] += context_boost
        
        # ì¤‘ìš”ë„ì™€ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: (
            x.get("importance_score", 0) * thresholds["importance_weight"] + 
            x.get("similarity", 0) * thresholds["similarity_weight"]
        ), reverse=True)
        
        return results
    
    def _apply_search_context_boost(self, result: Dict[str, Any], search_context: Dict[str, Any]) -> float:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìˆ˜ ê³„ì‚°"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        boost_score = 0.0
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        
        # ìš°ì„ ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ ë¶€ìŠ¤íŒ…
        priority_sources = search_context.get("priority_data_sources", [])
        data_source = metadata.get("data_source", "")
        if data_source in priority_sources:
            boost_score += 0.3
            result["boosted"] = True
            result["boost_reason"] = f"ìš°ì„ ìˆœìœ„ ë°ì´í„°ì†ŒìŠ¤: {data_source}"
        
        # ë„ë©”ì¸ íŒíŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
        domain_hints = search_context.get("domain_hints", [])
        for hint in domain_hints:
            if hint in content.lower() or hint in str(metadata).lower():
                boost_score += 0.2
                result["boosted"] = True
                result["boost_reason"] = result.get("boost_reason", "") + f" ë„ë©”ì¸ë§¤ì¹­: {hint}"
        
        # ë¶€ìŠ¤íŒ… í‚¤ì›Œë“œ ë§¤ì¹­
        boost_keywords = search_context.get("boost_keywords", [])
        for keyword in boost_keywords:
            if keyword in content or keyword in str(metadata):
                boost_score += 0.1
        
        return min(boost_score, thresholds["boost_score_limit"])
    
    def _calculate_importance_score(self, result: Dict[str, Any], processed_query: Dict[str, Any]) -> float:
        """ë¬¸ì„œ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        metadata = result.get("metadata", {})
        
        # ë™ì‹ë¬¼ ìˆ˜ì… ê·œì œ ë°ì´í„° ìµœìš°ì„  ì²˜ë¦¬
        data_source = metadata.get("data_source", "")
        if data_source == "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­":
            score += 0.5
            
            priority = metadata.get("priority", 0)
            regulation_intensity = metadata.get("regulation_intensity", "")
            
            try:
                priority_int = int(priority) if priority else 0
            except (ValueError, TypeError):
                priority_int = 0
            
            if priority_int >= 2:
                score += 0.2
            if regulation_intensity == "high":
                score += 0.15
            if metadata.get("has_global_prohibition", False):
                score += 0.1
                
        elif "ê·œì œ" in data_source:
            score += 0.3
        
        # ê·œì œ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        regulation_type = metadata.get("regulation_type", "")
        if regulation_type == "import_regulations":
            score += 0.25
        elif "restrictions" in regulation_type:
            score += 0.2
        elif "prohibitions" in regulation_type:
            score += 0.3
        
        # HSì½”ë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜
        if result.get("match_type") == "exact_hs_code":
            score += 0.4
        elif result.get("match_type") == "related_hs_code":
            score += 0.2
        
        return min(score, 1.0)
    
    def _generate_llm_metadata_filters(self, query: str, search_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•´ì„œ ì¿¼ë¦¬ì— ë§ëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ìƒì„±"""
        try:
            intent_info = self.query_normalizer.extract_intent(query)
            filters = {}
            
            # ë°ì´í„° íƒ€ì… ê²°ì •
            if intent_info.get("intent_type") == "ë™ì‹ë¬¼ê²€ì—­":
                filters["data_type"] = "trade_regulation"
                filters["data_source"] = "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"
            elif intent_info.get("trade_category") == "ë™ì‹ë¬¼ìˆ˜ì…ê·œì œ":
                filters["data_type"] = "trade_regulation"
                filters["data_source"] = "ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­"
            elif search_context and search_context.get("agent_type") == "regulation_agent":
                filters["data_type"] = "trade_regulation"
            elif search_context and search_context.get("agent_type") == "consultation_agent":
                filters["data_type"] = "consultation_case"
            
            logger.debug(f"ğŸ¤– LLM ìƒì„± í•„í„°: {filters}")
            return filters
            
        except Exception as e:
            logger.error(f"LLM ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _merge_filters(self, user_filters: Optional[Dict[str, Any]], llm_filters: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì œê³µ í•„í„°ì™€ LLM ìƒì„± í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©"""
        if not user_filters:
            return llm_filters
        
        if not llm_filters:
            return user_filters or {}
        
        merged = llm_filters.copy()
        merged.update(user_filters)
        return merged
    
    def get_langchain_retriever(self, 
                               search_type: str = "similarity",
                               search_kwargs: Optional[Dict[str, Any]] = None):
        """LangChain í‘œì¤€ Retriever ê°ì²´ ë°˜í™˜"""
        if search_kwargs is None:
            search_kwargs = {"k": 5}
        
        return self.vector_store.get_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )


class RegulationsAgent:
    """ë¬´ì—­ ê·œì œ ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self,
                 retriever=None,
                 model_name: str = "gpt-4.1-mini",
                 temperature: float = None,
                 max_context_docs: int = 8,
                 similarity_threshold: float = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            retriever: ë¬´ì—­ ì •ë³´ ê²€ìƒ‰ê¸°
            model_name (str): GPT ëª¨ë¸ëª…  
            temperature (float): ìƒì„± ì˜¨ë„
            max_context_docs (int): ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if temperature is None:
            temperature = thresholds["regulation_temperature"]
        if similarity_threshold is None:
            similarity_threshold = thresholds["similarity_threshold"]
        
        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        if retriever is None:
            self.retriever = TradeRegulationRetriever()
        else:
            self.retriever = retriever
        
        self.model_name = model_name
        self.temperature = temperature
        self.max_context_docs = max_context_docs
        self.similarity_threshold = similarity_threshold
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = openai.OpenAI(api_key=api_key)
        logger.info(f"RegulationsAgent initialized with model: {model_name}")
    
    def query_regulation(self, user_query: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ë¬´ì—­ ê·œì œ ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆì˜
            
        Returns:
            Tuple[str, List[Dict[str, Any]]]: (ì‘ë‹µ, ì°¸ì¡° ë¬¸ì„œë“¤)
        """
        try:
            # 1. ê´€ë ¨ ê·œì œ ì •ë³´ ê²€ìƒ‰
            search_context = {
                "agent_type": "regulation_agent",
                "domain_hints": ["trade_regulation", "import_export"],
                "priority_data_sources": ["ë™ì‹ë¬¼í—ˆìš©ê¸ˆì§€ì§€ì—­", "ìˆ˜ì…ê·œì œDB_ì „ì²´"]
            }
            
            relevant_docs = self.retriever.search_trade_info(
                raw_query=user_query,
                top_k=self.max_context_docs,
                search_context=search_context,
                similarity_threshold=self.similarity_threshold
            )
            
            # 2. ì‘ë‹µ ìƒì„±
            if relevant_docs:
                response = self._generate_response_with_context(user_query, relevant_docs)
            else:
                response = self._generate_fallback_response(user_query)
            
            return response, relevant_docs
            
        except Exception as e:
            logger.error(f"ë¬´ì—­ ê·œì œ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            error_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬´ì—­ ê·œì œ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            return error_response, []
    
    def _generate_response_with_context(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±"""
        try:
            context_text = self._format_documents_for_prompt(documents)
            
            system_prompt = f"""ë‹¹ì‹ ì€ ë¬´ì—­ ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬´ì—­ ê·œì œ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì—¬ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ê´€ë ¨ ê·œì œ ì •ë³´:
{context_text}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
1. HSì½”ë“œë‚˜ ì œí’ˆëª…ì„ ëª…í™•íˆ ì œì‹œ
2. í•´ë‹¹ êµ­ê°€ì˜ êµ¬ì²´ì ì¸ ê·œì œ ë‚´ìš© ì„¤ëª…
3. ìˆ˜ì…/ìˆ˜ì¶œ ì‹œ í•„ìš”í•œ ì ˆì°¨ë‚˜ ì£¼ì˜ì‚¬í•­ ì•ˆë‚´
4. ê´€ë ¨ëœ ë‹¤ë¥¸ ê·œì œê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì•ˆë‚´
5. ë™ì‹ë¬¼ ê²€ì—­ì˜ ê²½ìš° íŠ¹ë³„ ì£¼ì˜ì‚¬í•­ ê°•ì¡°"""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=1200
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"ê·œì œ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê·œì œ ì •ë³´ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_fallback_response(self, query: str) -> str:
        """ë¬¸ì„œ ì—†ì´ ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±"""
        return f"'{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë¬´ì—­ ê·œì œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HSì½”ë“œë‚˜ êµ¬ì²´ì ì¸ ì œí’ˆëª…, êµ­ê°€ëª…ì„ í¬í•¨í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
    
    def _format_documents_for_prompt(self, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_docs = []
        
        for doc in documents:
            metadata = doc.get("metadata", {})
            content = doc.get("content", "")
            
            # ê·œì œ ì •ë³´ íŠ¹í™” í¬ë§·íŒ…
            doc_info = []
            if metadata.get("hs_code"):
                doc_info.append(f"HSì½”ë“œ: {metadata['hs_code']}")
            if metadata.get("country"):
                doc_info.append(f"êµ­ê°€: {metadata['country']}")
            if metadata.get("product_name"):
                doc_info.append(f"ì œí’ˆ: {metadata['product_name']}")
            if metadata.get("regulation_type"):
                doc_info.append(f"ê·œì œìœ í˜•: {metadata['regulation_type']}")
            
            doc_header = " | ".join(doc_info) if doc_info else "ê·œì œ ì •ë³´"
            formatted_docs.append(f"[{doc_header}]\n{content}")
        
        return "\n\n".join(formatted_docs)
    
    def get_agent_stats(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            retriever_stats = self.retriever.vector_store.get_collection_stats() if hasattr(self.retriever, 'vector_store') else {}
            return {
                "agent_type": "regulations",
                "model_name": self.model_name,
                "temperature": self.temperature,
                "max_context_docs": self.max_context_docs,
                "similarity_threshold": self.similarity_threshold,
                "retriever_stats": retriever_stats
            }
        except Exception as e:
            logger.error(f"ê·œì œ ì—ì´ì „íŠ¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
TradeRegulationAgent = RegulationsAgent
TradeInfoRetriever = TradeRegulationRetriever