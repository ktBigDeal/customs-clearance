"""
Complaints Agent Module

ìƒë‹´ ì‚¬ë¡€ ì „ë¬¸ ì—ì´ì „íŠ¸ - ì‹¤ë¬´ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í†µí•©
"""

import logging
import openai
from typing import List, Dict, Any, Optional, Tuple
import json
import os
from datetime import datetime

logger = logging.getLogger(__name__)


class ComplaintsMemory:
    """ìƒë‹´ ì‚¬ë¡€ ì „ìš© ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, max_history: int = 12):
        """
        ì´ˆê¸°í™”
        
        Args:
            max_history (int): ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ìˆ˜ (ìƒë‹´ì‚¬ë¡€ëŠ” ë§¥ë½ì´ ì¤‘ìš”)
        """
        self.max_history = max_history
        self.messages = []
        self.case_context = []  # ì°¸ì¡°ëœ ìƒë‹´ ì‚¬ë¡€ë“¤
        self.search_history = []  # ìƒë‹´ ê²€ìƒ‰ ê¸°ë¡
        self.user_patterns = {}  # ì‚¬ìš©ì ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„
        
    def add_user_message(self, message: str, search_context: Optional[Dict] = None) -> None:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€"""
        message_data = {
            "role": "user",
            "content": message,
            "timestamp": datetime.now().isoformat()
        }
        
        if search_context:
            message_data["search_context"] = search_context
            self.search_history.append(search_context)
            
            # ì‚¬ìš©ì íŒ¨í„´ ë¶„ì„
            self._analyze_user_patterns(message)
        
        self.messages.append(message_data)
        self._trim_history()
    
    def add_assistant_message(self, message: str, source_cases: List[Dict] = None) -> None:
        """ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€"""
        self.messages.append({
            "role": "assistant", 
            "content": message,
            "timestamp": datetime.now().isoformat(),
            "source_cases": source_cases or []
        })
        
        # ì°¸ì¡°ëœ ìƒë‹´ ì‚¬ë¡€ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        if source_cases:
            for case in source_cases:
                if case not in self.case_context:
                    self.case_context.append(case)
        
        self._trim_history()
    
    def get_conversation_history(self, include_timestamps: bool = False) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        if include_timestamps:
            return self.messages.copy()
        else:
            return [{"role": msg["role"], "content": msg["content"]} for msg in self.messages]
    
    def get_recent_context(self, num_turns: int = 3) -> List[Dict]:
        """ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (ìƒë‹´ì‚¬ë¡€ëŠ” ë§¥ë½ì´ ì¤‘ìš”)"""
        recent_messages = self.messages[-num_turns*2:] if self.messages else []
        return [{"role": msg["role"], "content": msg["content"]} for msg in recent_messages]
    
    def get_user_patterns(self) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        return self.user_patterns.copy()
    
    def clear_history(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.messages.clear()
        self.case_context.clear()
        self.search_history.clear()
        self.user_patterns.clear()
    
    def _analyze_user_patterns(self, message: str) -> None:
        """ì‚¬ìš©ì ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒ¨í„´ ë¶„ì„
        keywords = {
            "í†µê´€": ["í†µê´€", "ì‹ ê³ ", "ì‹ ê³ ì„œ", "ì„¸ê´€"],
            "ê´€ì„¸": ["ê´€ì„¸", "ì„¸ê¸ˆ", "ë©´ì„¸", "ê°ë©´"],
            "ì ˆì°¨": ["ì ˆì°¨", "ë°©ë²•", "ì–´ë–»ê²Œ", "í•´ì•¼"],
            "ì„œë¥˜": ["ì„œë¥˜", "ë¬¸ì„œ", "ì¦ëª…ì„œ", "í—ˆê°€ì„œ"],
            "FTA": ["FTA", "ì›ì‚°ì§€", "íŠ¹í˜œê´€ì„¸"],
            "ê²€ì—­": ["ê²€ì—­", "ê²€ì‚¬", "ìŠ¹ì¸"]
        }
        
        for category, words in keywords.items():
            for word in words:
                if word in message:
                    self.user_patterns[category] = self.user_patterns.get(category, 0) + 1
    
    def _trim_history(self) -> None:
        """ëŒ€í™” ê¸°ë¡ í¬ê¸° ì œí•œ"""
        if len(self.messages) > self.max_history:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ëŒ€í™”ë§Œ ì œê±°
            system_messages = [msg for msg in self.messages if msg["role"] == "system"]
            other_messages = [msg for msg in self.messages if msg["role"] != "system"]
            
            # ìµœê·¼ ëŒ€í™”ë§Œ ìœ ì§€
            trimmed_other = other_messages[-(self.max_history - len(system_messages)):]
            self.messages = system_messages + trimmed_other


class ComplaintsRetriever:
    """ìƒë‹´ ì‚¬ë¡€ ê²€ìƒ‰ ë° ê´€ë ¨ ë°ì´í„° ì¶”ì  í´ë˜ìŠ¤ (í†µí•©)"""
    
    def __init__(self,
                 embedder=None,
                 vector_store=None,
                 query_normalizer=None,
                 collection_name: str = "consultation_collection"):
        """
        ì´ˆê¸°í™” (LangChain í‘œì¤€ ì‚¬ìš©)
        
        Args:
            embedder: LangChain ì„ë² ë”© ìƒì„±ê¸°
            vector_store: LangChain ë²¡í„° ì €ì¥ì†Œ
            query_normalizer: ì¿¼ë¦¬ ì •ê·œí™”ê¸°
            collection_name (str): ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„
        """
        from ..utils.embeddings import LangChainEmbedder
        from ..utils.db_connect import LangChainVectorStore
        from ..utils.query_normalizer import get_query_normalizer
        
        # ê¸°ë³¸ê°’ ì„¤ì • (ìë™ ì´ˆê¸°í™”)
        if embedder is None:
            self.embedder = LangChainEmbedder()
        else:
            self.embedder = embedder
            
        if vector_store is None:
            self.vector_store = LangChainVectorStore(
                collection_name=collection_name,
                embedding_function=self.embedder.embeddings
            )
        else:
            self.vector_store = vector_store
            
        if query_normalizer is None:
            self.query_normalizer = get_query_normalizer("trade")
        else:
            self.query_normalizer = query_normalizer
            
        self.collection_name = collection_name
        
        # ë‚´ë¶€ ë¬¸ì„œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._document_cache = {}
        
        logger.info(f"ComplaintsRetriever initialized with collection: {collection_name}")
    
    def search_consultation_cases(self, 
                                 raw_query: str, 
                                 top_k: int = 8,
                                 include_related: bool = True,
                                 expand_with_synonyms: bool = True,
                                 similarity_threshold: float = 0.0,
                                 filter_by: Optional[Dict[str, Any]] = None,
                                 search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ìƒë‹´ ì‚¬ë¡€ ê²€ìƒ‰
        
        Args:
            raw_query (str): ì›ë³¸ ì‚¬ìš©ì ì§ˆì˜
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            include_related (bool): ê´€ë ¨ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
            expand_with_synonyms (bool): ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
            filter_by (Optional[Dict[str, Any]]): í•„í„°ë§ ì¡°ê±´
            search_context (Optional[Dict[str, Any]]): ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            from ..utils.query_normalizer import AdvancedQueryProcessor
            
            # 1. consultation_case ë°ì´í„°ë§Œ ê²€ìƒ‰í•˜ë„ë¡ í•„í„° ê°•ì œ ì„¤ì •
            if not filter_by:
                filter_by = {}
            filter_by["data_type"] = "consultation_case"
            
            # 2. LLM ê¸°ë°˜ ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„±
            llm_filters = self._generate_llm_metadata_filters(raw_query, search_context)
            
            # 3. ê¸°ì¡´ í•„í„°ì™€ LLM í•„í„° ë³‘í•©
            combined_filters = self._merge_filters(filter_by, llm_filters)
            
            if combined_filters:
                filter_by.update(combined_filters)
            
            # 4. ì¿¼ë¦¬ ì •ê·œí™” ë° í™•ì¥
            normalized_query = self.query_normalizer.normalize(raw_query, search_context)
            if expand_with_synonyms:
                expanded_query = self.query_normalizer.expand_query_with_synonyms(normalized_query)
            else:
                expanded_query = normalized_query
            
            logger.info(f"ğŸ” Query: '{raw_query}' â†’ '{expanded_query}'")
            logger.info(f"ğŸ·ï¸ LLM Filters: {llm_filters}")
            
            # 5. ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ë³„ ì²˜ë¦¬
            query_processor = AdvancedQueryProcessor(self.query_normalizer)
            if search_context and search_context.get("domain_hints"):
                logger.info(f"ğŸ¯ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì ìš©: {search_context.get('domain_hints')}")
                context_keywords = search_context.get("boost_keywords", [])
                if context_keywords:
                    enhanced_query = f"{raw_query} {' '.join(context_keywords)}"
                    processed_query = query_processor.process_complex_query(enhanced_query)
                else:
                    processed_query = query_processor.process_complex_query(raw_query)
            else:
                processed_query = query_processor.process_complex_query(raw_query)
            
            # 6. ì‚¬ìš©í•  ì¿¼ë¦¬ ê²°ì •
            if expand_with_synonyms:
                search_query = processed_query["expanded_query"]
            else:
                search_query = processed_query["normalized_query"]
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            # 7. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            where_condition = self._build_where_condition(filter_by)
            
            primary_results = self.vector_store.search_similar(
                query_text=search_query,
                top_k=top_k,
                where=where_condition
            )
            logger.info(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(primary_results)}ê°œ")
            
            # 8. ê´€ë ¨ ë°ì´í„° í™•ì¥ ê²€ìƒ‰
            if include_related:
                expanded_results = self._expand_with_related_cases(primary_results, top_k)
            else:
                expanded_results = primary_results
            
            # 9. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì •ë ¬
            final_results = self._post_process_results(expanded_results, processed_query, search_context)
            
            # 10. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            if similarity_threshold > 0.0:
                filtered_results = [result for result in final_results 
                                  if result.get("similarity", 0) >= similarity_threshold]
                logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold}ë¡œ í•„í„°ë§: {len(final_results)} â†’ {len(filtered_results)}ê°œ")
                final_results = filtered_results
            
            logger.info(f"âœ… {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ìš”ì²­ëœ top_k: {top_k})")
            return final_results
            
        except Exception as e:
            logger.error(f"ìƒë‹´ ì‚¬ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _expand_with_related_cases(self, results: List[Dict[str, Any]], max_total: int) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ìƒë‹´ ì‚¬ë¡€ë¡œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥"""
        expanded_results = results.copy()
        seen_ids = {result.get("id", "") for result in results}
        
        for result in results[:3]:
            metadata = result.get("metadata", {})
            
            # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í™•ì¥
            category = metadata.get("category", "")
            if category:
                related_cases = self._search_related_by_category(category, 2)
                for related in related_cases:
                    if related.get("id", "") not in seen_ids:
                        related["reference_info"] = {"is_related": True, "related_to": result.get("id", "")}
                        expanded_results.append(related)
                        seen_ids.add(related.get("id", ""))
        
        return expanded_results[:max_total]
    
    def _search_related_by_category(self, category: str, top_k: int) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰"""
        try:
            query = f"{category} ìƒë‹´ ì‚¬ë¡€"
            where_condition = {"category": {"$eq": category}}
            
            results = self.vector_store.search_similar(
                query_text=query,
                top_k=top_k,
                where=where_condition
            )
            
            for result in results:
                result["match_type"] = "related_category"
                result["category_match"] = category
            
            return results
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _build_where_condition(self, filter_by: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """í•„í„°ë§ ì¡°ê±´ êµ¬ì„±"""
        if not filter_by:
            return None
        
        supported_fields = [
            "category", "sub_category", "consultation_type", "case_number", "data_type",
            "data_source", "management_number", "keywords", "priority", "status"
        ]
        
        conditions = []
        for key, value in filter_by.items():
            if key in supported_fields:
                conditions.append({key: {"$eq": value}})
        
        if not conditions:
            return None
        elif len(conditions) == 1:
            return conditions[0]
        else:
            return {"$and": conditions}
    
    def _post_process_results(self, results: List[Dict[str, Any]], processed_query: Dict[str, Any], search_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        for result in results:
            importance_score = self._calculate_importance_score(result, processed_query)
            result["importance_score"] = importance_score
            
            # search_context ê¸°ë°˜ ë¶€ìŠ¤íŒ…
            if search_context:
                context_boost = self._apply_search_context_boost(result, search_context)
                result["context_boost"] = context_boost
                result["importance_score"] += context_boost
        
        # ì¤‘ìš”ë„ì™€ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: (
            x.get("importance_score", 0) * thresholds["importance_weight"] + 
            x.get("similarity", 0) * thresholds["similarity_weight"]
        ), reverse=True)
        
        return results
    
    def _apply_search_context_boost(self, result: Dict[str, Any], search_context: Dict[str, Any]) -> float:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ì ìˆ˜ ê³„ì‚°"""
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        boost_score = 0.0
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        
        # ìš°ì„ ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ ë¶€ìŠ¤íŒ…
        priority_sources = search_context.get("priority_data_sources", [])
        data_source = metadata.get("data_source", "")
        if data_source in priority_sources:
            boost_score += 0.3
            result["boosted"] = True
            result["boost_reason"] = f"ìš°ì„ ìˆœìœ„ ë°ì´í„°ì†ŒìŠ¤: {data_source}"
        
        # ë„ë©”ì¸ íŒíŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
        domain_hints = search_context.get("domain_hints", [])
        for hint in domain_hints:
            if hint in content.lower() or hint in str(metadata).lower():
                boost_score += 0.2
                result["boosted"] = True
                result["boost_reason"] = result.get("boost_reason", "") + f" ë„ë©”ì¸ë§¤ì¹­: {hint}"
        
        # ë¶€ìŠ¤íŒ… í‚¤ì›Œë“œ ë§¤ì¹­
        boost_keywords = search_context.get("boost_keywords", [])
        for keyword in boost_keywords:
            if keyword in content or keyword in str(metadata):
                boost_score += 0.1
        
        return min(boost_score, thresholds["boost_score_limit"])
    
    def _calculate_importance_score(self, result: Dict[str, Any], processed_query: Dict[str, Any]) -> float:
        """ë¬¸ì„œ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        metadata = result.get("metadata", {})
        
        # ìƒë‹´ ì‚¬ë¡€ ë°ì´í„° ìš°ì„  ì²˜ë¦¬
        data_source = metadata.get("data_source", "")
        if data_source == "ìƒë‹´ì‚¬ë¡€DB":
            score += 0.5
            
            priority = metadata.get("priority", 0)
            case_type = metadata.get("consultation_type", "")
            
            try:
                priority_int = int(priority) if priority else 0
            except (ValueError, TypeError):
                priority_int = 0
            
            if priority_int >= 2:
                score += 0.2
            if case_type == "complex":
                score += 0.15
            if metadata.get("has_precedent", False):
                score += 0.1
                
        elif "ë¯¼ì›" in data_source or "ìƒë‹´" in data_source:
            score += 0.3
        
        # ìƒë‹´ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        consultation_type = metadata.get("consultation_type", "")
        if consultation_type == "complex_case":
            score += 0.25
        elif "procedure" in consultation_type:
            score += 0.2
        elif "document" in consultation_type:
            score += 0.3
        
        # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ê°€ì¤‘ì¹˜
        if result.get("match_type") == "exact_category":
            score += 0.4
        elif result.get("match_type") == "related_category":
            score += 0.2
        
        return min(score, 1.0)
    
    def _generate_llm_metadata_filters(self, query: str, search_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•´ì„œ ì¿¼ë¦¬ì— ë§ëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ìƒì„±"""
        try:
            intent_info = self.query_normalizer.extract_intent(query)
            filters = {}
            
            # ë°ì´í„° íƒ€ì… ê²°ì •
            if intent_info.get("intent_type") == "ì ˆì°¨ì•ˆë‚´":
                filters["data_type"] = "consultation_case"
                filters["consultation_type"] = "procedure"
            elif intent_info.get("trade_category") == "ì‹¤ë¬´ìƒë‹´":
                filters["data_type"] = "consultation_case"
                filters["data_source"] = "ìƒë‹´ì‚¬ë¡€DB"
            elif search_context and search_context.get("agent_type") == "consultation_agent":
                filters["data_type"] = "consultation_case"
            
            logger.debug(f"ğŸ¤– LLM ìƒì„± í•„í„°: {filters}")
            return filters
            
        except Exception as e:
            logger.error(f"LLM ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _merge_filters(self, user_filters: Optional[Dict[str, Any]], llm_filters: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì œê³µ í•„í„°ì™€ LLM ìƒì„± í•„í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©"""
        if not user_filters:
            return llm_filters
        
        if not llm_filters:
            return user_filters or {}
        
        merged = llm_filters.copy()
        merged.update(user_filters)
        return merged
    
    def get_langchain_retriever(self, 
                               search_type: str = "similarity",
                               search_kwargs: Optional[Dict[str, Any]] = None):
        """LangChain í‘œì¤€ Retriever ê°ì²´ ë°˜í™˜"""
        if search_kwargs is None:
            search_kwargs = {"k": 8}
        
        return self.vector_store.get_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )


class ComplaintsAgent:
    """ìƒë‹´ ì‚¬ë¡€ ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self,
                 retriever=None,
                 model_name: str = "gpt-4.1-mini",
                 temperature: float = None,
                 max_context_docs: int = 8,
                 similarity_threshold: float = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            retriever: ìƒë‹´ ì‚¬ë¡€ ê²€ìƒ‰ê¸°
            model_name (str): GPT ëª¨ë¸ëª…  
            temperature (float): ìƒì„± ì˜¨ë„
            max_context_docs (int): ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        from ..config.config import get_quality_thresholds
        thresholds = get_quality_thresholds()
        
        # ê¸°ë³¸ê°’ ì„¤ì • (ìƒë‹´ì‚¬ë¡€ëŠ” ì•½ê°„ ë” ìœ ì—°í•˜ê²Œ)
        if temperature is None:
            temperature = thresholds["consultation_temperature"]
        if similarity_threshold is None:
            similarity_threshold = thresholds["similarity_threshold"]
        
        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        if retriever is None:
            self.retriever = ComplaintsRetriever()
        else:
            self.retriever = retriever
        
        self.model_name = model_name
        self.temperature = temperature
        self.max_context_docs = max_context_docs
        self.similarity_threshold = similarity_threshold
        
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
        self.memory = ComplaintsMemory()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = openai.OpenAI(api_key=api_key)
        logger.info(f"ComplaintsAgent initialized with model: {model_name}")
    
    def query_consultation(self, user_query: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ìƒë‹´ ì‚¬ë¡€ ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆì˜
            
        Returns:
            Tuple[str, List[Dict[str, Any]]]: (ì‘ë‹µ, ì°¸ì¡° ë¬¸ì„œë“¤)
        """
        try:
            # 1. ìƒë‹´ ì‚¬ë¡€ ì „ìš© ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            search_context = {
                "agent_type": "consultation_agent",
                "domain_hints": ["consultation_case", "ì‹¤ë¬´", "ì ˆì°¨", "ë°©ë²•", "ê²½í—˜", "ì‚¬ë¡€"],
                "boost_keywords": ["ì ˆì°¨", "ë°©ë²•", "ì‹¤ë¬´", "ê²½í—˜", "ì‚¬ë¡€", "ì‹ ê³ ", "ì‹ ì²­", "ìŠ¹ì¸", "ì²˜ë¦¬", "ì„œë¥˜", "ë¹„ìš©", "ê¸°ê°„", "í†µê´€", "ì„¸ê´€", "ê´€ì„¸"],
                "priority_data_sources": ["ìƒë‹´ì‚¬ë¡€DB", "ì‹¤ë¬´ê°€ì´ë“œ", "ë¯¼ì›ì²˜ë¦¬ì‚¬ë¡€"]
            }
            
            # 2. ê´€ë ¨ ìƒë‹´ ì‚¬ë¡€ ê²€ìƒ‰
            relevant_docs = self.retriever.search_consultation_cases(
                raw_query=user_query,
                top_k=self.max_context_docs,
                search_context=search_context,
                similarity_threshold=self.similarity_threshold
            )
            
            # 3. ì‘ë‹µ ìƒì„±
            if relevant_docs:
                response = self._generate_response_with_context(user_query, relevant_docs)
            else:
                response = self._generate_fallback_response(user_query)
            
            # 4. ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.memory.add_user_message(user_query, search_context)
            self.memory.add_assistant_message(response, relevant_docs)
            
            return response, relevant_docs
            
        except Exception as e:
            logger.error(f"ìƒë‹´ ì‚¬ë¡€ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            error_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ìƒë‹´ ì‚¬ë¡€ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            return error_response, []
    
    def get_similar_cases(self, category: str, top_k: int = 5) -> List[Dict]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìœ ì‚¬í•œ ìƒë‹´ ì‚¬ë¡€ ì¡°íšŒ"""
        try:
            search_filters = {
                "data_type": "consultation_case",
                "category": category
            }
            
            retrieved_docs = self.retriever.search_consultation_cases(
                raw_query=f"{category} ìƒë‹´ ì‚¬ë¡€",
                top_k=top_k,
                filter_by=search_filters
            )
            
            return retrieved_docs
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ ì‚¬ë¡€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_response_with_context(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±"""
        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
            conversation_context = self.memory.get_recent_context(3)
            
            # ë¬¸ì„œ ì •ë³´ í¬ë§·íŒ…
            context_text = self._format_documents_for_prompt(documents)
            
            # ì‚¬ìš©ì íŒ¨í„´ ë¶„ì„
            user_patterns = self.memory.get_user_patterns()
            pattern_info = f"ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼: {', '.join(user_patterns.keys())}" if user_patterns else ""
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ë¬´ì—­ ì—…ë¬´ ì‹¤ë¬´ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œ ë¯¼ì›ìƒë‹´ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™:**
1. **ì‹¤ìš©ì„± ìµœìš°ì„ **: ì‹¤ì œ ì—…ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
2. **ê²½í—˜ ê¸°ë°˜**: ì œê³µëœ ìƒë‹´ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ì¦ëœ í•´ê²°ë°©ë²•ê³¼ ì ˆì°¨ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.
3. **ë‹¨ê³„ë³„ ì•ˆë‚´**: ë³µì¡í•œ ì ˆì°¨ëŠ” ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
4. **ì˜ˆì™¸ìƒí™© ê³ ë ¤**: ì¼ë°˜ì ì¸ ê²½ìš°ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ˆì™¸ìƒí™©ê³¼ íŠ¹ìˆ˜í•œ ê²½ìš°ë„ í•¨ê»˜ ì•ˆë‚´í•˜ì„¸ìš”.
5. **ê´€ë ¨ ê¸°ê´€ ì—°ê³„**: í•„ìš”ì‹œ ë‹´ë‹¹ ê¸°ê´€ê³¼ ì—°ë½ì²˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

ê´€ë ¨ ìƒë‹´ ì‚¬ë¡€:
{context_text}

{pattern_info}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
1. ì‹¤ì œ ìƒë‹´ ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ê²€ì¦ëœ í•´ê²°ë°©ë²• ì œì‹œ
2. ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ë²•ê³¼ í•„ìš” ì„œë¥˜ ì•ˆë‚´
3. í”í•œ ì‹¤ìˆ˜ì™€ ì£¼ì˜í•  ì  ë¯¸ë¦¬ ì•ˆë‚´
4. ë‹´ë‹¹ ê¸°ê´€ê³¼ ì—°ë½ì²˜ ì •ë³´ ì œê³µ
5. ìƒë‹´ ì‚¬ë¡€ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì ìš© ì‹œ ê´€ë ¨ ê¸°ê´€ì— ìµœì¢… í™•ì¸ í•„ìš”í•¨ì„ ì•ˆë‚´"""
            
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend(conversation_context)
            messages.append({"role": "user", "content": query})
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=1200
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"ìƒë‹´ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ìƒë‹´ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_fallback_response(self, query: str) -> str:
        """ë¬¸ì„œ ì—†ì´ ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±"""
        return f"'{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ìƒë‹´ ì‚¬ë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
    
    def _format_documents_for_prompt(self, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_docs = []
        
        for doc in documents:
            metadata = doc.get("metadata", {})
            content = doc.get("content", "")
            
            # ìƒë‹´ ì‚¬ë¡€ íŠ¹í™” í¬ë§·íŒ…
            doc_info = []
            if metadata.get("case_number"):
                doc_info.append(f"ì‚¬ë¡€ë²ˆí˜¸: {metadata['case_number']}")
            if metadata.get("category"):
                doc_info.append(f"ë¶„ì•¼: {metadata['category']}")
            if metadata.get("sub_category"):
                doc_info.append(f"ì„¸ë¶€ë¶„ì•¼: {metadata['sub_category']}")
            if metadata.get("consultation_type"):
                doc_info.append(f"ìƒë‹´ìœ í˜•: {metadata['consultation_type']}")
            
            doc_header = " | ".join(doc_info) if doc_info else "ìƒë‹´ ì‚¬ë¡€"
            formatted_docs.append(f"[{doc_header}]\n{content}")
        
        return "\n\n".join(formatted_docs)
    
    def get_conversation_summary(self) -> str:
        """í˜„ì¬ ìƒë‹´ ëŒ€í™”ì˜ ìš”ì•½ ìƒì„±"""
        try:
            if not self.memory.messages:
                return "ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = ""
            for msg in self.memory.messages:
                role = "ìƒë‹´ì" if msg["role"] == "user" else "ìƒë‹´ì›"
                conversation_text += f"{role}: {msg['content']}\n\n"
            
            # ì‚¬ìš©ì íŒ¨í„´ ì •ë³´ ì¶”ê°€
            user_patterns = self.memory.get_user_patterns()
            
            # GPTë¥¼ ì‚¬ìš©í•œ ìƒë‹´ ìš”ì•½ ìƒì„±
            summary_prompt = f"""ë‹¤ìŒ ë¬´ì—­ ì—…ë¬´ ìƒë‹´ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{conversation_text}

ìƒë‹´ ì£¼ì œ ë¶„í¬: {user_patterns}

ìƒë‹´ ìš”ì•½:"""
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.2,
                max_tokens=400
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"ìƒë‹´ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ìƒë‹´ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def reset_conversation(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear_history()
        logger.info("ìƒë‹´ ì‚¬ë¡€ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”ë¨")
    
    def get_agent_stats(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            retriever_stats = self.retriever.vector_store.get_collection_stats() if hasattr(self.retriever, 'vector_store') else {}
            return {
                "agent_type": "complaints",
                "model_name": self.model_name,
                "temperature": self.temperature,
                "max_context_docs": self.max_context_docs,
                "similarity_threshold": self.similarity_threshold,
                "conversation_stats": {
                    "total_messages": len(self.memory.messages),
                    "context_cases": len(self.memory.case_context),
                    "user_patterns": self.memory.get_user_patterns()
                },
                "retriever_stats": retriever_stats
            }
        except Exception as e:
            logger.error(f"ìƒë‹´ ì—ì´ì „íŠ¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ConsultationCaseAgent = ComplaintsAgent
ConsultationCaseRetriever = ComplaintsRetriever