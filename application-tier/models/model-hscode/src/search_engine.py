# search_engine.py - ê²€ìƒ‰ ì—”ì§„ ë‹´ë‹¹ (final_combined_text ëŒ€ì‘)

import pandas as pd
import numpy as np
import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (model-hscode í´ë”)
sys.path.append(str(Path(__file__).parent.parent))
from config import SYSTEM_CONFIG

class SearchEngine:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ í´ë˜ìŠ¤ (final_combined_text ì§€ì›)"""
    
    def __init__(self, semantic_model_name: str = None):
        self.semantic_model_name = semantic_model_name or SYSTEM_CONFIG['semantic_model']
        self.top_k = SYSTEM_CONFIG['top_k']
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.tfidf_vectorizer = TfidfVectorizer(**SYSTEM_CONFIG['tfidf_config'])
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.semantic_model = SentenceTransformer(self.semantic_model_name, device=device)
        
        # ë°ì´í„° ë° ì¸ë±ìŠ¤
        self.integrated_df = None
        self.tfidf_matrix = None
        self.semantic_embeddings = None
        
        # ë§¤í•‘ ë°ì´í„°
        self.standard_mapping = {}
        self.reverse_mapping = {}
        self.chapter_descriptions = {}
        self.hs_hierarchy = None
        
        # ë™ì˜ì–´ ì‚¬ì „
        self.synonym_dict = self._build_synonym_dictionary()
        
        print(f"ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”: {self.semantic_model_name}")
        print(f"ë™ì˜ì–´ ì‚¬ì „: {len(self.synonym_dict)}ê°œ ì¹´í…Œê³ ë¦¬")
    
    def _build_synonym_dictionary(self) -> Dict:
        """ë™ì˜ì–´/ìœ ì‚¬ì–´ ì‚¬ì „ êµ¬ì¶•"""
        return {
            # ë‚˜ì‚¬/ë³¼íŠ¸ë¥˜
            'fasteners': {
                'keywords': ['ë‚˜ì‚¬', 'ë³¼íŠ¸', 'ìŠ¤í¬ë¥˜', 'ë„ˆíŠ¸', 'ì™€ì…”', 'screw', 'bolt', 'nut', 'washer', 
                           'ì²´ê²°êµ¬', 'ì¡°ì„ì‡ ', 'ê³ ì •êµ¬', 'fastener', 'fixing', 'ë³¼íŠ¸ë„ˆíŠ¸', 'ìœ¡ê°ë³¼íŠ¸',
                           'ì‹­ìë‚˜ì‚¬', 'ì¼ìë‚˜ì‚¬', 'ëª©ë‚˜ì‚¬', 'ì² ë‚˜ì‚¬', 'ê¸°ê³„ë‚˜ì‚¬', 'íƒœí•‘ë‚˜ì‚¬', 'tapping'],
                'boost_chapters': ['73'],
                'boost_headings': ['7318'],
                'negative_keywords': ['íŒŒì´í”„', 'ê´€', 'ë“œë¦´', 'pipe', 'tube', 'drill']
            },
            
            # ì „ì/ì „ê¸°ì œí’ˆ
            'electronics': {
                'keywords': ['ì „ì', 'ì „ê¸°', 'ë””ì§€í„¸', 'electronic', 'electric', 'digital',
                           'ë°˜ë„ì²´', 'semiconductor', 'IC', 'ì¹©', 'chip', 'íšŒë¡œ', 'circuit',
                           'íŠ¸ëœì§€ìŠ¤í„°', 'transistor', 'ë‹¤ì´ì˜¤ë“œ', 'diode', 'ì €í•­', 'resistor',
                           'ì½˜ë´ì„œ', 'capacitor', 'ì„¼ì„œ', 'sensor'],
                'boost_chapters': ['84', '85'],
                'boost_headings': ['8471', '8473', '8517', '8518', '8541', '8542']
            },
            
            # ì»´í“¨í„°/IT
            'computer': {
                'keywords': ['ì»´í“¨í„°', 'ë…¸íŠ¸ë¶', 'ëª¨ë‹ˆí„°', 'í‚¤ë³´ë“œ', 'ë§ˆìš°ìŠ¤', 'computer', 'laptop', 
                           'monitor', 'keyboard', 'mouse', 'PC', 'í”„ë¦°í„°', 'printer', 'ìŠ¤ìºë„ˆ', 'scanner',
                           'ì„œë²„', 'server', 'í•˜ë“œë””ìŠ¤í¬', 'HDD', 'SSD', 'USB', 'ë©”ëª¨ë¦¬', 'memory',
                           'ê·¸ë˜í”½ì¹´ë“œ', 'GPU', 'CPU', 'ë§ˆë”ë³´ë“œ', 'motherboard'],
                'boost_chapters': ['84', '85'],
                'boost_headings': ['8471', '8443', '8517', '8523']
            },
            
            # ì¸ì‡„/í† ë„ˆ
            'printing': {
                'keywords': ['í† ë„ˆ', 'ì¹´íŠ¸ë¦¬ì§€', 'ì‰í¬', 'toner', 'cartridge', 'ink', 'ì¸ì‡„', 'print',
                           'í”„ë¦°í„°', 'printer', 'ë³µì‚¬ê¸°', 'copier', 'íŒ©ìŠ¤', 'fax', 'ìŠ¤ìºë„ˆ', 'scanner',
                           'ì‰í¬ì ¯', 'inkjet', 'ë ˆì´ì €', 'laser', 'ë“œëŸ¼', 'drum', 'ì •ì°©ê¸°', 'fuser'],
                'boost_chapters': ['84'],
                'boost_headings': ['8443', '8471']
            },
            
       
            
            # í™”í•™ì œí’ˆ
            'chemical': {
                'keywords': ['í™”í•™', 'í™”í•©ë¬¼', 'ìš©ì•¡', 'ìš©ë§¤', 'chemical', 'compound', 'solution', 'solvent',
                           'ì‚°', 'acid', 'ì—¼ê¸°', 'base', 'ì´‰ë§¤', 'catalyst', 'ì‹œì•½', 'reagent',
                           'ì ‘ì°©ì œ', 'adhesive', 'ë„ë£Œ', 'paint', 'ì‰í¬', 'ink', 'ì„¸ì œ', 'detergent'],
                'boost_chapters': ['28', '29', '32', '34', '38'],
                'boost_headings': []
            },
            
            # í”Œë¼ìŠ¤í‹±
            'plastic': {
                'keywords': ['í”Œë¼ìŠ¤í‹±', 'ìˆ˜ì§€', 'í´ë¦¬ë¨¸', 'plastic', 'resin', 'polymer', 'PE', 'PP', 'PVC',
                           'í•©ì„±ìˆ˜ì§€', 'synthetic', 'ABS', 'PC', 'í´ë¦¬ì¹´ë³´ë„¤ì´íŠ¸', 'polycarbonate',
                           'ì•„í¬ë¦´', 'acrylic', 'PET', 'í´ë¦¬ì—í‹¸ë Œ', 'polyethylene'],
                'boost_chapters': ['39'],
                'boost_headings': []
            },
            
            # ê¸ˆì†
            'metal': {
                'keywords': ['ê¸ˆì†', 'ì² ', 'ê°•', 'ì•Œë£¨ë¯¸ëŠ„', 'êµ¬ë¦¬', 'metal', 'iron', 'steel', 'aluminum', 'copper',
                           'ìŠ¤í…Œì¸ë ˆìŠ¤', 'stainless', 'í•©ê¸ˆ', 'alloy', 'í™©ë™', 'brass', 'ì²­ë™', 'bronze',
                           'ì•„ì—°', 'zinc', 'ë‹ˆì¼ˆ', 'nickel', 'í‹°íƒ€ëŠ„', 'titanium', 'ë§ˆê·¸ë„¤ìŠ˜', 'magnesium'],
                'boost_chapters': ['72', '73', '74', '75', '76', '78', '79', '81'],
                'boost_headings': []
            },
            
            # ë³´ì„/ê·€ê¸ˆì†
            'jewelry': {
                'keywords': ['ê¸ˆ', 'ì€', 'ë°±ê¸ˆ', 'ê·€ê¸ˆì†', 'ë³´ì„', 'ë‹¤ì´ì•„ëª¬ë“œ', 'ë°˜ì§€', 'ëª©ê±¸ì´', 'ê·€ê±¸ì´', 
                           'gold', 'silver', 'platinum', 'jewelry', 'diamond', 'ring', 'necklace', 'earring',
                           'ì¥ì‹ êµ¬', 'íŒ”ì°Œ', 'bracelet', 'ë¸Œë¡œì¹˜', 'brooch', 'ì‹œê³„', 'watch',
                           'ì§„ì£¼', 'pearl', 'ë£¨ë¹„', 'ruby', 'ì‚¬íŒŒì´ì–´', 'sapphire', 'ì—ë©”ë„ë“œ', 'emerald'],
                'boost_chapters': ['71'],
                'boost_headings': ['7113', '7114', '7115', '7116', '7117']
            },
            
            # ì¶œíŒë¬¼/ë„ì„œ
            'books': {
                'keywords': ['ì±…', 'ë„ì„œ', 'ì„œì ', 'ë§Œí™”', 'ì†Œì„¤', 'êµê³¼ì„œ', 'ì°¸ê³ ì„œ', 'book', 'novel', 'textbook',
                           'ë§Œí™”ì±…', 'ì›¹íˆ°', 'ë¼ì´íŠ¸ë…¸ë²¨', 'ì½”ë¯¹', 'comic', 'manga', 'webtoon',
                           'ì¶œíŒë¬¼', 'ì¸ì‡„ë¬¼', 'ì¡ì§€', 'magazine', 'ì‹ ë¬¸', 'newspaper', 'ì¹´íƒˆë¡œê·¸', 'catalog',
                           'í¬ìŠ¤í„°', 'poster', 'ì „ë‹¨ì§€', 'leaflet', 'ë¸Œë¡œì…”', 'brochure', 'íŒœí”Œë ›', 'pamphlet',
                           'ì§€ë„', 'map', 'ë„ë©´', 'drawing', 'ì•…ë³´', 'music sheet'],
                'boost_chapters': ['49'],
                'boost_headings': ['4901', '4902', '4903', '4904', '4905', '4906', '4909', '4911']
            },
            
            # ìë™ì°¨ ë¶€í’ˆ
            'automotive': {
                'keywords': ['ìë™ì°¨', 'ì°¨ëŸ‰', 'ì—”ì§„', 'íƒ€ì´ì–´', 'automotive', 'vehicle', 'engine', 'tire',
                           'ë¸Œë ˆì´í¬', 'brake', 'ë°°í„°ë¦¬', 'battery', 'ë¶€í’ˆ', 'parts', 'í•„í„°', 'filter',
                           'ì í™”í”ŒëŸ¬ê·¸', 'spark plug', 'ì˜¤ì¼', 'oil', 'ì¿¨ëŸ°íŠ¸', 'coolant', 'ë²¨íŠ¸', 'belt',
                           'í˜¸ìŠ¤', 'hose', 'ë²”í¼', 'bumper', 'í—¤ë“œë¼ì´íŠ¸', 'headlight'],
                'boost_chapters': ['84', '87'],
                'boost_headings': ['8407', '8408', '8483', '8507', '8708']
            },
            
            # ì˜ë£Œê¸°ê¸°
            'medical': {
                'keywords': ['ì˜ë£Œ', 'ì˜ë£Œê¸°ê¸°', 'ë³‘ì›', 'ì¹˜ë£Œ', 'medical', 'hospital', 'treatment',
                           'ìˆ˜ìˆ ', 'surgery', 'ì§„ë‹¨', 'diagnosis', 'í˜ˆì••ê³„', 'ì²´ì˜¨ê³„', 'ì²­ì§„ê¸°',
                           'ì£¼ì‚¬ê¸°', 'syringe', 'ë¶•ëŒ€', 'bandage', 'ë§ˆìŠ¤í¬', 'mask'],
                'boost_chapters': ['90'],
                'boost_headings': ['9018', '9019', '9020', '9021', '9022']
            },
            
            # ìŠ¤í¬ì¸ /ë ˆì €
            'sports': {
                'keywords': ['ìŠ¤í¬ì¸ ', 'ìš´ë™', 'í—¬ìŠ¤', 'í”¼íŠ¸ë‹ˆìŠ¤', 'sports', 'fitness', 'exercise',
                           'ê³¨í”„', 'golf', 'í…Œë‹ˆìŠ¤', 'tennis', 'ì¶•êµ¬', 'soccer', 'ë†êµ¬', 'basketball',
                           'ìˆ˜ì˜', 'swimming', 'ìì „ê±°', 'bicycle', 'ë“±ì‚°', 'climbing'],
                'boost_chapters': ['95'],
                'boost_headings': ['9506', '9507']
            }
        }
    
    def load_data(self, integrated_df: pd.DataFrame, 
                  standard_mapping: Dict = None,
                  reverse_mapping: Dict = None,
                  chapter_descriptions: Dict = None):
        """ê²€ìƒ‰ì— í•„ìš”í•œ ë°ì´í„° ë¡œë“œ (final_combined_text ì§€ì›)"""
        print("ê²€ìƒ‰ ì—”ì§„ì— ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        self.integrated_df = integrated_df.copy()
        self.standard_mapping = standard_mapping or {}
        self.reverse_mapping = reverse_mapping or {}
        self.chapter_descriptions = chapter_descriptions or {}
        
        # ğŸ” ë°ì´í„° êµ¬ì¡° í™•ì¸
        print(f"  ë¡œë“œëœ ë°ì´í„°: {len(self.integrated_df)}ê°œ í•­ëª©")
        print(f"  ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(self.integrated_df.columns)}")
        
        # final_combined_text ì»¬ëŸ¼ í™•ì¸
        if 'final_combined_text' not in self.integrated_df.columns:
            print("  âŒ final_combined_text ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            # ëŒ€ì²´ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
            text_candidates = ['combined_text', 'í•œê¸€í’ˆëª©ëª…', 'í‘œì¤€í’ˆëª…']
            for candidate in text_candidates:
                if candidate in self.integrated_df.columns:
                    print(f"  â¡ï¸ ëŒ€ì²´ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì‚¬ìš©: {candidate}")
                    self.integrated_df['final_combined_text'] = self.integrated_df[candidate].fillna('')
                    break
            else:
                print("  âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                return False
        
        # ê³„ì¸µ êµ¬ì¡° êµ¬ì¶•
        self._build_hierarchy()
        
        print(f"  ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.integrated_df)}ê°œ í•­ëª©")
        print(f"  í‘œì¤€í’ˆëª… ë§¤í•‘: {len(self.standard_mapping)}ê°œ")
        return True
    
    def _build_hierarchy(self):
        """HS ê³„ì¸µ êµ¬ì¡° êµ¬ì¶•"""
        self.hs_hierarchy = {
            'chapters': defaultdict(list),
            'headings': defaultdict(list),
            'subheadings': defaultdict(list)
        }
        
        # chapter, heading, subheading ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ HS_KEYì—ì„œ ì¶”ì¶œ
        if 'chapter' not in self.integrated_df.columns:
            self.integrated_df['chapter'] = self.integrated_df['HS_KEY'].str[:2]
        if 'heading' not in self.integrated_df.columns:
            self.integrated_df['heading'] = self.integrated_df['HS_KEY'].str[:4]
        if 'subheading' not in self.integrated_df.columns:
            self.integrated_df['subheading'] = self.integrated_df['HS_KEY'].str[:6]
        
        for chapter, group in self.integrated_df.groupby('chapter'):
            self.hs_hierarchy['chapters'][chapter] = group.index.tolist()
            
        for heading, group in self.integrated_df.groupby('heading'):
            self.hs_hierarchy['headings'][heading] = group.index.tolist()
            
        for subheading, group in self.integrated_df.groupby('subheading'):
            self.hs_hierarchy['subheadings'][subheading] = group.index.tolist()
    
    def build_index(self):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• (final_combined_text ê¸°ë°˜)"""
        if self.integrated_df is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        print("ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # final_combined_text ì‚¬ìš©
        if 'final_combined_text' not in self.integrated_df.columns:
            raise ValueError("final_combined_text ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # í…ìŠ¤íŠ¸ ì¤€ë¹„
        texts = self.integrated_df['final_combined_text'].fillna('').astype(str).tolist()
        texts = [text if text.strip() else 'empty_text' for text in texts]
        
        print(f"  ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬")
        print(f"  í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {np.mean([len(text) for text in texts]):.1f}ì")
        
        # TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•
        print("  TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±...")
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        
        # ì˜ë¯¸ ì„ë² ë”© êµ¬ì¶•
        print("  ì˜ë¯¸ ì„ë² ë”© ìƒì„±...")
        self.semantic_embeddings = self.semantic_model.encode(
            texts, 
            convert_to_tensor=True,
            show_progress_bar=True,
            batch_size=32
        )
        
        print("ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print(f"  TF-IDF ë§¤íŠ¸ë¦­ìŠ¤: {self.tfidf_matrix.shape}")
        print(f"  ì˜ë¯¸ ì„ë² ë”©: {self.semantic_embeddings.shape}")
    
    def expand_query(self, query: str) -> str:
        """ì¿¼ë¦¬ í™•ì¥ - ë™ì˜ì–´/ìœ ì‚¬ì–´ ì¶”ê°€"""
        expanded_terms = [query]
        query_lower = query.lower()
        
        for category, data in self.synonym_dict.items():
            keywords = data['keywords']
            
            # ì¿¼ë¦¬ì— í¬í•¨ëœ í‚¤ì›Œë“œ ì°¾ê¸°
            matched_keywords = []
            for keyword in keywords:
                if keyword.lower() in query_lower or query_lower in keyword.lower():
                    matched_keywords.append(keyword)
            
            # ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê´€ë ¨ ìš©ì–´ë“¤ ì¶”ê°€
            if matched_keywords:
                for keyword in keywords:
                    if keyword.lower() not in query_lower:
                        expanded_terms.append(keyword)
                
                print(f"  '{category}' ì¹´í…Œê³ ë¦¬ í™•ì¥: {len(keywords)}ê°œ ìš©ì–´ ì¶”ê°€")
                break
        
        unique_terms = list(dict.fromkeys(expanded_terms))
        expanded_query = ' '.join(unique_terms)
        
        if len(unique_terms) > 1:
            print(f"  ì¿¼ë¦¬ í™•ì¥: '{query}' -> {len(unique_terms)}ê°œ ìš©ì–´")
        
        return expanded_query
    
    def get_category_boost_info(self, query: str) -> Dict:
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶€ìŠ¤íŠ¸ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        boost_info = {'chapters': [], 'headings': []}
        
        for category, data in self.synonym_dict.items():
            keywords = data['keywords']
            
            if any(keyword.lower() in query_lower or query_lower in keyword.lower() 
                   for keyword in keywords):
                boost_info['chapters'].extend(data.get('boost_chapters', []))
                boost_info['headings'].extend(data.get('boost_headings', []))
                print(f"  '{category}' ì¹´í…Œê³ ë¦¬ ë¶€ìŠ¤íŠ¸ ì ìš©")
                break
        
        return boost_info
    
    def search_with_standard_names(self, query: str) -> List[str]:
        """í‘œì¤€í’ˆëª… ë§¤í•‘ì„ í™œìš©í•œ ì§ì ‘ ê²€ìƒ‰"""
        queries_to_search = [query.lower().strip()]
        
        # ì¿¼ë¦¬ í™•ì¥
        expanded_query = self.expand_query(query)
        if expanded_query != query:
            expanded_terms = expanded_query.split()
            queries_to_search.extend([term.lower().strip() for term in expanded_terms])
        
        direct_matches = []
        
        for search_query in queries_to_search:
            # ì •í™•í•œ ë§¤ì¹­
            if search_query in self.standard_mapping:
                direct_matches.extend(self.standard_mapping[search_query])
                
            # ë¶€ë¶„ ë§¤ì¹­
            for std_name, hs_codes in self.standard_mapping.items():
                if search_query in std_name or std_name in search_query:
                    direct_matches.extend(hs_codes)
                    
        return list(set(direct_matches))
    
    def calculate_dynamic_weights(self, query: str) -> Tuple[float, float]:
        """ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        words = query.split()
        query_length = len(words)
        
        if query_length <= 2:
            return 0.7, 0.3  # ì§§ì€ ì¿¼ë¦¬ - í‚¤ì›Œë“œ ì¤‘ì‹¬
        elif query_length >= 8:
            return 0.6, 0.4  # í‚¤ì›Œë“œ ë¹„ì¤‘ ìœ ì§€
        elif query_length >= 5:
            return 0.4, 0.6  # ê¸´ ì¿¼ë¦¬ - ì˜ë¯¸ ì¤‘ì‹¬
        else:
            return 0.5, 0.5  # ì¤‘ê°„ - ê· í˜•
    
    def search(self, query: str, material: str = "", usage: str = "") -> pd.DataFrame:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (final_combined_text ê¸°ë°˜)"""
        if self.integrated_df is None or self.tfidf_matrix is None:
            raise ValueError("ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰: '{query}'")
        
        try:
            # 1. ì¿¼ë¦¬ í™•ì¥
            expanded_query = self.expand_query(query)
            
            # 2. ì¹´í…Œê³ ë¦¬ë³„ ë¶€ìŠ¤íŠ¸ ì •ë³´
            boost_info = self.get_category_boost_info(query)
            
            # 3. í‘œì¤€í’ˆëª… ì§ì ‘ ë§¤ì¹­
            direct_hs_codes = self.search_with_standard_names(query)
            if direct_hs_codes:
                print(f"  í‘œì¤€í’ˆëª… ë§¤ì¹­: {len(direct_hs_codes)}ê°œ HS ì½”ë“œ")
            
            # 4. ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
            keyword_weight, semantic_weight = self.calculate_dynamic_weights(expanded_query)
            print(f"  ë™ì  ê°€ì¤‘ì¹˜: í‚¤ì›Œë“œ={keyword_weight:.2f}, ì˜ë¯¸={semantic_weight:.2f}")
            
            # 5. ì „ì²´ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            full_query = f"{expanded_query} {material} {usage}".strip()
            
            # 6. TF-IDF ê²€ìƒ‰
            query_tfidf = self.tfidf_vectorizer.transform([full_query])
            keyword_scores = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()
            
            # 7. ì˜ë¯¸ ê²€ìƒ‰
            semantic_queries = [query, expanded_query] if expanded_query != query else [query]
            semantic_scores_list = []
            
            for sq in semantic_queries:
                sq_full = f"{sq} {material} {usage}".strip()
                query_embedding = self.semantic_model.encode([sq_full], convert_to_tensor=True)
                scores = cosine_similarity(
                    query_embedding.cpu().numpy(),
                    self.semantic_embeddings.cpu().numpy()
                ).flatten()
                semantic_scores_list.append(scores)
            
            semantic_scores = np.mean(semantic_scores_list, axis=0)
            
            # 8. í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
            hybrid_scores = (keyword_weight * keyword_scores + 
                            semantic_weight * semantic_scores)
            
            # 9. í‘œì¤€í’ˆëª… ê¸°ë°˜ ë³´ë„ˆìŠ¤
            if direct_hs_codes:
                # HSë¶€í˜¸ ì»¬ëŸ¼ ì°¾ê¸°
                hs_code_col = None
                for col in ['HSë¶€í˜¸', 'HS_KEY']:
                    if col in self.integrated_df.columns:
                        hs_code_col = col
                        break
                
                if hs_code_col:
                    for hs_code in direct_hs_codes:
                        mask = self.integrated_df[hs_code_col] == hs_code
                        indices = self.integrated_df[mask].index
                        for idx in indices:
                            if idx < len(hybrid_scores):
                                hybrid_scores[idx] *= 2.0  # 100% ë³´ë„ˆìŠ¤
            
            # 10. í‘œì¤€í’ˆëª… ì „ìš© í•­ëª© ë³´ë„ˆìŠ¤
            if 'data_source' in self.integrated_df.columns:
                standard_only_mask = self.integrated_df['data_source'].str.contains('std', na=False)
                if standard_only_mask.any():
                    hybrid_scores[standard_only_mask] *= 1.3  # 30% ë³´ë„ˆìŠ¤
            
            # 11. ì¹´í…Œê³ ë¦¬ë³„ ë¶€ìŠ¤íŠ¸ ì ìš©
            if boost_info['chapters']:
                for chapter in boost_info['chapters']:
                    mask = self.integrated_df['chapter'] == chapter
                    hybrid_scores[mask] *= 1.5  # 50% ë¶€ìŠ¤íŠ¸
            
            if boost_info['headings']:
                for heading in boost_info['headings']:
                    mask = self.integrated_df['heading'] == heading
                    hybrid_scores[mask] *= 2.0  # 100% ë¶€ìŠ¤íŠ¸
            
            # 12. ê²°ê³¼ ìƒì„±
            results = self.integrated_df.copy()
            results['keyword_score'] = keyword_scores
            results['semantic_score'] = semantic_scores
            results['hybrid_score'] = hybrid_scores
            results['is_standard_match'] = results.get('HSë¶€í˜¸', results.get('HS_KEY', '')).isin(direct_hs_codes)
            results['expanded_query'] = expanded_query
            
            # 13. ë„¤ê±°í‹°ë¸Œ í•„í„°ë§
            for category, data in self.synonym_dict.items():
                if any(word.lower() in query.lower() for word in data['keywords']):
                    negative_keywords = data.get('negative_keywords', [])
                    for neg_keyword in negative_keywords:
                        # í•œê¸€í’ˆëª©ëª… ì»¬ëŸ¼ ì°¾ê¸°
                        for col in ['í•œê¸€í’ˆëª©ëª…', 'final_combined_text']:
                            if col in results.columns:
                                mask = results[col].str.contains(neg_keyword, case=False, na=False)
                                results.loc[mask, 'hybrid_score'] *= 0.2  # 80% ê°ì 
                                break
            
        except Exception as e:
            print(f"  ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("  ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±...")
            
            # í´ë°±: ê¸°ë³¸ TF-IDF ê²€ìƒ‰ë§Œ
            query_tfidf = self.tfidf_vectorizer.transform([query])
            keyword_scores = cosine_similarity(query_tfidf, self.tfidf_matrix).flatten()
            
            results = self.integrated_df.copy()
            results['hybrid_score'] = keyword_scores
            results['keyword_score'] = keyword_scores
            results['semantic_score'] = 0.0
            results['is_standard_match'] = False
        
        # 14. ì •ë ¬ ë° ìƒìœ„ ì„ íƒ
        results = results.sort_values('hybrid_score', ascending=False)
        
        # 15. ì´ìƒì¹˜ í•„í„°ë§
        if len(results) >= 10:
            top_10_chapters = results.head(10)['chapter'].value_counts()
            dominant_chapters = top_10_chapters[top_10_chapters >= 2].index.tolist()
            
            if dominant_chapters:
                print(f"  ì£¼ìš” ì¥ ê°ì§€: {dominant_chapters}")
                
                main_chapter_scores = results[results['chapter'].isin(dominant_chapters)]['hybrid_score']
                other_chapter_scores = results[~results['chapter'].isin(dominant_chapters)]['hybrid_score']
                
                if len(main_chapter_scores) > 0 and len(other_chapter_scores) > 0:
                    main_avg = main_chapter_scores.mean()
                    other_avg = other_chapter_scores.mean()
                    
                    if main_avg > other_avg * 1.5:
                        results = results[results['chapter'].isin(dominant_chapters)]
                        print(f"  {len(results)}ê°œ í•­ëª©ìœ¼ë¡œ í•„í„°ë§ë¨")
        
        # 16. ìµœì¢… ìƒìœ„ ì„ íƒ
        max_results = min(self.top_k, len(results))
        results = results.head(max_results)
        
        print(f"  ìµœì¢… í›„ë³´: {len(results)}ê°œ")
        if len(results) > 0:
            print(f"    ì ìˆ˜ ë²”ìœ„: {results['hybrid_score'].min():.3f} ~ {results['hybrid_score'].max():.3f}")
            chapters = results['chapter'].value_counts()
            print(f"    ì¥ ë¶„í¬: {dict(chapters.head(3))}")
        
        return results
    
    def get_search_stats(self) -> Dict:
        """ê²€ìƒ‰ ì—”ì§„ í†µê³„ ë°˜í™˜"""
        stats = {
            'model_name': self.semantic_model_name,
            'synonym_categories': len(self.synonym_dict),
            'standard_mappings': len(self.standard_mapping),
            'chapter_descriptions': len(self.chapter_descriptions)
        }
        
        if self.integrated_df is not None:
            stats['total_items'] = len(self.integrated_df)
            stats['chapters'] = len(self.integrated_df['chapter'].unique())
        
        if self.tfidf_matrix is not None:
            stats['tfidf_features'] = self.tfidf_matrix.shape[1]
            stats['tfidf_density'] = self.tfidf_matrix.nnz / (self.tfidf_matrix.shape[0] * self.tfidf_matrix.shape[1])
        
        if self.semantic_embeddings is not None:
            stats['embedding_dim'] = self.semantic_embeddings.shape[1]
        
        return stats