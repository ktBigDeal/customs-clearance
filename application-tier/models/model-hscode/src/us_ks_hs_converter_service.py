import numpy as np
from typing import Dict, List, Optional, Tuple
import os
import sys
import re
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch
from dataclasses import dataclass
import pandas as pd
from dotenv import load_dotenv

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

def convert_numpy_types(obj):
    """NumPy íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    elif pd.isna(obj):
        return None
    else:
        return obj


@dataclass
class ProductAnalysis:
    """LLM ë¶„ì„ ê²°ê³¼ êµ¬ì¡°ì²´"""
    material: str
    function: str
    target_user: str
    size_category: str
    key_features: List[str]
    alternative_hs6: List[str]
    reasoning: str

@dataclass
class LLMRanking:
    """LLM ìˆœìœ„ ë§¤ê¹€ ê²°ê³¼"""
    rank: int
    hs_code: str
    confidence: float
    reason: str

class HSCodeStructureAnalyzer:
    """HS ì½”ë“œ êµ¬ì¡° ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_hs_components(hs_code: str) -> Dict[str, str]:
        """HS ì½”ë“œë¥¼ êµ¬ì„± ìš”ì†Œë³„ë¡œ ë¶„í•´"""
        hs_code = str(hs_code).strip()
        
        if len(hs_code) <= 10:
            hs_code = hs_code.ljust(10, '0')
        else:
            hs_code = hs_code[:10]
        
        return {
            'full_code': hs_code,
            'chapter': hs_code[:2],
            'heading': hs_code[:4],
            'subheading': hs_code[:6],
            'detailed': hs_code[6:],
            'hs6': hs_code[:6],
            'hs4_national': hs_code[6:]
        }
    
    @staticmethod
    def is_valid_hs6_match(us_code: str, korea_code: str) -> bool:
        """HS 6ìë¦¬(êµ­ì œ ê³µí†µ) ë§¤ì¹­ ê²€ì¦"""
        us_hs6 = str(us_code).zfill(10)[:6]
        korea_hs6 = str(korea_code).zfill(10)[:6]
        return us_hs6 == korea_hs6
    
    @staticmethod
    def calculate_hs_similarity(us_code: str, korea_code: str) -> float:
        """HS êµ¬ì¡° ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        us_comp = HSCodeStructureAnalyzer.get_hs_components(us_code)
        korea_comp = HSCodeStructureAnalyzer.get_hs_components(korea_code)
        
        if us_comp['hs6'] != korea_comp['hs6']:
            return 0.0
        
        similarity = 0.6
        if us_comp['heading'] == korea_comp['heading']:
            similarity += 0.2
        if us_comp['full_code'] == korea_comp['full_code']:
            similarity += 0.2
        
        return min(similarity, 1.0)

class HSCodeConverterService:
    """HS ì²´ê³„ë¥¼ ë°˜ì˜í•œ ë¯¸êµ­â†’í•œêµ­ HSì½”ë“œ ë³€í™˜ ì„œë¹„ìŠ¤ (LLM ê°•í™”)"""
    
    def __init__(self, us_tariff_file: str = None, korea_recommender_system=None,
                 openai_api_key: str = None):
        self.us_tariff_file = us_tariff_file
        self.korea_recommender = korea_recommender_system
        
        # ë¯¸êµ­ ë°ì´í„°
        self.us_data = None
        self.us_hs6_index = {}
        
        # í•œêµ­ ë°ì´í„°
        self.korea_data = None
        self.korea_hs6_index = {}
        
        # HS 6ìë¦¬ ë¶„ë¥˜ ì„¤ëª…
        self.hs6_descriptions = {}
        
        # HS êµ¬ì¡° ë¶„ì„ê¸°
        self.hs_analyzer = HSCodeStructureAnalyzer()
        
        # ë³€í™˜ ìºì‹œ
        self.conversion_cache = {}
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì—”ì§„
        self.semantic_model = None
        
        # LLM ê´€ë ¨ ì„¤ì •
        self.openai_client = None
        self.llm_available = False
        self.use_llm = False
        
        # LLM ì´ˆê¸°í™” ì‹œë„
        if openai_api_key and OPENAI_AVAILABLE:
            try:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                # API í‚¤ ìœ íš¨ì„± ê°„ë‹¨ í…ŒìŠ¤íŠ¸
                test_response = self.openai_client.models.list()
                self.llm_available = True
                self.use_llm = True
                print("ğŸ¤– OpenAI LLM ì—°ê²° ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ OpenAI LLM ì—°ê²° ì‹¤íŒ¨: {e}")
                print("ğŸ“Š ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.llm_available = False
                self.openai_client = None
        
        # LLM ìºì‹œ
        self.llm_analysis_cache = {}
        self.llm_ranking_cache = {}
        self.initialized = False
        
        llm_status = "LLM ê°•í™”" if self.llm_available else "ê¸°ë³¸ ëª¨ë“œ"
        print(f"HS ì½”ë“œ ë³€í™˜ ì‹œìŠ¤í…œ ({llm_status}) ì´ˆê¸°í™”")
    
    def initialize_system(self, progress_callback=None):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if progress_callback:
                progress_callback(0.1, "ì‹œë§¨í‹± ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # ì‹œë§¨í‹± ëª¨ë¸ ë¡œë“œ
            print("ğŸ“¥ ì‹œë§¨í‹± ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.semantic_model = SentenceTransformer('jhgan/ko-sbert-nli')
            print("âœ… ì‹œë§¨í‹± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            if progress_callback:
                progress_callback(0.3, "ë¯¸êµ­ ë°ì´í„° ë¡œë”© ì¤‘...")
            
            # ë¯¸êµ­ ë°ì´í„° ë¡œë“œ
            if self.us_tariff_file and os.path.exists(self.us_tariff_file):
                print("ğŸ“¥ ë¯¸êµ­ ê´€ì„¸ìœ¨í‘œ ë°ì´í„° ë¡œë”© ì¤‘...")
                if self.load_us_tariff_data():
                    print("âœ… ë¯¸êµ­ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
                else:
                    print("âŒ ë¯¸êµ­ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            else:
                print("âš ï¸ ë¯¸êµ­ ê´€ì„¸ìœ¨í‘œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            if progress_callback:
                progress_callback(0.7, "í•œêµ­ ë°ì´í„° ë¡œë”© ì¤‘...")
            
            # í•œêµ­ ë°ì´í„° ë¡œë“œ ì‹œë„
            if self.korea_recommender:
                print("ğŸ“¥ í•œêµ­ HSK ë°ì´í„° ë¡œë”© ì¤‘...")
                if self.load_korea_data_from_recommender():
                    print("âœ… í•œêµ­ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
                else:
                    print("âŒ í•œêµ­ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            else:
                print("âš ï¸ í•œêµ­ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì œê³µë˜ì§€ ì•ŠìŒ")
            
            if progress_callback:
                progress_callback(1.0, "ì´ˆê¸°í™” ì™„ë£Œ!")
            
            self.initialized = True
            llm_status = " (LLM ê°•í™” ëª¨ë“œ)" if self.llm_available else ""
            return True, f"âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!{llm_status}"
            
        except Exception as e:
            error_msg = f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            print(error_msg)
            return False, error_msg
    
    def load_us_tariff_data(self) -> bool:
        """ë¯¸êµ­ ê´€ì„¸ìœ¨í‘œ ë°ì´í„° ë¡œë“œ ë° HS 6ìë¦¬ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            self.us_data = pd.read_excel(self.us_tariff_file, sheet_name=0)
            print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(self.us_data)}ê°œ í–‰")
            
            # ì»¬ëŸ¼ëª… í‘œì¤€í™”
            column_mapping = {
                'ì„¸ë²ˆ': 'hs_code',
                'ì˜ë¬¸í’ˆëª…': 'english_name',
                'í•œê¸€í’ˆëª…': 'korean_name'
            }
            self.us_data = self.us_data.rename(columns=column_mapping)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if 'hs_code' not in self.us_data.columns:
                print("âŒ HS ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            if 'english_name' not in self.us_data.columns:
                print("âŒ ì˜ë¬¸ëª… ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ë°ì´í„° ì •ë¦¬
            initial_count = len(self.us_data)
            self.us_data = self.us_data.dropna(subset=['hs_code'])
            
            # HS ì½”ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì •ë¦¬
            self.us_data['hs_code'] = self.us_data['hs_code'].astype(str).str.strip()
            self.us_data['hs_code'] = self.us_data['hs_code'].str.replace('.0', '', regex=False)
            self.us_data = self.us_data[self.us_data['hs_code'] != '']
            
            # ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ì½”ë“œë§Œ í•„í„°ë§
            numeric_mask = self.us_data['hs_code'].str.match(r'^\d+$', na=False)
            self.us_data = self.us_data[numeric_mask]
            
            # ì˜ë¬¸ëª…ì´ ìˆëŠ” ê²ƒë§Œ í•„í„°ë§
            name_mask = self.us_data['english_name'].notna() & (self.us_data['english_name'].str.strip() != '')
            self.us_data = self.us_data[name_mask]
            
            print(f"ğŸ“Š ì •ë¦¬ í›„ ë°ì´í„°: {len(self.us_data)}ê°œ í–‰")
            
            if len(self.us_data) == 0:
                print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # HS ì½”ë“œë¥¼ 10ìë¦¬ë¡œ í‘œì¤€í™”
            self.us_data['hs_code'] = self.us_data['hs_code'].str.ljust(10, '0')
            
            # HS êµ¬ì¡° ì •ë³´ ì¶”ê°€
            self.us_data['hs6'] = self.us_data['hs_code'].str[:6]
            self.us_data['chapter'] = self.us_data['hs_code'].str[:2]
            self.us_data['heading'] = self.us_data['hs_code'].str[:4]
            
            # í•œê¸€ëª…ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
            if 'korean_name' not in self.us_data.columns:
                self.us_data['korean_name'] = ''
            else:
                self.us_data['korean_name'] = self.us_data['korean_name'].fillna('')
            
            # í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
            self.us_data['combined_text'] = (
                self.us_data['english_name'].fillna('') + ' ' +
                self.us_data['korean_name'].fillna('')
            ).str.strip()
            
            # HS 6ìë¦¬ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
            self._build_us_hs6_index()
            
            print(f"âœ… ë¯¸êµ­ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.us_data)}ê°œ í•­ëª©, HS 6ìë¦¬ {len(self.us_hs6_index)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë¯¸êµ­ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _build_us_hs6_index(self):
        """ë¯¸êµ­ ë°ì´í„°ì˜ HS 6ìë¦¬ë³„ ì¸ë±ìŠ¤ êµ¬ì¶• + HS6 ë¶„ë¥˜ ì„¤ëª… ì¶”ì¶œ"""
        self.us_hs6_index = {}
        self.hs6_descriptions = {}
        
        for hs6, group in self.us_data.groupby('hs6'):
            # ê¸°ë³¸ ì¸ë±ìŠ¤ ì •ë³´
            self.us_hs6_index[hs6] = {
                'count': len(group),
                'codes': group['hs_code'].tolist(),
                'names_en': group['english_name'].tolist(),
                'names_kr': group['korean_name'].tolist(),
                'combined_texts': group['combined_text'].tolist()
            }
            
            # HS6 ë¶„ë¥˜ ì„¤ëª… ì¶”ì¶œ (ëŒ€ë¶„ë¥˜ ë§¥ë½)
            hs6_description = self._extract_hs6_description(group)
            if hs6_description:
                self.hs6_descriptions[hs6] = hs6_description
    
    def _extract_hs6_description(self, hs6_group) -> str:
        """HS 6ìë¦¬ ë¶„ë¥˜ì˜ ëŒ€í‘œ ì„¤ëª… ì¶”ì¶œ"""
        # 1ìˆœìœ„: ê°€ì¥ ì¼ë°˜ì ì¸ ì„¤ëª… (ê¸°íƒ€ê°€ ì•„ë‹Œ ê²ƒ ì¤‘ì—ì„œ)
        non_other_names = []
        
        for _, row in hs6_group.iterrows():
            eng_name = str(row['english_name']).lower()
            kor_name = str(row['korean_name']).lower()
            
            # "ê¸°íƒ€", "other", "nesoi" ë“±ì´ ì•„ë‹Œ êµ¬ì²´ì ì¸ ì„¤ëª… ì°¾ê¸°
            if not any(word in eng_name for word in ['other', 'nesoi', 'not elsewhere']) and \
               not any(word in kor_name for word in ['ê¸°íƒ€', 'ê·¸ ë°–ì˜', 'ë”°ë¡œ ë¶„ë¥˜ë˜ì§€']):
                if len(eng_name) > 10:  # ì¶©ë¶„íˆ êµ¬ì²´ì ì¸ ì„¤ëª…
                    non_other_names.append({
                        'eng': row['english_name'],
                        'kor': row['korean_name'],
                        'length': len(eng_name)
                    })
        
        # ê°€ì¥ ì ì ˆí•œ ì„¤ëª… ì„ íƒ
        if non_other_names:
            # ê¸¸ì´ê°€ ì ë‹¹í•œ ê²ƒ ì¤‘ì—ì„œ ì„ íƒ (ë„ˆë¬´ ê¸¸ì§€ë„ ì§§ì§€ë„ ì•Šê²Œ)
            suitable_names = [n for n in non_other_names if 15 <= n['length'] <= 100]
            if suitable_names:
                best_name = suitable_names[0]
            else:
                best_name = non_other_names[0]
            
            # í•œê¸€ëª…ì´ ìˆìœ¼ë©´ í•œê¸€ëª… ìš°ì„ , ì—†ìœ¼ë©´ ì˜ë¬¸ëª…
            if best_name['kor'] and str(best_name['kor']).strip():
                return str(best_name['kor']).strip()
            else:
                return str(best_name['eng']).strip()
        
        # 2ìˆœìœ„: ì²« ë²ˆì§¸ í•­ëª©ì˜ ì´ë¦„ (ê¸°íƒ€ë¼ë„ ì‚¬ìš©)
        first_row = hs6_group.iloc[0]
        if first_row['korean_name'] and str(first_row['korean_name']).strip():
            return str(first_row['korean_name']).strip()
        else:
            return str(first_row['english_name']).strip()
    
    def load_korea_data_from_recommender(self) -> bool:
        """í•œêµ­ HSK ë°ì´í„°ë¥¼ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œ"""
        if not self.korea_recommender or not hasattr(self.korea_recommender, 'integrated_df'):
            return False
        
        try:
            self.korea_data = self.korea_recommender.integrated_df.copy()
            
            # HS êµ¬ì¡° ì •ë³´ ì¶”ê°€
            self.korea_data['hs6'] = self.korea_data['HS_KEY'].str[:6]
            self.korea_data['chapter'] = self.korea_data['HS_KEY'].str[:2]
            self.korea_data['heading'] = self.korea_data['HS_KEY'].str[:4]
            
            # í•œêµ­ HS 6ìë¦¬ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
            self._build_korea_hs6_index()
            
            print(f"âœ… í•œêµ­ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.korea_data)}ê°œ í•­ëª©, HS 6ìë¦¬ {len(self.korea_hs6_index)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âŒ í•œêµ­ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _build_korea_hs6_index(self):
        """í•œêµ­ ë°ì´í„°ì˜ HS 6ìë¦¬ë³„ ì¸ë±ìŠ¤ êµ¬ì¶• + HS6 ë¶„ë¥˜ ì„¤ëª… ë³´ì™„"""
        self.korea_hs6_index = {}
        
        for hs6, group in self.korea_data.groupby('hs6'):
            # ëŒ€í‘œ ì´ë¦„ ì¶”ì¶œ
            names_kr = []
            names_en = []
            combined_texts = []
            
            for _, row in group.iterrows():
                # í•œê¸€ëª… ìš°ì„ ìˆœìœ„
                name_kr = ''
                for col in ['í•œê¸€í’ˆëª©ëª…', 'ì„¸ë²ˆ10ë‹¨ìœ„í’ˆëª…', 'í‘œì¤€í’ˆëª…']:
                    if col in row and pd.notna(row[col]) and str(row[col]).strip():
                        name_kr = str(row[col]).strip()
                        break
                
                # ì˜ë¬¸ëª…
                name_en = ''
                for col in ['ì˜ë¬¸í’ˆëª©ëª…', 'í‘œì¤€í’ˆëª…ì˜ë¬¸']:
                    if col in row and pd.notna(row[col]) and str(row[col]).strip():
                        name_en = str(row[col]).strip()
                        break
                
                # í†µí•© í…ìŠ¤íŠ¸
                combined_text = ''
                if 'final_combined_text' in row and pd.notna(row['final_combined_text']):
                    combined_text = str(row['final_combined_text'])
                
                names_kr.append(name_kr)
                names_en.append(name_en)
                combined_texts.append(combined_text)
            
            self.korea_hs6_index[hs6] = {
                'count': len(group),
                'codes': group['HS_KEY'].tolist(),
                'names_kr': names_kr,
                'names_en': names_en,
                'combined_texts': combined_texts,
                'data_sources': group['data_source'].tolist() if 'data_source' in group.columns else []
            }
            
            # í•œêµ­ ë°ì´í„°ì—ì„œë„ HS6 ë¶„ë¥˜ ì„¤ëª… ì¶”ì¶œ (ë¯¸êµ­ ë°ì´í„°ì— ì—†ëŠ” ê²½ìš°)
            if hs6 not in self.hs6_descriptions:
                korea_hs6_desc = self._extract_korea_hs6_description(group)
                if korea_hs6_desc:
                    self.hs6_descriptions[hs6] = korea_hs6_desc
    
    def _extract_korea_hs6_description(self, hs6_group) -> str:
        """í•œêµ­ ë°ì´í„°ì—ì„œ HS 6ìë¦¬ ë¶„ë¥˜ ì„¤ëª… ì¶”ì¶œ"""
        non_other_names = []
        
        for _, row in hs6_group.iterrows():
            # í•œê¸€ëª… ìš°ì„  í™•ì¸
            for col in ['í•œê¸€í’ˆëª©ëª…', 'ì„¸ë²ˆ10ë‹¨ìœ„í’ˆëª…', 'í‘œì¤€í’ˆëª…']:
                if col in row and pd.notna(row[col]):
                    name = str(row[col]).strip()
                    if name and not any(word in name for word in ['ê¸°íƒ€', 'ê·¸ ë°–ì˜', 'ë”°ë¡œ ë¶„ë¥˜ë˜ì§€']):
                        if len(name) > 5:  # ì¶©ë¶„íˆ êµ¬ì²´ì ì¸ ì„¤ëª…
                            non_other_names.append(name)
                            break
        
        if non_other_names:
            # ê°€ì¥ ì ì ˆí•œ ì„¤ëª… ì„ íƒ (ê¸¸ì´ ê¸°ì¤€)
            suitable_names = [n for n in non_other_names if 10 <= len(n) <= 50]
            if suitable_names:
                return suitable_names[0]
            else:
                return non_other_names[0]
        
        # ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©
        first_row = hs6_group.iloc[0]
        for col in ['í•œê¸€í’ˆëª©ëª…', 'ì„¸ë²ˆ10ë‹¨ìœ„í’ˆëª…', 'í‘œì¤€í’ˆëª…']:
            if col in first_row and pd.notna(first_row[col]):
                name = str(first_row[col]).strip()
                if name:
                    return name
        
        return ""
    
    def get_hs6_description(self, hs6: str) -> str:
        """HS 6ìë¦¬ ë¶„ë¥˜ ì„¤ëª… ë°˜í™˜"""
        return self.hs6_descriptions.get(hs6, f"HS {hs6} ë¶„ë¥˜")
    
    def lookup_us_hs_code(self, us_hs_code: str) -> Optional[Dict]:
        """ë¯¸êµ­ HS ì½”ë“œ ì¡°íšŒ"""
        us_hs_code = str(us_hs_code).strip().ljust(10, '0')
        
        # ì •í™•í•œ 10ìë¦¬ ë§¤ì¹­
        matching_rows = self.us_data[self.us_data['hs_code'] == us_hs_code]
        
        if matching_rows.empty:
            return None
        
        row = matching_rows.iloc[0]
        hs_components = self.hs_analyzer.get_hs_components(us_hs_code)
        
        return {
            'hs_code': us_hs_code,
            'english_name': row['english_name'],
            'korean_name': row['korean_name'],
            'combined_text': row['combined_text'],
            'hs_components': hs_components
        }
    
    def get_korea_candidates_by_hs6(self, hs6: str) -> List[Dict]:
        """HS 6ìë¦¬ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ í›„ë³´êµ° ìƒì„±"""
        if hs6 not in self.korea_hs6_index:
            return []
        
        korea_group = self.korea_hs6_index[hs6]
        candidates = []
        
        for i in range(korea_group['count']):
            candidates.append({
                'hs_code': korea_group['codes'][i],
                'name_kr': korea_group['names_kr'][i],
                'name_en': korea_group['names_en'][i],
                'combined_text': korea_group['combined_texts'][i],
                'data_source': korea_group['data_sources'][i] if korea_group['data_sources'] else ''
            })
        
        return candidates
    
    def _build_enhanced_search_query(self, us_info: Dict, additional_name: str = "") -> str:
        """HS ì²´ê³„ë¥¼ ê³ ë ¤í•œ í–¥ìƒëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        query_parts = []
        
        # 1ìˆœìœ„: ë¯¸êµ­ ê´€ì„¸ìœ¨í‘œì˜ í•œê¸€ëª…
        if us_info['korean_name'] and us_info['korean_name'].strip():
            query_parts.append(us_info['korean_name'].strip())
        
        # 2ìˆœìœ„: ì¶”ê°€ ìƒí’ˆëª…
        if additional_name and additional_name.strip():
            query_parts.append(additional_name.strip())
        
        # 3ìˆœìœ„: ì˜ë¬¸ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        if us_info['english_name']:
            english_keywords = self._extract_core_keywords(us_info['english_name'])
            query_parts.extend(english_keywords)
        
        # 4ìˆœìœ„: HS ì²´ê³„ ê¸°ë°˜ ì¼ë°˜ì  ìš©ì–´
        hs_chapter = us_info['hs_components']['chapter']
        chapter_keywords = self._get_chapter_keywords(hs_chapter)
        query_parts.extend(chapter_keywords)
        
        return ' '.join(query_parts)
    
    def _extract_core_keywords(self, english_name: str) -> List[str]:
        """ì˜ë¬¸ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'of', 'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for',
            'with', 'by', 'from', 'other', 'others', 'nesoi', 'not', 'elsewhere',
            'specified', 'provided', 'including', 'excluding', 'containing',
            'having', 'being', 'used', 'suitable', 'designed', 'intended'
        }
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ
        words = re.findall(r'[A-Za-z]+', english_name.lower())
        keywords = []
        
        for word in words:
            if len(word) >= 3 and word not in stopwords:
                # ê¸°ìˆ ì  ìš©ì–´ë‚˜ ì¬ì§ˆëª… ìš°ì„ 
                if any(tech in word for tech in ['digital', 'electronic', 'automatic', 'portable', 'cellular']):
                    keywords.insert(0, word)
                else:
                    keywords.append(word)
        
        return keywords[:4]
    
    def _get_chapter_keywords(self, chapter: str) -> List[str]:
        """HS ì¥(Chapter)ë³„ ê´€ë ¨ í‚¤ì›Œë“œ ë°˜í™˜"""
        chapter_map = {
            '84': ['ê¸°ê³„', 'ì—”ì§„', 'ì¥ì¹˜', 'ì„¤ë¹„'],
            '85': ['ì „ê¸°', 'ì „ì', 'í†µì‹ ', 'ìŒí–¥'],
            '87': ['ìë™ì°¨', 'ì°¨ëŸ‰', 'ìš´ì†¡'],
            '73': ['ì² ê°•', 'ê¸ˆì†', 'ì œí’ˆ'],
            '39': ['í”Œë¼ìŠ¤í‹±', 'ìˆ˜ì§€', 'í•©ì„±'],
            '90': ['ê´‘í•™', 'ì˜ë£Œ', 'ì •ë°€', 'ê¸°ê¸°'],
            '94': ['ê°€êµ¬', 'ì¡°ëª…', 'ë¨í”„'],
            '95': ['ì™„êµ¬', 'ê²Œì„', 'ìŠ¤í¬ì¸ ']
        }
        
        return chapter_map.get(chapter, [])
    
    def _rank_candidates_by_similarity(self, search_query: str, candidates: List[Dict]) -> List[Dict]:
        """í›„ë³´êµ°ì„ ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¹€"""
        if not candidates or not self.semantic_model:
            for candidate in candidates:
                candidate['similarity_score'] = 0.5
            return candidates
        
        # í›„ë³´ë“¤ì˜ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        candidate_texts = []
        for candidate in candidates:
            text_parts = []
            if candidate['name_kr']:
                text_parts.append(candidate['name_kr'])
            if candidate['name_en']:
                text_parts.append(candidate['name_en'])
            if candidate['combined_text']:
                text_parts.append(candidate['combined_text'])
            
            combined_text = ' '.join(text_parts)
            candidate_texts.append(combined_text)
        
        # ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        try:
            query_embedding = self.semantic_model.encode([search_query])
            candidate_embeddings = self.semantic_model.encode(candidate_texts)
            similarities = cosine_similarity(query_embedding, candidate_embeddings).flatten()
                
                # í›„ë³´ì— ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ê°€
            for i, candidate in enumerate(candidates):
                candidate['similarity_score'] = float(similarities[i])  # NumPy floatë¥¼ Python floatë¡œ ë³€í™˜
                
                # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            ranked_candidates = sorted(candidates, key=lambda x: x['similarity_score'], reverse=True)
            return ranked_candidates
                
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì›ë³¸ ìˆœì„œ ìœ ì§€
            for candidate in candidates:
                candidate['similarity_score'] = 0.5
            return candidates   
    
    # ===========================================
    # LLM ê´€ë ¨ ìƒˆë¡œìš´ ë©”ì†Œë“œë“¤
    # ===========================================
    
    def analyze_product_with_llm(self, us_info: Dict, additional_name: str = "") -> Optional[ProductAnalysis]:
        """LLMì„ í™œìš©í•œ ìƒí’ˆ íŠ¹ì„± ë¶„ì„"""
        if not self.llm_available:
            return None
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{us_info['hs_code']}:{additional_name}"
        if cache_key in self.llm_analysis_cache:
            return self.llm_analysis_cache[cache_key]
        
        try:
            prompt = self._build_product_analysis_prompt(us_info, additional_name)
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ êµ­ì œë¬´ì—­ê³¼ HS ì½”ë“œ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒí’ˆì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            analysis = self._parse_llm_analysis(response.choices[0].message.content)
            
            # ìºì‹œ ì €ì¥
            if analysis:
                self.llm_analysis_cache[cache_key] = analysis
            return analysis
            
        except Exception as e:
            print(f"âš ï¸ LLM ìƒí’ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _build_product_analysis_prompt(self, us_info: Dict, additional_name: str) -> str:
        """ìƒí’ˆ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        hs_chapter = us_info['hs_components']['chapter']
        hs6 = us_info['hs_components']['hs6']
        
        return f"""
ë‹¤ìŒ ìƒí’ˆì˜ íŠ¹ì„±ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

**ê¸°ë³¸ ì •ë³´**:
- ë¯¸êµ­ HS ì½”ë“œ: {us_info['hs_code']} (ì¥: {hs_chapter}, HS6: {hs6})
- ì˜ë¬¸ëª…: {us_info['english_name']}
- í•œê¸€ëª…: {us_info.get('korean_name', 'ì—†ìŒ')}
- ì¶”ê°€ ìƒí’ˆëª…: {additional_name or 'ì—†ìŒ'}

**ë¶„ì„ ìš”ì²­**:
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

```json
{{
    "material": "ì£¼ìš” ì¬ì§ˆ (ì˜ˆ: í”Œë¼ìŠ¤í‹±, ì•Œë£¨ë¯¸ëŠ„, ì„¬ìœ , ë³µí•©ì¬ë£Œ ë“±)",
    "function": "ì£¼ìš” ê¸°ëŠ¥ (ì˜ˆ: í†µì‹ , ì¸¡ì •, ê°€ê³µ, ë³´ê´€, ì´ë™ ë“±)",
    "target_user": "ëŒ€ìƒ ì‚¬ìš©ì (ì˜ˆ: ì¼ë°˜ì†Œë¹„ì, ì „ë¬¸ê°€, ì‚°ì—…ìš©, ì˜ë£Œìš© ë“±)",
    "size_category": "í¬ê¸° ë¶„ë¥˜ (ì˜ˆ: íœ´ëŒ€ìš©, íƒìƒìš©, ëŒ€í˜•ì‚°ì—…ìš© ë“±)",
    "key_features": ["ì£¼ìš”íŠ¹ì§•1", "ì£¼ìš”íŠ¹ì§•2", "ì£¼ìš”íŠ¹ì§•3"],
    "alternative_hs6": ["{hs6}", "ëŒ€ì•ˆHS6ìë¦¬1", "ëŒ€ì•ˆHS6ìë¦¬2"],
    "reasoning": "ë¶„ì„ ê·¼ê±°ì™€ HS ë¶„ë¥˜ ë§¥ë½"
}}
```

**ì£¼ì˜ì‚¬í•­**:
- HS ì½”ë“œ ì²´ê³„ì˜ ë…¼ë¦¬ë¥¼ ê³ ë ¤í•´ì£¼ì„¸ìš”
- ëŒ€ì•ˆ HS6ëŠ” í˜„ì¬ {hs6}ì™€ ë‹¤ë¥´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ê°™ì€ ìƒí’ˆì¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë¶„ë¥˜ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”
- JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”
"""
    
    def _parse_llm_analysis(self, llm_response: str) -> Optional[ProductAnalysis]:
        """LLM ì‘ë‹µì„ ProductAnalysis ê°ì²´ë¡œ íŒŒì‹±"""
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ JSON ì°¾ê¸°
                json_text = llm_response
            
            # JSON íŒŒì‹±
            data = json.loads(json_text)
            
            return ProductAnalysis(
                material=data.get('material', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                function=data.get('function', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                target_user=data.get('target_user', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                size_category=data.get('size_category', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                key_features=data.get('key_features', []),
                alternative_hs6=data.get('alternative_hs6', []),
                reasoning=data.get('reasoning', '')
            )
            
        except Exception as e:
            print(f"âš ï¸ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def get_llm_suggested_candidates(self, product_analysis: ProductAnalysis) -> List[Dict]:
        """LLMì´ ì œì•ˆí•œ ëŒ€ì•ˆ HS ë¶„ë¥˜ì˜ í•œêµ­ í›„ë³´ë“¤"""
        if not product_analysis:
            return []
        
        candidates = []
        
        for alt_hs6 in product_analysis.alternative_hs6:
            if alt_hs6 in self.korea_hs6_index:
                alt_candidates = self.get_korea_candidates_by_hs6(alt_hs6)
                # ëŒ€ì•ˆ í›„ë³´ì„ì„ í‘œì‹œ
                for candidate in alt_candidates:
                    candidate['is_alternative'] = True
                    candidate['source_hs6'] = alt_hs6
                candidates.extend(alt_candidates)
        
        return candidates
    
    def llm_rank_candidates(self, product_analysis: ProductAnalysis, 
                           candidates: List[Dict]) -> List[LLMRanking]:
        """LLMì„ í™œìš©í•œ í›„ë³´ ìˆœìœ„ ë§¤ê¹€"""
        
        if not self.llm_available or not product_analysis or len(candidates) == 0:
            return []
        
        # ë„ˆë¬´ ë§ì€ í›„ë³´ëŠ” ìƒìœ„ 15ê°œë§Œ LLMì—ê²Œ ì „ë‹¬
        top_candidates = candidates[:15]
        
        try:
            prompt = self._build_ranking_prompt(product_analysis, top_candidates)
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ HS ì½”ë“œ ë§¤ì¹­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìƒí’ˆ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ í•œêµ­ HSK ì½”ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            
            return self._parse_llm_rankings(response.choices[0].message.content)
            
        except Exception as e:
            print(f"âš ï¸ LLM ìˆœìœ„ ë§¤ê¹€ ì‹¤íŒ¨: {e}")
            return []
    
    def _build_ranking_prompt(self, product_analysis: ProductAnalysis, 
                             candidates: List[Dict]) -> str:
        """í›„ë³´ ìˆœìœ„ ë§¤ê¹€ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        candidates_text = ""
        for i, candidate in enumerate(candidates, 1):
            alt_info = f" (ëŒ€ì•ˆë¶„ë¥˜: {candidate['source_hs6']})" if candidate.get('is_alternative') else ""
            candidates_text += f"{i}. {candidate['hs_code']}: {candidate['name_kr']}{alt_info}\n"
        
        return f"""
ë‹¤ìŒ ìƒí’ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ í•œêµ­ HSK ì½”ë“œ 3ê°œë¥¼ ìˆœìœ„ë³„ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”:

**ìƒí’ˆ íŠ¹ì„±**:
- ì¬ì§ˆ: {product_analysis.material}
- ê¸°ëŠ¥: {product_analysis.function}
- ëŒ€ìƒ ì‚¬ìš©ì: {product_analysis.target_user}
- í¬ê¸° ë¶„ë¥˜: {product_analysis.size_category}
- ì£¼ìš” íŠ¹ì§•: {', '.join(product_analysis.key_features)}

**ë¶„ì„ ê·¼ê±°**: {product_analysis.reasoning}

**í›„ë³´ ëª©ë¡**:
{candidates_text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

```json
{{
    "rankings": [
        {{
            "rank": 1,
            "hs_code": "ê°€ì¥ ì í•©í•œ HSK ì½”ë“œ",
            "confidence": 0.95,
            "reason": "ì´ ì½”ë“œë¥¼ 1ìˆœìœ„ë¡œ ì„ íƒí•œ êµ¬ì²´ì ì¸ ì´ìœ "
        }},
        {{
            "rank": 2,
            "hs_code": "ë‘ ë²ˆì§¸ë¡œ ì í•©í•œ HSK ì½”ë“œ",
            "confidence": 0.80,
            "reason": "ì´ ì½”ë“œë¥¼ 2ìˆœìœ„ë¡œ ì„ íƒí•œ êµ¬ì²´ì ì¸ ì´ìœ "
        }},
        {{
            "rank": 3,
            "hs_code": "ì„¸ ë²ˆì§¸ë¡œ ì í•©í•œ HSK ì½”ë“œ",
            "confidence": 0.65,
            "reason": "ì´ ì½”ë“œë¥¼ 3ìˆœìœ„ë¡œ ì„ íƒí•œ êµ¬ì²´ì ì¸ ì´ìœ "
        }}
    ]
}}
```

**ì„ íƒ ê¸°ì¤€**:
1. ìƒí’ˆì˜ ì¬ì§ˆê³¼ ê¸°ëŠ¥ì´ ê°€ì¥ ì˜ ë§ëŠ” ì½”ë“œ
2. ëŒ€ìƒ ì‚¬ìš©ìì™€ ìš©ë„ê°€ ì¼ì¹˜í•˜ëŠ” ì½”ë“œ
3. HS ì½”ë“œ ì²´ê³„ì˜ ë…¼ë¦¬ì— ë¶€í•©í•˜ëŠ” ì½”ë“œ
"""
    
    def _parse_llm_rankings(self, llm_response: str) -> List[LLMRanking]:
        """LLM ìˆœìœ„ ì‘ë‹µ íŒŒì‹±"""
        try:
            json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                json_text = llm_response
            
            data = json.loads(json_text)
            rankings = []
            
            for item in data.get('rankings', []):
                rankings.append(LLMRanking(
                    rank=item.get('rank', 0),
                    hs_code=item.get('hs_code', ''),
                    confidence=item.get('confidence', 0.5),
                    reason=item.get('reason', '')
                ))
            
            return rankings
            
        except Exception as e:
            print(f"âš ï¸ LLM ìˆœìœ„ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def generate_matching_explanation(self, us_info: Dict, korea_match: Dict, 
                                    product_analysis: Optional[ProductAnalysis] = None) -> str:
        """LLM ê¸°ë°˜ ë§¤ì¹­ ì„¤ëª… ìƒì„±"""
        
        if not self.llm_available:
            return self._generate_simple_explanation(us_info, korea_match)
        
        try:
            analysis_text = ""
            if product_analysis:
                analysis_text = f"""
**ìƒí’ˆ ë¶„ì„**:
- ì¬ì§ˆ: {product_analysis.material}
- ê¸°ëŠ¥: {product_analysis.function}
- ëŒ€ìƒ: {product_analysis.target_user}

**ë¶„ì„ ê·¼ê±°**: {product_analysis.reasoning}"""
            
            prompt = f"""
ë‹¤ìŒ HS ì½”ë“œ ë³€í™˜ ê²°ê³¼ì— ëŒ€í•´ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

**ë³€í™˜ ê²°ê³¼**:
- ë¯¸êµ­ HS: {us_info['hs_code']} - {us_info['english_name']}
- í•œêµ­ HSK: {korea_match['hs_code']} - {korea_match['name_kr']}
{analysis_text}

ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ì™œ ì´ í•œêµ­ ì½”ë“œê°€ ì í•©í•œì§€
2. ë‘ ì½”ë“œì˜ ê³µí†µì 
3. ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ë¬´ì—­ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ë³µì¡í•œ HS ì½”ë“œ ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âš ï¸ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            return self._generate_simple_explanation(us_info, korea_match)
    
    def _generate_simple_explanation(self, us_info: Dict, korea_match: Dict) -> str:
        """ê°„ë‹¨í•œ ì„¤ëª… ìƒì„± (LLM ì—†ì„ ë•Œ)"""
        us_hs6 = us_info['hs_components']['hs6']
        korea_hs6 = korea_match['hs_code'][:6]
        
        if us_hs6 == korea_hs6:
            return f"ë‘ ì½”ë“œ ëª¨ë‘ HS {us_hs6} ë¶„ë¥˜ì— ì†í•˜ë¯€ë¡œ êµ­ì œì ìœ¼ë¡œ ë™ì¼í•œ ìƒí’ˆêµ°ìœ¼ë¡œ ì¸ì •ë©ë‹ˆë‹¤. í•œêµ­ì—ì„œëŠ” '{korea_match['name_kr']}'ë¡œ ë¶„ë¥˜ë˜ë©°, ì„¸ë¶€ ê·œì •ì€ ê´€ì„¸ì²­ì— í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        else:
            return f"ë¯¸êµ­ ì½”ë“œëŠ” HS {us_hs6} ë¶„ë¥˜ì´ì§€ë§Œ, í•œêµ­ì—ì„œëŠ” HS {korea_hs6} ë¶„ë¥˜({korea_match['name_kr']})ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤. ë‘ êµ­ê°€ ê°„ ì„¸ë¶€ ë¶„ë¥˜ ì²´ê³„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê´€ì„¸ì²­ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    
    # ===========================================
    # ë©”ì¸ ë³€í™˜ ë¡œì§ (ê¸°ì¡´ ë°©ì‹ + LLM ê°•í™”)
    # ===========================================
    
    def convert_hs_code(self, us_hs_code: str, us_product_name: str = "") -> Dict:
        """HS ì½”ë“œ ë³€í™˜ ì‹¤í–‰ (LLM ê°•í™” ë²„ì „)"""
        if not self.initialized:
            return {
                'status': 'error',
                'message': 'ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
            }
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{us_hs_code}:{us_product_name}"
        if cache_key in self.conversion_cache:
            cached_result = self.conversion_cache[cache_key].copy()
            cached_result['from_cache'] = True
            return cached_result
        
        # 1ë‹¨ê³„: ë¯¸êµ­ HS ì½”ë“œ ì¡°íšŒ
        us_info = self.lookup_us_hs_code(us_hs_code)
        if not us_info:
            result = {
                'status': 'error',
                'message': f"ë¯¸êµ­ HSì½”ë“œ '{us_hs_code}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'us_hs_code': us_hs_code,
                'us_product_name': us_product_name,
                'suggestions': self._get_alternative_suggestions(us_hs_code)
            }
            return result
        
        # 2ë‹¨ê³„: LLM ê¸°ë°˜ ìƒí’ˆ íŠ¹ì„± ë¶„ì„ (ì„ íƒì )
        product_analysis = None
        if self.llm_available:
            product_analysis = self.analyze_product_with_llm(us_info, us_product_name)
        
        # 3ë‹¨ê³„: í›„ë³´êµ° ìƒì„±
        hs6 = us_info['hs_components']['hs6']
        primary_candidates = self.get_korea_candidates_by_hs6(hs6)
        
        # LLM ì œì•ˆ ëŒ€ì•ˆ í›„ë³´êµ°
        alternative_candidates = []
        if self.llm_available and product_analysis:
            alternative_candidates = self.get_llm_suggested_candidates(product_analysis)
        
        all_candidates = primary_candidates + alternative_candidates
        
        if not all_candidates:
            result = {
                'status': 'no_hs6_match',
                'message': f"HS 6ìë¦¬ '{hs6}' ë° ëŒ€ì•ˆ ë¶„ë¥˜ì—ì„œ í•œêµ­ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'us_hs_code': us_hs_code,
                'us_info': us_info,
                'product_analysis': self._analysis_to_dict(product_analysis) if product_analysis else None,
                'hs6': hs6
            }
            return result
        
        # 4ë‹¨ê³„: ì‹œë§¨í‹± ìœ ì‚¬ë„ ê³„ì‚°
        search_query = self._build_enhanced_search_query(us_info, us_product_name)
        semantic_ranked = self._rank_candidates_by_similarity(search_query, all_candidates.copy())
        
        # 5ë‹¨ê³„: LLM ê¸°ë°˜ ì‹¬ì¸µ ë§¤ì¹­ (ì„ íƒì )
        llm_rankings = []
        if self.llm_available and product_analysis and len(all_candidates) > 0:
            llm_rankings = self.llm_rank_candidates(product_analysis, semantic_ranked)
        
        # 6ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        final_candidates = self._calculate_hybrid_scores(
            semantic_ranked, llm_rankings, us_info['hs_code']
        )
        
        if not final_candidates:
            result = {
                'status': 'no_match',
                'message': 'ì í•©í•œ í•œêµ­ HSK ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'us_hs_code': us_hs_code,
                'us_info': us_info,
                'product_analysis': self._analysis_to_dict(product_analysis) if product_analysis else None
            }
            return result
        
        # ìµœì¢… ì¶”ì²œ ê²°ê³¼
        best_match = final_candidates[0]
        
        # 7ë‹¨ê³„: LLM ê¸°ë°˜ ì„¤ëª… ìƒì„± (ì„ íƒì )
        explanation = ""
        if self.llm_available:
            explanation = self.generate_matching_explanation(us_info, best_match, product_analysis)
        
        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        result = {
            'status': 'success',
            'method': 'llm_enhanced' if self.llm_available else 'traditional',
            'us_hs_code': us_hs_code,
            'us_product_name': us_product_name,
            'us_info': us_info,
            'korea_recommendation': {
                'hs_code': best_match['hs_code'],
                'name_kr': best_match['name_kr'],
                'name_en': best_match.get('name_en', ''),
                'data_source': best_match.get('data_source', ''),
                'confidence': float(best_match.get('final_score', 0.5)),  # floatë¡œ ë³€í™˜
                'is_alternative_classification': best_match.get('is_alternative', False),
                'source_hs6': best_match.get('source_hs6', hs6)
            },
            'hs_analysis': {
                'hs6_match': True,
                'hs_similarity': float(best_match.get('score_breakdown', {}).get('hs_structure', 0.5)),  # floatë¡œ ë³€í™˜
                'semantic_similarity': float(best_match.get('score_breakdown', {}).get('semantic', 0.5)),  # floatë¡œ ë³€í™˜
                'total_candidates': len(all_candidates),
                'us_hs6': hs6,
                'korea_hs6': best_match['hs_code'][:6],
                'llm_enhanced': self.llm_available
            },
            'search_query': search_query,
            'all_candidates': final_candidates[:3],
            'explanation': explanation,
            'product_analysis': self._analysis_to_dict(product_analysis) if product_analysis else None
        }
        
        # NumPy íƒ€ì… ë³€í™˜ ì ìš©
        result = convert_numpy_types(result)
        
        # ìºì‹œ ì €ì¥
        self.conversion_cache[cache_key] = result
        return result
    
    def _calculate_hybrid_scores(self, semantic_candidates: List[Dict], 
                                llm_rankings: List[LLMRanking], us_hs_code: str) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ì‹œë§¨í‹± + LLM + êµ¬ì¡°ì  ìœ ì‚¬ë„)"""
        
        # LLM ìˆœìœ„ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        llm_scores = {}
        for ranking in llm_rankings:
            llm_scores[ranking.hs_code] = {
                'confidence': ranking.confidence,
                'rank': ranking.rank,
                'reason': ranking.reason
            }
        
        # ê° í›„ë³´ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        for candidate in semantic_candidates:
            korea_code = candidate['hs_code']
            
            # 1. ì‹œë§¨í‹± ìœ ì‚¬ë„ (ê¸°ì¡´)
            semantic_score = candidate.get('similarity_score', 0.5)
            
            # 2. HS êµ¬ì¡°ì  ìœ ì‚¬ë„ (ê¸°ì¡´)
            hs_similarity = self.hs_analyzer.calculate_hs_similarity(us_hs_code, korea_code)
            
             # 3. LLM ì‹ ë¢°ë„
            llm_score = 0.5  # ê¸°ë³¸ê°’
            llm_reason = ""
            if korea_code in llm_scores:
                llm_info = llm_scores[korea_code]
                llm_score = float(llm_info['confidence'])  # floatë¡œ ë³€í™˜
                llm_reason = llm_info['reason']
                # ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ì¶”ê°€ ë³´ë„ˆìŠ¤
                rank_bonus = max(0, (4 - llm_info['rank']) * 0.1)
                llm_score = min(1.0, llm_score + rank_bonus)
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            if self.llm_available and llm_rankings:
                # LLM ì‚¬ìš© ê°€ëŠ¥ì‹œ: ì‹œë§¨í‹± 25% + LLM 55% + êµ¬ì¡°ì  20%
                final_score = (semantic_score * 0.25) + (llm_score * 0.55) + (hs_similarity * 0.20)
            else:
                # LLM ì—†ì„ ì‹œ: ì‹œë§¨í‹± 50% + êµ¬ì¡°ì  50% (ê¸°ì¡´ê³¼ ë™ì¼)
                final_score = (semantic_score * 0.5) + (hs_similarity * 0.5)
            
            # ëŒ€ì•ˆ ë¶„ë¥˜ í˜ë„í‹° (ê¸°ë³¸ HS6ê°€ ì•„ë‹Œ ê²½ìš° ì•½ê°„ ê°ì )
            if candidate.get('is_alternative', False):
                final_score *= 0.9
            
            candidate['final_score'] = float(final_score)  # floatë¡œ ë³€í™˜
            candidate['score_breakdown'] = {
                'semantic': float(semantic_score),  # floatë¡œ ë³€í™˜
                'hs_structure': float(hs_similarity),  # floatë¡œ ë³€í™˜
                'llm_confidence': float(llm_score),  # floatë¡œ ë³€í™˜
                'llm_reason': llm_reason
            }
        
        # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        return sorted(semantic_candidates, key=lambda x: x['final_score'], reverse=True)

    
    def _analysis_to_dict(self, analysis: Optional[ProductAnalysis]) -> Optional[Dict]:
        """ProductAnalysisë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not analysis:
            return None
        
        return {
            'material': analysis.material,
            'function': analysis.function,
            'target_user': analysis.target_user,
            'size_category': analysis.size_category,
            'key_features': analysis.key_features,
            'alternative_hs6': analysis.alternative_hs6,
            'reasoning': analysis.reasoning
        }
    
    def _get_alternative_suggestions(self, us_hs_code: str) -> List[str]:
        """ìœ ì‚¬í•œ HS ì½”ë“œ ëŒ€ì•ˆ ì œì‹œ"""
        if self.us_data is None:
            return []
        
        suggestions = []
        
        # 1. ì• 6ìë¦¬ ê¸°ì¤€ ìœ ì‚¬ ì½”ë“œ
        if len(us_hs_code) >= 6:
            hs6 = us_hs_code[:6]
            similar_6 = self.us_data[self.us_data['hs_code'].str.startswith(hs6)]
            suggestions.extend(similar_6['hs_code'].head(3).tolist())
        
        # 2. ì• 4ìë¦¬ ê¸°ì¤€ ìœ ì‚¬ ì½”ë“œ
        if len(us_hs_code) >= 4 and len(suggestions) < 3:
            hs4 = us_hs_code[:4]
            similar_4 = self.us_data[self.us_data['hs_code'].str.startswith(hs4)]
            suggestions.extend(similar_4['hs_code'].head(5-len(suggestions)).tolist())
        
        # 3. ì¥(Chapter) ê¸°ì¤€ ìœ ì‚¬ ì½”ë“œ
        if len(us_hs_code) >= 2 and len(suggestions) < 3:
            chapter = us_hs_code[:2]
            similar_chapter = self.us_data[self.us_data['chapter'] == chapter]
            suggestions.extend(similar_chapter['hs_code'].head(5-len(suggestions)).tolist())
        
        return list(set(suggestions))  # ì¤‘ë³µ ì œê±°
    
    def get_system_statistics(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜ (LLM ì •ë³´ í¬í•¨)"""
        stats = {
            'system_status': {
                'initialized': self.initialized,
                'us_data_loaded': self.us_data is not None,
                'korea_data_loaded': self.korea_data is not None,
                'semantic_model_loaded': self.semantic_model is not None,
                'openai_available': self.openai_client is not None,
                'llm_available': self.llm_available,
                'conversion_cache_size': len(self.conversion_cache),
                'llm_analysis_cache_size': len(self.llm_analysis_cache),
                'llm_ranking_cache_size': len(self.llm_ranking_cache)
            }
        }

        if self.us_data is not None:
            stats['us_data'] = {
                'total_codes': len(self.us_data),
                'unique_hs6': len(self.us_hs6_index),
                'unique_chapters': len(self.us_data['chapter'].unique()),
                'has_korean_names': (~self.us_data['korean_name'].isna()).sum(),
                'hs6_descriptions': len(self.hs6_descriptions)
            }

        if self.korea_data is not None:
            stats['korea_data'] = {
                'total_codes': len(self.korea_data),
                'unique_hs6': len(self.korea_hs6_index),
                'unique_chapters': len(self.korea_data['chapter'].unique())
            }

        # HS 6ìë¦¬ êµì§‘í•© ë¶„ì„
        if self.us_hs6_index and self.korea_hs6_index:
            us_hs6_set = set(self.us_hs6_index.keys())
            korea_hs6_set = set(self.korea_hs6_index.keys())
            stats['hs6_analysis'] = {
                'us_only_hs6': len(us_hs6_set - korea_hs6_set),
                'korea_only_hs6': len(korea_hs6_set - us_hs6_set),
                'common_hs6': len(us_hs6_set & korea_hs6_set),
                'coverage_rate': len(us_hs6_set & korea_hs6_set) / len(us_hs6_set) * 100 if us_hs6_set else 0
            }

        return stats

    def clear_cache(self):
        """ë³€í™˜ ìºì‹œ ì´ˆê¸°í™”"""
        cache_size = len(self.conversion_cache)
        llm_cache_size = len(self.llm_analysis_cache) + len(self.llm_ranking_cache)
        self.conversion_cache.clear()
        self.llm_analysis_cache.clear()
        self.llm_ranking_cache.clear()
        return cache_size + llm_cache_size


def main():
    """ëŒ€í™”í˜• HS ì½”ë“œ ë³€í™˜ ì‹œìŠ¤í…œ (LLM ê°•í™”)"""
    print("="*80)
    print("ğŸš€ HS Code Converter - ë¯¸êµ­â†’í•œêµ­ HSì½”ë“œ ë³€í™˜ ì‹œìŠ¤í…œ (LLM ê°•í™”)")
    print("="*80)
    
    # .env íŒŒì¼ ë¡œë“œ ë° OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
    from pathlib import Path
    project_root = Path(__file__).parent.parent
    env_file = project_root / ".env"
    if env_file.exists():
        load_dotenv(env_file)
        print(f"í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ë¡œë“œ: {env_file}")
    else:
        print(f"í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì´ ì—†ìŒ: {env_file}")
    
    openai_api_key = os.getenv("OPENAI_API_KEY")
    
    if not openai_api_key:
        print("ğŸ“Š ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ).")
    else:
        print("ğŸ¤– LLM ê°•í™” ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ).")
    
    # ë³€í™˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    us_tariff_file = project_root / "us_tariff_table_20250714.xlsx"
    
    # í•œêµ­ ì¶”ì²œ ì‹œìŠ¤í…œ ë¡œë“œ ì‹œë„
    korea_recommender = None
    try:
        from hs_recommender import HSCodeRecommender
        
        cache_dir = project_root /"cache"/ "hs_code_cache"
        korea_recommender = HSCodeRecommender(cache_dir=str(cache_dir))
        if korea_recommender.load_data():
            print("âœ… í•œêµ­ ì¶”ì²œ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        else:
            print("âš ï¸ í•œêµ­ ì¶”ì²œ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨")
            korea_recommender = None
    except ImportError:
        print("âš ï¸ í•œêµ­ ì¶”ì²œ ì‹œìŠ¤í…œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        korea_recommender = None
    
    # LLM ê°•í™” ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = HSCodeConverterService(
        str(us_tariff_file), 
        korea_recommender, 
        openai_api_key if openai_api_key else None
    )
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\nğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    success, message = converter.initialize_system()
    print(f"ğŸš€ {message}")
    
    if not success:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
    print_system_status(converter)
    
    # ëŒ€í™”í˜• ë³€í™˜ ì‹œì‘
    print("\n" + "="*80)
    print("ğŸ¯ ëŒ€í™”í˜• HS ì½”ë“œ ë³€í™˜ ì‹œì‘")
    print("="*80)
    print("ğŸ’¡ ì‚¬ìš©ë²•:")
    print("- ë¯¸êµ­ HS ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (4-10ìë¦¬ ìˆ«ì)")
    print("- ìƒí’ˆëª…ì€ ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤ (LLM ì‚¬ìš©ì‹œ ë” ì •í™•í•œ ë¶„ì„ ê°€ëŠ¥)")
    print("- 'quit', 'exit', 'q' ì…ë ¥ì‹œ ì¢…ë£Œ")
    print("- 'status' ì…ë ¥ì‹œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
    print("- 'cache' ì…ë ¥ì‹œ ìºì‹œ ì •ë³´ í™•ì¸")
    print("="*80)
    
    while True:
        try:
            print("\n" + "-"*50)
            
            # HS ì½”ë“œ ì…ë ¥
            us_hs_code = input("ğŸ”¢ ë¯¸êµ­ HS ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            # ì¢…ë£Œ ëª…ë ¹ì–´ í™•ì¸
            if us_hs_code.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # íŠ¹ë³„ ëª…ë ¹ì–´ ì²˜ë¦¬
            if us_hs_code.lower() == 'status':
                print_system_status(converter)
                continue
            
            if us_hs_code.lower() == 'cache':
                print_cache_info(converter)
                continue
            
            # HS ì½”ë“œ ìœ íš¨ì„± ê²€ì‚¬
            if not us_hs_code:
                print("âŒ HS ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            if not re.match(r'^\d{4,10}$', us_hs_code):
                print("âŒ ì˜¬ë°”ë¥¸ HS ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (4-10ìë¦¬ ìˆ«ì)")
                print("ğŸ’¡ ì˜ˆì‹œ: 8471300000, 9507109000")
                continue
            
            # ìƒí’ˆëª… ì…ë ¥ (ì„ íƒì‚¬í•­)
            llm_hint = " (LLM ë¶„ì„ì— ë„ì›€ë¨)" if converter.llm_available else ""
            product_name = input(f"ğŸ“¦ ìƒí’ˆëª… (ì„ íƒì‚¬í•­{llm_hint}): ").strip()
            
            print(f"\në³€í™˜ ì¤‘... [{us_hs_code}" + (f" - {product_name}" if product_name else "") + "]")
            print("-"*50)
            
            # ë³€í™˜ ì‹¤í–‰
            result = converter.convert_hs_code(us_hs_code, product_name)
            
            # ê²°ê³¼ ì¶œë ¥
            if result['status'] == 'success':
                print_enhanced_success_result(result, converter)
            elif result['status'] == 'error':
                print_error_result(result)
            elif result['status'] == 'no_hs6_match':
                print_no_match_result(result, converter)
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {result.get('message', 'ì˜¤ë¥˜ ì •ë³´ ì—†ìŒ')}")
            
            # ê³„ì†í• ì§€ ë¬»ê¸°
            print("\n" + "-"*50)
            continue_choice = input("ë‹¤ë¥¸ ì½”ë“œë¥¼ ë³€í™˜í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Enter: ê³„ì†, q: ì¢…ë£Œ): ").strip()
            if continue_choice.lower() in ['q', 'quit', 'exit']:
                print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            continue

def print_system_status(converter):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥ (LLM ì •ë³´ í¬í•¨)"""
    print("\n" + "="*50)
    print("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
    print("="*50)
    
    stats = converter.get_system_statistics()
    
    print("âœ… **ê¸°ë³¸ ìƒíƒœ**")
    print(f"- ì´ˆê¸°í™”: {'âœ… ì™„ë£Œ' if converter.initialized else 'âŒ ë¯¸ì™„ë£Œ'}")
    print(f"- ë¯¸êµ­ ë°ì´í„°: {'âœ… ë¡œë“œë¨' if converter.us_data is not None else 'âŒ ì—†ìŒ'}")
    print(f"- í•œêµ­ ë°ì´í„°: {'âœ… ë¡œë“œë¨' if converter.korea_data is not None else 'âŒ ì—†ìŒ'}")
    print(f"- ì‹œë§¨í‹± ëª¨ë¸: {'âœ… ë¡œë“œë¨' if converter.semantic_model is not None else 'âŒ ì—†ìŒ'}")
    print(f"- LLM ê¸°ëŠ¥: {'ğŸ¤– í™œì„±í™”' if converter.llm_available else 'âŒ ë¹„í™œì„±í™”'}")
    print(f"- ë™ì‘ ëª¨ë“œ: {'ğŸš€ LLM ê°•í™”' if converter.llm_available else 'ğŸ“Š ê¸°ë³¸ ëª¨ë“œ'}")
    
    if converter.us_data is not None:
        print(f"\nğŸ“Š **ë¯¸êµ­ ë°ì´í„°**")
        print(f"- ì´ ì½”ë“œ ìˆ˜: {len(converter.us_data):,}ê°œ")
        print(f"- HS 6ìë¦¬ ì¢…ë¥˜: {len(converter.us_hs6_index):,}ê°œ")
        print(f"- ì¥(Chapter) ì¢…ë¥˜: {len(converter.us_data['chapter'].unique())}ê°œ")
    
    if converter.korea_data is not None:
        print(f"\nğŸ“Š **í•œêµ­ ë°ì´í„°**")
        print(f"- ì´ ì½”ë“œ ìˆ˜: {len(converter.korea_data):,}ê°œ")
        print(f"- HS 6ìë¦¬ ì¢…ë¥˜: {len(converter.korea_hs6_index):,}ê°œ")
    
    if converter.llm_available:
        print(f"\nğŸ¤– **LLM ìºì‹œ**")
        print(f"- ìƒí’ˆ ë¶„ì„ ìºì‹œ: {len(converter.llm_analysis_cache)}ê°œ")
        print(f"- ìˆœìœ„ ë¶„ì„ ìºì‹œ: {len(converter.llm_ranking_cache)}ê°œ")

def print_cache_info(converter):
    """ìºì‹œ ì •ë³´ ì¶œë ¥ (LLM ìºì‹œ í¬í•¨)"""
    print("\n" + "="*50)
    print("ğŸ’¾ ìºì‹œ ì •ë³´")
    print("="*50)
    
    cache_size = len(converter.conversion_cache)
    llm_analysis_size = len(converter.llm_analysis_cache)
    llm_ranking_size = len(converter.llm_ranking_cache)
    
    print(f"- ë³€í™˜ ìºì‹œ: {cache_size}ê°œ í•­ëª©")
    if converter.llm_available:
        print(f"- LLM ë¶„ì„ ìºì‹œ: {llm_analysis_size}ê°œ í•­ëª©")
        print(f"- LLM ìˆœìœ„ ìºì‹œ: {llm_ranking_size}ê°œ í•­ëª©")
    
    if cache_size > 0:
        print(f"\nğŸ“‹ **ìµœê·¼ ë³€í™˜ ë‚´ì—­** (ìµœëŒ€ 5ê°œ)")
        for i, (cache_key, result) in enumerate(list(converter.conversion_cache.items())[-5:], 1):
            us_code, product_name = cache_key.split(':', 1)
            status = result.get('status', 'unknown')
            method = result.get('method', 'unknown')
            print(f"{i}. {us_code}" + (f" ({product_name})" if product_name else "") + f" - {status} ({method})")
        
        clear_choice = input("\nğŸ—‘ï¸ ëª¨ë“  ìºì‹œë¥¼ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip()
        if clear_choice.lower() in ['y', 'yes']:
            cleared_count = converter.clear_cache()
            print(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ ({cleared_count}ê°œ í•­ëª© ì‚­ì œ)")

def is_other_item(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ê¸°íƒ€ í•­ëª© ì—¬ë¶€ í™•ì¸"""
    if not text:
        return False
    
    text_lower = str(text).lower().strip()
    
    other_keywords = [
        'ê¸°íƒ€', 'ê·¸ ë°–ì˜', 'ë”°ë¡œ ë¶„ë¥˜ë˜ì§€', 'ê¸°íƒ€ì˜',
        'other', 'others', 'nesoi', 'not elsewhere', 'not specified',
        'not elsewhere specified or included',
        'other, not elsewhere specified'
    ]
    
    return any(keyword in text_lower for keyword in other_keywords)

def enhance_product_name_with_context(korea_name, hs6_description):
    """ê¸°íƒ€ ìƒí’ˆëª…ì„ ë§¥ë½ ì •ë³´ì™€ í•¨ê»˜ í‘œì‹œ"""
    if not korea_name or not hs6_description:
        return korea_name or ""
    
    if is_other_item(korea_name):
        return f"{hs6_description} ë¶„ì•¼ì˜ ê¸°íƒ€ í•­ëª©"
    else:
        return korea_name

def get_display_width(text):
    """í…ìŠ¤íŠ¸ì˜ ì‹¤ì œ í‘œì‹œ í­ ê³„ì‚° (í•œê¸€ 2, ì˜ë¬¸/ìˆ«ì 1)"""
    if not text:
        return 0
    
    width = 0
    for char in str(text):
        # í•œê¸€, í•œì, ì¼ë³¸ì–´ ë“±ì€ í­ì´ 2
        if ord(char) > 127:
            width += 2
        else:
            width += 1
    return width

def truncate_text_to_width(text, max_width):
    """ì§€ì •ëœ í­ì— ë§ê²Œ í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
    if not text:
        return ""
    
    text = str(text)
    current_width = 0
    result = ""
    
    for char in text:
        char_width = 2 if ord(char) > 127 else 1
        if current_width + char_width > max_width:
            if max_width >= 3:  # "..." ì¶”ê°€í•  ê³µê°„ì´ ìˆìœ¼ë©´
                # ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ "..." ê³µê°„ í™•ë³´
                while get_display_width(result + "...") > max_width and result:
                    result = result[:-1]
                result += "..."
            break
        result += char
        current_width += char_width
    
    return result

def pad_text_to_width(text, target_width):
    """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í­ì— ë§ê²Œ íŒ¨ë”©"""
    if not text:
        text = ""
    
    current_width = get_display_width(text)
    padding_needed = target_width - current_width
    
    if padding_needed > 0:
        return text + " " * padding_needed
    else:
        return text

def print_candidates_table(candidates, hs6_description):
    """í›„ë³´ ëª©ë¡ì„ ì •ë ¬ëœ í…Œì´ë¸”ë¡œ ì¶œë ¥"""
    
    # í…Œì´ë¸” í­ ì„¤ì •
    rank_width = 4
    code_width = 13
    name_width = 36  # ìƒí’ˆëª… í­
    similarity_width = 8
    
    # í—¤ë” ì¶œë ¥
    print("â”Œ" + "â”€" * rank_width + "â”¬" + "â”€" * code_width + "â”¬" + "â”€" * name_width + "â”¬" + "â”€" * similarity_width + "â”")
    
    # í—¤ë” í…ìŠ¤íŠ¸
    rank_header = pad_text_to_width("ìˆœìœ„", rank_width)
    code_header = pad_text_to_width("HSK ì½”ë“œ", code_width)
    name_header = pad_text_to_width("ìƒí’ˆëª…", name_width)
    similarity_header = pad_text_to_width("ìœ ì‚¬ë„", similarity_width)
    
    print(f"â”‚{rank_header}â”‚{code_header}â”‚{name_header}â”‚{similarity_header}â”‚")
    print("â”œ" + "â”€" * rank_width + "â”¼" + "â”€" * code_width + "â”¼" + "â”€" * name_width + "â”¼" + "â”€" * similarity_width + "â”¤")
    
    # ë°ì´í„° í–‰ ì¶œë ¥
    for i, candidate in enumerate(candidates, 1):
        final_score = candidate.get('final_score', 0.0)
        
        # í›„ë³´ ìƒí’ˆëª…ë„ ë§¥ë½ ì •ë³´ì™€ í•¨ê»˜ í‘œì‹œ
        candidate_name = enhance_product_name_with_context(
            candidate['name_kr'], 
            hs6_description
        )
        
        # ê° ì»¬ëŸ¼ ë°ì´í„° í¬ë§·íŒ…
        rank_text = pad_text_to_width(str(i), rank_width)
        code_text = pad_text_to_width(candidate['hs_code'], code_width)
        
        # ìƒí’ˆëª…ì€ ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³  íŒ¨ë”©
        name_truncated = truncate_text_to_width(candidate_name, name_width)
        name_text = pad_text_to_width(name_truncated, name_width)
        
        similarity_text = pad_text_to_width(f"{final_score:.1%}", similarity_width)
        
        print(f"â”‚{rank_text}â”‚{code_text}â”‚{name_text}â”‚{similarity_text}â”‚")
    
    # í•˜ë‹¨ ê²½ê³„
    print("â””" + "â”€" * rank_width + "â”´" + "â”€" * code_width + "â”´" + "â”€" * name_width + "â”´" + "â”€" * similarity_width + "â”˜")

def print_enhanced_success_result(result, converter):
    """ì„±ê³µ ê²°ê³¼ ì¶œë ¥ (LLM ê°•í™” ë²„ì „)"""
    us_info = result['us_info']
    korea_rec = result['korea_recommendation']
    hs_analysis = result['hs_analysis']
    
    # HS6 ë¶„ë¥˜ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
    hs6 = us_info['hs_components']['hs6']
    hs6_description = converter.get_hs6_description(hs6)
    
    # í•œêµ­ ìƒí’ˆëª…ì— ë§¥ë½ ì •ë³´ ì¶”ê°€ (ê¸°íƒ€ í•­ëª©ì¸ ê²½ìš°)
    enhanced_korea_name = enhance_product_name_with_context(
        korea_rec['name_kr'], 
        hs6_description
    )
    
    # ê²°ê³¼ í—¤ë”
    method_icon = "ğŸ¤–" if result.get('method') == 'llm_enhanced' else "ğŸ“Š"
    method_text = "LLM ê°•í™” ë³€í™˜ ì„±ê³µ!" if result.get('method') == 'llm_enhanced' else "ê¸°ë³¸ ë³€í™˜ ì„±ê³µ!"
    print(f"{method_icon} **{method_text}**\n")
    
    # ìºì‹œ ì •ë³´
    if result.get('from_cache'):
        print("ğŸ’¾ (ìºì‹œì—ì„œ ë¡œë“œë¨)\n")
    
    print("ğŸ“‹ **ë¯¸êµ­ HS ì½”ë“œ ì •ë³´**")
    print(f"- ì½”ë“œ: {result['us_hs_code']}")
    print(f"- ì˜ë¬¸ëª…: {us_info['english_name']}")
    print(f"- í•œê¸€ëª…: {us_info.get('korean_name', 'ì—†ìŒ')}")
    
    # LLM ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    if result.get('product_analysis'):
        analysis = result['product_analysis']
        print(f"\nğŸ§  **LLM ìƒí’ˆ ë¶„ì„**")
        print(f"- ì¬ì§ˆ: {analysis['material']}")
        print(f"- ê¸°ëŠ¥: {analysis['function']}")
        print(f"- ëŒ€ìƒ: {analysis['target_user']}")
        print(f"- í¬ê¸°: {analysis['size_category']}")
        if analysis['key_features']:
            print(f"- íŠ¹ì§•: {', '.join(analysis['key_features'])}")
    
    print(f"\nğŸ¯ **ì¶”ì²œ í•œêµ­ HSK ì½”ë“œ**")
    print(f"- ì½”ë“œ: {korea_rec['hs_code']}")
    print(f"- ìƒí’ˆëª…: {enhanced_korea_name}")
    print(f"- ì‹ ë¢°ë„: {korea_rec['confidence']:.1%}")
    print(f"- ë°ì´í„° ì¶œì²˜: {korea_rec.get('data_source', 'í†µí•©')}")
    
    # ëŒ€ì•ˆ ë¶„ë¥˜ ì •ë³´
    if korea_rec.get('is_alternative_classification'):
        print(f"- ë¶„ë¥˜ ìœ í˜•: ëŒ€ì•ˆ ë¶„ë¥˜ (HS {korea_rec['source_hs6']})")
    
    print(f"\nğŸ“Š **ë¶„ì„ ì •ë³´**")
    print(f"- HS 6ìë¦¬ ë§¤ì¹­: âœ… ì™„ë£Œ ({hs_analysis['us_hs6']})")
    print(f"- êµ¬ì¡° ìœ ì‚¬ë„: {hs_analysis['hs_similarity']:.1%}")
    print(f"- ì˜ë¯¸ ìœ ì‚¬ë„: {hs_analysis['semantic_similarity']:.1%}")
    print(f"- í›„ë³´êµ° ìˆ˜: {hs_analysis['total_candidates']}ê°œ")
    
    # LLM ì„¤ëª…
    if result.get('explanation'):
        print(f"\nğŸ’¬ **ë§¤ì¹­ ì„¤ëª…**")
        print(f"{result['explanation']}")
    
    # ê¸°íƒ€ í•­ëª©ì¸ ê²½ìš° ë§¥ë½ ì„¤ëª… ì¶”ê°€
    if is_other_item(us_info.get('english_name', '')) or is_other_item(korea_rec.get('name_kr', '')):
        print(f"\nğŸ’¡ **ë¶„ì•¼ ë§¥ë½**")
        print(f"- HS {hs6} ë¶„ë¥˜: {hs6_description}")
        print(f"- í•´ì„: ì´ëŠ” '{hs6_description} ë¶„ì•¼ì˜ ê¸°íƒ€ í•­ëª©'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
    
    # ìƒìœ„ í›„ë³´ ëª©ë¡ (ê°œì„ ëœ í…Œì´ë¸”) - 3ê°œ ëª¨ë‘ í‘œì‹œ
    if 'all_candidates' in result and result['all_candidates']:
        print(f"\nğŸ¯ **ìƒìœ„ í›„ë³´ ëª©ë¡**")
        print_candidates_table(result['all_candidates'][:3], hs6_description)

def print_success_result(result, converter):
    """ê¸°ì¡´ ì„±ê³µ ê²°ê³¼ ì¶œë ¥ (í˜¸í™˜ì„± ìœ ì§€)"""
    print_enhanced_success_result(result, converter)

def print_error_result(result):
    """ì˜¤ë¥˜ ê²°ê³¼ ì¶œë ¥"""
    print("âŒ **ì˜¤ë¥˜ ë°œìƒ**\n")
    print(f"ë©”ì‹œì§€: {result['message']}")
    
    if 'suggestions' in result and result['suggestions']:
        print(f"\nğŸ’¡ **ìœ ì‚¬í•œ ì½”ë“œ ì œì•ˆ:**")
        for suggestion in result['suggestions'][:3]:
            print(f"- {suggestion}")

def print_no_match_result(result, converter):
    """HS 6ìë¦¬ ë§¤ì¹­ ì‹¤íŒ¨ ê²°ê³¼ ì¶œë ¥"""
    print("âŒ **HS 6ìë¦¬ ë§¤ì¹­ ì‹¤íŒ¨**\n")
    print(f"ë¯¸êµ­ HS ì½”ë“œ '{result['us_hs_code']}'ì˜ HS 6ìë¦¬ '{result['hs6']}'ì— í•´ë‹¹í•˜ëŠ” í•œêµ­ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.\n")
    
    print("ğŸ“‹ **ë¯¸êµ­ ì½”ë“œ ì •ë³´**")
    print(f"- ì˜ë¬¸ëª…: {result['us_info']['english_name']}")
    print(f"- í•œê¸€ëª…: {result['us_info'].get('korean_name', 'ì—†ìŒ')}")
    
    # LLM ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    if result.get('product_analysis'):
        analysis = result['product_analysis']
        print(f"\nğŸ§  **LLM ìƒí’ˆ ë¶„ì„**")
        print(f"- ì¬ì§ˆ: {analysis['material']}")
        print(f"- ê¸°ëŠ¥: {analysis['function']}")
        print(f"- ë¶„ì„: {analysis['reasoning']}")
    
    print(f"\nğŸ’¡ **ë¶„ì•¼ ë§¥ë½**")
    hs6_description = converter.get_hs6_description(result['hs6'])
    print(f"- HS {result['hs6']} ë¶„ë¥˜: {hs6_description}")
    print(f"- ì´ ìƒí’ˆì€ {hs6_description} ë¶„ì•¼ì— ì†í•˜ì§€ë§Œ í•œêµ­ì—ì„œëŠ” ë‹¤ë¥¸ ë¶„ë¥˜ ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜")
    print(f"  í•´ë‹¹ ìƒí’ˆì´ ì·¨ê¸‰ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
