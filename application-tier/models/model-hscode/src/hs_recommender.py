
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any
import os

import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, current_dir)
sys.path.insert(0, parent_dir)

from data_processor import DataProcessor
from search_engine import SearchEngine
from cache_manager import CacheManager
from config import SYSTEM_CONFIG

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class HSCodeRecommender:
    """HS ì½”ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤ (final_combined_text ì§€ì›)"""
    
    def __init__(self, semantic_model_name: str = None, top_k: int = None, cache_dir: str = './cache'):
        self.semantic_model_name = semantic_model_name or SYSTEM_CONFIG['semantic_model']
        self.top_k = top_k or SYSTEM_CONFIG['top_k']
        self.cache_dir = cache_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_processor = DataProcessor(debug_mode=False)
        self.search_engine = SearchEngine(self.semantic_model_name)
        self.cache_manager = CacheManager(cache_dir)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.is_initialized = False
        self.openai_client = None
        self.integrated_df = None
        
        print(f"HS ì½”ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"  ì˜ë¯¸ ëª¨ë¸: {self.semantic_model_name}")
        print(f"  ìƒìœ„ ê²°ê³¼ ìˆ˜: {self.top_k}")
        print(f"  ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")
    
    def initialize_openai(self) -> bool:
        """OpenAI API ì´ˆê¸°í™”"""
        if not OPENAI_AVAILABLE:
            print("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        try:
            api_file = SYSTEM_CONFIG.get('openai_api_file', 'openai_api.txt')
            
            if os.path.exists(api_file):
                with open(api_file, 'r', encoding='utf-8') as f:
                    api_key = f.read().strip()
                
                self.openai_client = openai.OpenAI(api_key=api_key)
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
                test_response = self.openai_client.chat.completions.create(
                    model="gpt-4.1-mini",
                    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
                    max_tokens=10
                )
                
                print("OpenAI API ì´ˆê¸°í™” ì„±ê³µ")
                return True
            else:
                print(f"API í‚¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {api_file}")
                return False
                
        except Exception as e:
            print(f"OpenAI API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_data(self, force_rebuild: bool = False) -> bool:
        """ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì¶• (final_combined_text ì§€ì›)"""
        try:
            # ìºì‹œ í™•ì¸
            if not force_rebuild and self.cache_manager.is_cache_valid(self.semantic_model_name):
                print("ìœ íš¨í•œ ìºì‹œ ë°œê²¬ - ìºì‹œì—ì„œ ë¡œë“œ")
                cache_data = self.cache_manager.load_cache()
                
                if cache_data:
                    self.integrated_df = cache_data['integrated_df']
                    
                    # final_combined_text ì»¬ëŸ¼ í™•ì¸
                    if 'final_combined_text' not in self.integrated_df.columns:
                        print("  âŒ ìºì‹œëœ ë°ì´í„°ì— final_combined_text ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                        print("  ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤...")
                        force_rebuild = True
                    else:
                        # ê²€ìƒ‰ ì—”ì§„ì— ë°ì´í„° ë¡œë“œ
                        if self.search_engine.load_data(
                            self.integrated_df,
                            cache_data.get('standard_mapping', {}),
                            cache_data.get('reverse_mapping', {}),
                            cache_data.get('chapter_descriptions', {})
                        ):
                            # ì¸ë±ìŠ¤ ë³µì›
                            self.search_engine.tfidf_matrix = cache_data['tfidf_matrix']
                            self.search_engine.tfidf_vectorizer = cache_data['tfidf_vectorizer']
                            self.search_engine.semantic_embeddings = cache_data['semantic_embeddings']
                            
                            self.is_initialized = True
                            print("ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
                            return True
                        else:
                            force_rebuild = True
            
            # ìºì‹œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ê°•ì œ ì¬êµ¬ì¶•
            if force_rebuild or not self.is_initialized:
                print("ë°ì´í„° ìƒˆë¡œ ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
                
                # 1. ë°ì´í„° ë¡œë“œ ë° í†µí•©
                if not self.data_processor.load_all_data():
                    print("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
                    return False
                
                self.integrated_df = self.data_processor.get_integrated_data()
                chapter_descriptions = self.data_processor.get_chapter_descriptions()
                
                # final_combined_text ì»¬ëŸ¼ í™•ì¸
                if 'final_combined_text' not in self.integrated_df.columns:
                    print("âŒ ë¡œë“œëœ ë°ì´í„°ì— final_combined_text ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                    return False
                
                print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.integrated_df)}ê°œ í•­ëª©")
                
                # 2. í‘œì¤€í’ˆëª… ë§¤í•‘ êµ¬ì¶•
                standard_mapping, reverse_mapping = self._build_standard_mappings()
                
                # 3. ê²€ìƒ‰ ì—”ì§„ì— ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì¶•
                if not self.search_engine.load_data(self.integrated_df, standard_mapping, reverse_mapping, chapter_descriptions):
                    print("ê²€ìƒ‰ ì—”ì§„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
                    return False
                
                self.search_engine.build_index()
                
                # 4. ìºì‹œ ì €ì¥
                self.cache_manager.save_cache(
                    self.integrated_df,
                    self.search_engine.semantic_embeddings,
                    self.search_engine.tfidf_matrix,
                    self.search_engine.tfidf_vectorizer,
                    standard_mapping,
                    reverse_mapping,
                    chapter_descriptions,
                    self.semantic_model_name
                )
                
                self.is_initialized = True
                print("âœ… ëª¨ë“  ì´ˆê¸°í™” ì™„ë£Œ!")
            
            return True
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def _build_standard_mappings(self) -> tuple:
        """í‘œì¤€í’ˆëª… ë§¤í•‘ êµ¬ì¶•"""
        print("í‘œì¤€í’ˆëª… ë§¤í•‘ êµ¬ì¶• ì¤‘...")
        
        standard_mapping = {}
        reverse_mapping = {}
        
        if self.integrated_df is None:
            return standard_mapping, reverse_mapping
        
        # í‘œì¤€í’ˆëª… ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
        std_columns = []
        for col in self.integrated_df.columns:
            if any(keyword in col.lower() for keyword in ['í‘œì¤€í’ˆëª…', 'standard', 'í’ˆëª…']):
                if col not in ['HS_KEY', 'final_combined_text', 'data_source']:
                    std_columns.append(col)
        
        mapping_count = 0
        
        for _, row in self.integrated_df.iterrows():
            hs_key = row.get('HS_KEY', '')
            if not hs_key:
                continue
            
            # í‘œì¤€í’ˆëª…ë“¤ ìˆ˜ì§‘
            standard_names = set()
            for col in std_columns:
                if pd.notna(row[col]):
                    name = str(row[col]).strip().lower()
                    if name and len(name) > 1:
                        standard_names.add(name)
            
            # ë§¤í•‘ êµ¬ì¶•
            for std_name in standard_names:
                if std_name not in standard_mapping:
                    standard_mapping[std_name] = []
                if hs_key not in standard_mapping[std_name]:
                    standard_mapping[std_name].append(hs_key)
                
                if hs_key not in reverse_mapping:
                    reverse_mapping[hs_key] = []
                if std_name not in reverse_mapping[hs_key]:
                    reverse_mapping[hs_key].append(std_name)
                
                mapping_count += 1
        
        print(f"  ë§¤í•‘ ì™„ë£Œ: {len(standard_mapping)}ê°œ í‘œì¤€í’ˆëª…, {mapping_count}ê°œ ë§¤í•‘")
        return standard_mapping, reverse_mapping
    
    def recommend(self, query: str, material: str = "", usage: str = "", 
                  use_llm: bool = False, final_count: int = 3) -> Dict:
        """HS ì½”ë“œ ì¶”ì²œ ì‹¤í–‰"""
        if not self.is_initialized:
            raise ValueError("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_data()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        print(f"\n{'='*60}")
        print(f"HS ì½”ë“œ ì¶”ì²œ ì‹¤í–‰")
        print(f"{'='*60}")
        print(f"ì¿¼ë¦¬: '{query}'")
        if material:
            print(f"ì¬ì§ˆ: '{material}'")
        if usage:
            print(f"ìš©ë„: '{usage}'")
        print(f"LLM ë¶„ì„: {'ì‚¬ìš©' if use_llm else 'ë¯¸ì‚¬ìš©'}")
        
        try:
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
            search_results = self.search_engine.search(query, material, usage)
            
            if len(search_results) == 0:
                return {
                    'query': query,
                    'material': material,
                    'usage': usage,
                    'recommendations': [],
                    'search_info': {'total_candidates': 0, 'llm_analysis': None}
                }
            
            # 2. LLM ë¶„ì„ (ì„ íƒì )
            llm_analysis = None
            if use_llm and self.openai_client:
                llm_analysis = self._analyze_with_llm(query, material, usage, search_results.head(10))
            
            # 3. ìµœì¢… ì¶”ì²œ ê²°ê³¼ ìƒì„±
            recommendations = self._format_recommendations(search_results.head(final_count * 2), llm_analysis, final_count)
            
            return {
                'query': query,
                'material': material,
                'usage': usage,
                'recommendations': recommendations[:final_count],
                'search_info': {
                    'total_candidates': len(search_results),
                    'semantic_model': self.semantic_model_name,
                    'llm_analysis': llm_analysis,
                    'expanded_query': search_results.iloc[0]['expanded_query'] if len(search_results) > 0 else query
                }
            }
            
        except Exception as e:
            print(f"ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                'query': query,
                'material': material,
                'usage': usage,
                'recommendations': [],
                'search_info': {'error': str(e), 'total_candidates': 0}
            }
    
    def _analyze_with_llm(self, query: str, material: str, usage: str, candidates: pd.DataFrame) -> Optional[Dict]:
        """LLMì„ í™œìš©í•œ í›„ë³´ ë¶„ì„"""
        try:
            # í›„ë³´ ì •ë³´ ì¤€ë¹„
            candidate_info = []
            for idx, row in candidates.head(3).iterrows():
                hs_key = row.get('HS_KEY', '')
                hs_code = row.get('HSë¶€í˜¸', hs_key)
                
                # ì´ë¦„ ì •ë³´
                name_kr = ''
                for col in ['í•œê¸€í’ˆëª©ëª…', 'ì„¸ë²ˆ10ë‹¨ìœ„í’ˆëª…', 'í‘œì¤€í’ˆëª…']:
                    if col in row and pd.notna(row[col]):
                        name_kr = str(row[col])
                        break
                
                # ì„¤ëª… ì •ë³´
                description = ''
                if 'final_combined_text' in row and pd.notna(row['final_combined_text']):
                    description = str(row['final_combined_text'])[:200]
                
                candidate_info.append({
                    'hs_code': hs_code,
                    'name_kr': name_kr,
                    'description': description,
                    'score': row.get('hybrid_score', 0)
                })
            
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ìƒí’ˆì— ëŒ€í•œ HS ì½”ë“œ ì¶”ì²œì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ìƒí’ˆ ì •ë³´:
- ìƒí’ˆëª…: {query}
- ì¬ì§ˆ: {material if material else 'ëª…ì‹œë˜ì§€ ì•ŠìŒ'}
- ìš©ë„: {usage if usage else 'ëª…ì‹œë˜ì§€ ì•ŠìŒ'}

ì¶”ì²œ í›„ë³´ë“¤:
"""
            
            for i, candidate in enumerate(candidate_info, 1):
                prompt += f"""
{i}. HSì½”ë“œ: {candidate['hs_code']}
   í•œê¸€ëª…: {candidate['name_kr']}
   ì„¤ëª…: {candidate['description'][:100]}...
   ì ìˆ˜: {candidate['score']:.3f}
"""
            
            prompt += """
ê° í›„ë³´ì— ëŒ€í•´ ë¶„ì„ í›„ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
  "analysis": [
    {
      "hs_code": "ì½”ë“œ",
      "fitness_score": ì ìˆ˜(1-10),
      "reason": "ì¶”ì²œ ì´ìœ ",
      "caution": "ì£¼ì˜ì‚¬í•­"
    }
  ],
  "recommendation": "ì „ì²´ì ì¸ ì¶”ì²œ ì˜ê²¬"
}
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ HS ì½”ë“œ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            try:
                import json
                return json.loads(response.choices[0].message.content)
            except:
                return {"analysis": [], "recommendation": response.choices[0].message.content}
                
        except Exception as e:
            print(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _format_recommendations(self, search_results: pd.DataFrame, llm_analysis: Optional[Dict], final_count: int) -> List[Dict]:
        """ì¶”ì²œ ê²°ê³¼ í¬ë§·íŒ…"""
        recommendations = []
        
        # LLM ë¶„ì„ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        llm_scores = {}
        if llm_analysis and 'analysis' in llm_analysis:
            for item in llm_analysis['analysis']:
                hs_code = item.get('hs_code', '')
                llm_scores[hs_code] = {
                    'fitness_score': item.get('fitness_score', 0),
                    'reason': item.get('reason', ''),
                    'caution': item.get('caution', '')
                }
        
        for idx, row in search_results.iterrows():
            hs_key = row.get('HS_KEY', '')
            hs_code = row.get('HSë¶€í˜¸', hs_key)
            
            # ì´ë¦„ ì •ë³´
            name_kr = self._extract_best_name(row, ['í•œê¸€í’ˆëª©ëª…', 'ì„¸ë²ˆ10ë‹¨ìœ„í’ˆëª…', 'í‘œì¤€í’ˆëª…'])
            name_en = self._extract_best_name(row, ['ì˜ë¬¸í’ˆëª©ëª…', 'í‘œì¤€í’ˆëª…ì˜ë¬¸'])
            
            # ì„¤ëª… ì •ë³´
            description = ''
            if 'final_combined_text' in row and pd.notna(row['final_combined_text']):
                description = str(row['final_combined_text'])
            
            # ì ìˆ˜ ë° ì‹ ë¢°ë„
            hybrid_score = row.get('hybrid_score', 0)
            confidence = min(hybrid_score, 1.0)
            
            # LLM ì ìˆ˜ ë°˜ì˜
            llm_info = llm_scores.get(hs_code, {})
            if llm_info.get('fitness_score'):
                llm_fitness = llm_info['fitness_score'] / 10.0
                confidence = (confidence + llm_fitness) / 2
            
            recommendation = {
                'hs_code': hs_code,
                'hs_key': hs_key,
                'name_kr': name_kr,
                'name_en': name_en,
                'description': description[:500] if description else '',
                'chapter': row.get('chapter', hs_key[:2] if hs_key else ''),
                'heading': row.get('heading', hs_key[:4] if hs_key else ''),
                'confidence': round(confidence, 4),
                'scores': {
                    'hybrid': round(hybrid_score, 4),
                    'keyword': round(row.get('keyword_score', 0), 4),
                    'semantic': round(row.get('semantic_score', 0), 4)
                },
                'data_source': row.get('data_source', ''),
                'is_standard_match': row.get('is_standard_match', False)
            }
            
            if llm_info:
                recommendation['llm_analysis'] = llm_info
            
            recommendations.append(recommendation)
            
            if len(recommendations) >= final_count:
                break
        
        return recommendations
    
    def _extract_best_name(self, row: pd.Series, column_candidates: List[str]) -> str:
        """ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìµœì ì˜ ì´ë¦„ ì¶”ì¶œ"""
        for col in column_candidates:
            if col in row and pd.notna(row[col]):
                name = str(row[col]).strip()
                if name and len(name) > 1:
                    return name
        return ''
    
    def print_results(self, results: Dict, query: str):
        """ì¶”ì²œ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"'{query}' ê²€ìƒ‰ ê²°ê³¼")
        print(f"{'='*80}")
        
        recommendations = results.get('recommendations', [])
        
        if not recommendations:
            print("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        for i, rec in enumerate(recommendations, 1):
            print(f"\n{i}. HS ì½”ë“œ: {rec['hs_code']}")
            print(f"   í•œê¸€ëª…: {rec['name_kr']}")
            if rec['name_en']:
                print(f"   ì˜ë¬¸ëª…: {rec['name_en']}")
            print(f"   ì‹ ë¢°ë„: {rec['confidence']:.3f}")
            print(f"   ì¥: {rec['chapter']}, í˜¸: {rec['heading']}")
            
            if rec.get('description'):
                desc = rec['description']
                if len(desc) > 150:
                    desc = desc[:150] + "..."
                print(f"   ì„¤ëª…: {desc}")
            
            # LLM ë¶„ì„ ê²°ê³¼
            if rec.get('llm_analysis'):
                llm = rec['llm_analysis']
                if llm.get('reason'):
                    print(f"   AI ë¶„ì„: {llm['reason']}")
                if llm.get('caution'):
                    print(f"   ì£¼ì˜ì‚¬í•­: {llm['caution']}")
            
            print(f"   ë°ì´í„° ì†ŒìŠ¤: {rec.get('data_source', '')}")
        
        # ê²€ìƒ‰ ì •ë³´
        search_info = results.get('search_info', {})
        print(f"\nê²€ìƒ‰ ì •ë³´:")
        print(f"  ì´ í›„ë³´: {search_info.get('total_candidates', 0)}ê°œ")
        print(f"  ì˜ë¯¸ ëª¨ë¸: {search_info.get('semantic_model', '')}")
        
        if search_info.get('llm_analysis') and search_info['llm_analysis'].get('recommendation'):
            print(f"\nì „ì²´ AI ì¶”ì²œ ì˜ê²¬:")
            print(f"  {search_info['llm_analysis']['recommendation']}")
    
    def get_statistics(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        stats = {
            'system_initialized': self.is_initialized,
            'openai_available': self.openai_client is not None,
            'semantic_model': self.semantic_model_name,
            'cache_dir': self.cache_dir
        }
        
        if self.integrated_df is not None:
            stats.update({
                'total_items': len(self.integrated_df),
                'chapters': len(self.integrated_df['chapter'].unique()) if 'chapter' in self.integrated_df.columns else 0,
                'data_sources': self.integrated_df['data_source'].value_counts().to_dict() if 'data_source' in self.integrated_df.columns else {}
            })
            
            # í‘œì¤€í’ˆëª… ì»¤ë²„ë¦¬ì§€
            if 'data_source' in self.integrated_df.columns:
                with_std = self.integrated_df['data_source'].str.contains('std', na=False).sum()
                coverage = (with_std / len(self.integrated_df)) * 100
                stats['standard_coverage'] = coverage
        
        # ìºì‹œ ì •ë³´
        cache_info = self.cache_manager.get_cache_info(self.semantic_model_name)
        stats['cache_info'] = cache_info
        
        return stats
    
    def get_cache_info(self) -> Dict:
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        return self.cache_manager.get_cache_info(self.semantic_model_name)
    
    def clear_cache(self) -> int:
        """ìºì‹œ ì‚­ì œ"""
        return self.cache_manager.clear_cache()
    
    def copy_cache_from_colab(self, colab_cache_dir: str) -> bool:
        """ì½”ë©ì—ì„œ ìºì‹œ ë³µì‚¬"""
        return self.cache_manager.copy_from_colab(colab_cache_dir)
    
 
    
    def _parse_llm_candidates(self, llm_response: str) -> List[Dict]:
        """LLM ì‘ë‹µì—ì„œ HS ì½”ë“œ í›„ë³´ íŒŒì‹±"""
        try:
            import json
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            start = llm_response.find('{')
            end = llm_response.rfind('}') + 1
            if start != -1 and end != 0:
                json_str = llm_response[start:end]
                data = json.loads(json_str)
                return data.get('candidates', [])
        except:
            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
            candidates = []
            lines = llm_response.split('\n')
            for line in lines:
                if any(char.isdigit() for char in line) and len([c for c in line if c.isdigit()]) >= 10:
                    # 10ìë¦¬ ìˆ«ìê°€ í¬í•¨ëœ ë¼ì¸ì—ì„œ HS ì½”ë“œ ì¶”ì¶œ ì‹œë„
                    import re
                    hs_match = re.search(r'\b(\d{10})\b', line)
                    if hs_match:
                        candidates.append({
                            'hs_code': hs_match.group(1),
                            'confidence': 7,  # ê¸°ë³¸ê°’
                            'reasoning': line.strip(),
                            'chapter': hs_match.group(1)[:2],
                            'category': 'LLM ì œì•ˆ'
                        })
            return candidates[:5]  # ìµœëŒ€ 5ê°œ
        
        return []
    

    def recommend_ultimate(self, query: str, material: str = "", usage: str = "", 
                          final_count: int = 5) -> Dict:
        """LLM í†µí•© ì¶”ì²œ ì‹œìŠ¤í…œ"""
        if not self.is_initialized:
            raise ValueError("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_data()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if not self.openai_client:
            print("âš ï¸ OpenAIê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¶”ì²œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self.recommend(query, material, usage, use_llm=False, final_count=final_count)
        
        print(f"\n{'='*60}")
        print(f"ğŸ§  LLM í†µí•© ì¶”ì²œ ì‹œìŠ¤í…œ")
        print(f"{'='*60}")
        print(f"ì¿¼ë¦¬: '{query}'")
        if material:
            print(f"ì¬ì§ˆ: '{material}'")
        if usage:
            print(f"ìš©ë„: '{usage}'")
        
        try:
            # 1. LLM ì§ì ‘ í›„ë³´ ìƒì„±
            print(f"\n1. LLM ì§ì ‘ í›„ë³´ ìƒì„±...")
            llm_candidates = self._get_llm_candidates(query, material, usage)
            print(f"  LLM í›„ë³´ í†µí•©: {len(llm_candidates)}ê°œ")
            
            # 2. ê²€ìƒ‰ì—”ì§„ í›„ë³´ ìƒì„±
            print(f"\n2. ê²€ìƒ‰ì—”ì§„ í›„ë³´ ìƒì„±...")
            search_results = self.search_engine.search(query, material, usage)
            print(f"  ê²€ìƒ‰ í›„ë³´: {len(search_results)}ê°œ")
            
            # 3. LLM í›„ë³´ì™€ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
            print(f"\n3. LLM í›„ë³´ì™€ ê²€ìƒ‰ ê²°ê³¼ í†µí•©...")
            integrated_results = self._integrate_llm_and_search(llm_candidates, search_results)
            print(f"  í†µí•© í›„ë³´: {len(integrated_results)}ê°œ")
            
            # 4. LLM ì¬ìˆœìœ„ ë¶„ì„
            print(f"\n4. LLM ì¬ìˆœìœ„ ë¶„ì„...")
            reranked_results = self._llm_rerank(query, material, usage, integrated_results.head(20))
            print(f"  LLM ì¬ìˆœìœ„ ë¶„ì„: {len(reranked_results)}ê°œ í›„ë³´")
            
            # 5. ìµœì¢… ì¶”ì²œ ê²°ê³¼ ìƒì„±
            print(f"\n5. ìµœì¢… ì¶”ì²œ ê²°ê³¼ ìƒì„±...")
            recommendations = self._format_ultimate_recommendations(reranked_results, final_count)
            
            return {
                'query': query,
                'material': material,
                'usage': usage,
                'recommendations': recommendations,
                'search_info': {
                    'method': 'ultimate_llm_hybrid',
                    'llm_candidates': len(llm_candidates),
                    'search_candidates': len(search_results),
                    'total_candidates': len(integrated_results),
                    'semantic_model': self.semantic_model_name,
                    'llm_model': 'gpt-4.1-mini',
                }
            }
            
        except Exception as e:
            print(f"LLM í†µí•© ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            return self.recommend(query, material, usage, use_llm=True, final_count=final_count)
    
    def _get_llm_candidates(self, query: str, material: str, usage: str) -> List[Dict]:
        """LLMì´ ì§ì ‘ HS ì½”ë“œ í›„ë³´ë¥¼ ì œì•ˆ"""
        try:
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ìƒí’ˆì— ëŒ€í•œ HS ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ í›„ë³´ 3ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”:

ìƒí’ˆ ì •ë³´:
- ìƒí’ˆëª…: {query}
- ì¬ì§ˆ: {material if material else 'ëª…ì‹œë˜ì§€ ì•ŠìŒ'}
- ìš©ë„: {usage if usage else 'ëª…ì‹œë˜ì§€ ì•ŠìŒ'}

ìš”êµ¬ì‚¬í•­:
1. í•œêµ­ ê´€ì„¸ë²•ìƒ HS ì½”ë“œ 10ìë¦¬ë¥¼ ì •í™•íˆ ì œì•ˆí•˜ì„¸ìš”
2. ê° í›„ë³´ì— ëŒ€í•´ 1-10ì  í™•ì‹ ë„ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”
3. ì œì•ˆ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "candidates": [
    {{
      "hs_code": "1234567890",
      "confidence": 9,
      "reason": "ì œì•ˆ ì´ìœ "
    }}
  ]
}}
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ ê´€ì„¸ë²• ë° HS ì½”ë“œ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            try:
                import json
                result = json.loads(response.choices[0].message.content)
                candidates = result.get('candidates', [])
                
                # í›„ë³´ë“¤ì„ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
                candidates.sort(key=lambda x: x.get('confidence', 0), reverse=True)
                
                # ê²€ìƒ‰ì—”ì§„ì—ì„œ í•´ë‹¹ HS ì½”ë“œë“¤ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
                enriched_candidates = []
                for candidate in candidates[:5]:
                    hs_code = candidate.get('hs_code', '')
                    if len(hs_code) == 10:
                        # ë°ì´í„°ì—ì„œ í•´ë‹¹ HS ì½”ë“œ ì°¾ê¸°
                        matching_rows = self.integrated_df[
                            (self.integrated_df['HS_KEY'] == hs_code) |
                            (self.integrated_df.get('HSë¶€í˜¸', '') == hs_code)
                        ]
                        
                        if not matching_rows.empty:
                            row = matching_rows.iloc[0]
                            enriched_candidates.append({
                                'hs_code': hs_code,
                                'confidence': candidate.get('confidence', 0),
                                'reason': candidate.get('reason', ''),
                                'row_data': row,
                                'source': 'llm_direct'
                            })
                        else:
                            # ë°ì´í„°ì— ì—†ëŠ” ê²½ìš°ë„ í›„ë³´ë¡œ í¬í•¨ (LLM ì „ìš©)
                            enriched_candidates.append({
                                'hs_code': hs_code,
                                'confidence': candidate.get('confidence', 0),
                                'reason': candidate.get('reason', ''),
                                'row_data': None,
                                'source': 'llm_only'
                            })
                
                print(f"    âœ… LLM ì§ì ‘ ì œì•ˆ: {len(enriched_candidates)}ê°œ")
                for i, cand in enumerate(enriched_candidates, 1):
                    match_status = "âœ… ë°ì´í„° ë§¤ì¹­" if cand['row_data'] is not None else "ğŸ†• LLM ì „ìš©"
                    print(f"    {i}. {cand['hs_code']} (í™•ì‹ ë„: {cand['confidence']}) - {match_status}")
                
                return enriched_candidates
                
            except json.JSONDecodeError:
                print("    âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
                return []
                
        except Exception as e:
            print(f"    âŒ LLM í›„ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _integrate_llm_and_search(self, llm_candidates: List[Dict], search_results: pd.DataFrame) -> pd.DataFrame:
        """LLM í›„ë³´ì™€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©"""
        integrated_rows = []
        used_hs_codes = set()
        
        # 1. LLM í›„ë³´ë“¤ì„ ìš°ì„  ì²˜ë¦¬
        for llm_candidate in llm_candidates:
            hs_code = llm_candidate['hs_code']
            
            if llm_candidate['row_data'] is not None:
                # ë°ì´í„°ì— ìˆëŠ” LLM í›„ë³´
                row = llm_candidate['row_data'].copy()
                
                # ê²€ìƒ‰ ê²°ê³¼ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
                search_match = search_results[
                    (search_results['HS_KEY'] == hs_code) |
                    (search_results.get('HSë¶€í˜¸', '') == hs_code)
                ]
                
                if not search_match.empty:
                    # LLM + ê²€ìƒ‰ ë§¤ì¹­: ë†’ì€ ê°€ì¤‘ì¹˜
                    search_row = search_match.iloc[0]
                    hybrid_score = search_row.get('hybrid_score', 0)
                    confidence_score = llm_candidate['confidence'] / 10.0
                    ultimate_score = (hybrid_score * 0.6) + (confidence_score * 0.4)
                    
                    row['ultimate_score'] = ultimate_score
                    row['llm_confidence'] = llm_candidate['confidence']
                    row['llm_reason'] = llm_candidate['reason']
                    row['match_type'] = 'llm_search_match'
                    row['hybrid_score'] = hybrid_score
                    
                    print(f"    âœ… LLM+ê²€ìƒ‰ ë§¤ì¹­: {hs_code} (ì ìˆ˜: {ultimate_score:.3f})")
                else:
                    # LLM ì „ìš© í›„ë³´: ì¤‘ê°„ ê°€ì¤‘ì¹˜
                    confidence_score = llm_candidate['confidence'] / 10.0
                    ultimate_score = confidence_score * 0.7
                    
                    row['ultimate_score'] = ultimate_score
                    row['llm_confidence'] = llm_candidate['confidence']
                    row['llm_reason'] = llm_candidate['reason']
                    row['match_type'] = 'llm_only_with_data'
                    row['hybrid_score'] = 0.0
                    
                    print(f"    ğŸ” LLM ë°ì´í„° í›„ë³´: {hs_code} (ì ìˆ˜: {ultimate_score:.3f})")
                
                integrated_rows.append(row)
                used_hs_codes.add(hs_code)
            else:
                # ë°ì´í„°ì— ì—†ëŠ” LLM ì „ìš© í›„ë³´
                confidence_score = llm_candidate['confidence'] / 10.0
                ultimate_score = confidence_score * 0.5
                
                # ê°€ìƒì˜ í–‰ ìƒì„±
                fake_row = pd.Series({
                    'HS_KEY': hs_code,
                    'HSë¶€í˜¸': hs_code,
                    'í•œê¸€í’ˆëª©ëª…': f"LLM ì œì•ˆ: {hs_code}",
                    'ì˜ë¬¸í’ˆëª©ëª…': f"LLM Suggested: {hs_code}",
                    'ultimate_score': ultimate_score,
                    'llm_confidence': llm_candidate['confidence'],
                    'llm_reason': llm_candidate['reason'],
                    'match_type': 'llm_only_no_data',
                    'hybrid_score': 0.0,
                    'chapter': hs_code[:2] if len(hs_code) >= 2 else '',
                    'heading': hs_code[:4] if len(hs_code) >= 4 else '',
                    'data_source': 'llm_generated',
                    'final_combined_text': f"LLMì´ ì œì•ˆí•œ HS ì½”ë“œ: {hs_code}. ì´ìœ : {llm_candidate['reason']}"
                })
                
                integrated_rows.append(fake_row)
                used_hs_codes.add(hs_code)
                print(f"    ğŸ†• LLM ì „ìš© í›„ë³´: {hs_code} (í™•ì‹ ë„: {llm_candidate['confidence']})")
        
        # 2. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ LLMì— ì—†ëŠ” ê²ƒë“¤ ì¶”ê°€
        for _, search_row in search_results.iterrows():
            hs_key = search_row.get('HS_KEY', '')
            hs_code = search_row.get('HSë¶€í˜¸', hs_key)
            
            if hs_code not in used_hs_codes:
                # ê²€ìƒ‰ ì „ìš© í›„ë³´: ë‚®ì€ ê°€ì¤‘ì¹˜
                hybrid_score = search_row.get('hybrid_score', 0)
                ultimate_score = hybrid_score * 0.8
                
                row = search_row.copy()
                row['ultimate_score'] = ultimate_score
                row['llm_confidence'] = 0
                row['llm_reason'] = ''
                row['match_type'] = 'search_only'
                
                integrated_rows.append(row)
                used_hs_codes.add(hs_code)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  ì •ë ¬
        if integrated_rows:
            integrated_df = pd.DataFrame(integrated_rows)
            integrated_df = integrated_df.sort_values('ultimate_score', ascending=False).reset_index(drop=True)
            return integrated_df
        else:
            return pd.DataFrame()
    
    def _llm_rerank(self, query: str, material: str, usage: str, candidates: pd.DataFrame) -> pd.DataFrame:
        """LLMì„ í™œìš©í•œ í›„ë³´ ì¬ìˆœìœ„ ê²°ì •"""
        if len(candidates) == 0:
            return candidates
        
        try:
            # ìƒìœ„ í›„ë³´ë“¤ ì •ë³´ ì¤€ë¹„
            candidate_info = []
            for idx, row in candidates.head(10).iterrows():
                hs_code = row.get('HS_KEY') or row.get('HSë¶€í˜¸', '')
                name_kr = row.get('í•œê¸€í’ˆëª©ëª…', '')
                description = row.get('final_combined_text', '')[:150]
                current_score = row.get('ultimate_score', 0)
                match_type = row.get('match_type', '')
                
                candidate_info.append({
                    'hs_code': hs_code,
                    'name_kr': name_kr,
                    'description': description,
                    'current_score': current_score,
                    'match_type': match_type
                })
            
            # LLM ì¬ìˆœìœ„ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ìƒí’ˆì— ëŒ€í•œ HS ì½”ë“œ í›„ë³´ë“¤ì„ ì¬ìˆœìœ„í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”:

ìƒí’ˆ ì •ë³´:
- ìƒí’ˆëª…: {query}
- ì¬ì§ˆ: {material if material else 'ëª…ì‹œë˜ì§€ ì•ŠìŒ'}
- ìš©ë„: {usage if usage else 'ëª…ì‹œë˜ì§€ ì•ŠìŒ'}

í›„ë³´ ëª©ë¡:
"""
            
            for i, cand in enumerate(candidate_info, 1):
                prompt += f"""
{i}. HSì½”ë“œ: {cand['hs_code']}
   í•œê¸€ëª…: {cand['name_kr']}
   ì„¤ëª…: {cand['description']}
   í˜„ì¬ì ìˆ˜: {cand['current_score']:.3f}
   ë§¤ì¹­ìœ í˜•: {cand['match_type']}
"""
            
            prompt += """
ê° í›„ë³´ë¥¼ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-10ì  í‰ê°€í•˜ê³  ì¬ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”:
1. ì¬ì§ˆ ì •í™•ì„±
2. ìš©ë„ ì í•©ì„±  
3. HS ì²´ê³„ìƒ ì •í™•ì„±
4. ì‹¤ë¬´ ì‚¬ìš© ë¹ˆë„

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "rankings": [
    {
      "hs_code": "ì½”ë“œ",
      "rank": 1,
      "rerank_score": 9.5,
      "reason": "ì¬ìˆœìœ„ ê·¼ê±°"
    }
  ]
}
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ HS ì½”ë“œ ë¶„ë¥˜ ë° ë¬´ì—­ ì‹¤ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.2
            )
            
            try:
                import json
                result = json.loads(response.choices[0].message.content)
                rankings = result.get('rankings', [])
                
                # ì¬ìˆœìœ„ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                rerank_info = {}
                for rank_item in rankings:
                    hs_code = rank_item.get('hs_code', '')
                    rerank_info[hs_code] = {
                        'llm_rank': rank_item.get('rank', 999),
                        'llm_rerank_score': rank_item.get('rerank_score', 0),
                        'llm_rerank_reason': rank_item.get('reason', '')
                    }
                
                # ì›ë³¸ ë°ì´í„°ì— ì¬ìˆœìœ„ ì •ë³´ ì¶”ê°€
                reranked_candidates = candidates.copy()
                
                for idx, row in reranked_candidates.iterrows():
                    hs_code = row.get('HS_KEY') or row.get('HSë¶€í˜¸', '')
                    
                    if hs_code in rerank_info:
                        rerank_data = rerank_info[hs_code]
                        
                        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê¸°ì¡´ ì ìˆ˜ + LLM ì¬ìˆœìœ„ ì ìˆ˜)
                        current_score = row.get('ultimate_score', 0)
                        llm_rerank_score = rerank_data['llm_rerank_score'] / 10.0
                        final_score = (current_score * 0.6) + (llm_rerank_score * 0.4)
                        
                        reranked_candidates.at[idx, 'ultimate_score'] = final_score
                        reranked_candidates.at[idx, 'llm_rank'] = rerank_data['llm_rank']
                        reranked_candidates.at[idx, 'llm_rerank_score'] = rerank_data['llm_rerank_score']
                        reranked_candidates.at[idx, 'llm_rerank_reason'] = rerank_data['llm_rerank_reason']
                
                # ìµœì¢… ì ìˆ˜ë¡œ ì¬ì •ë ¬
                reranked_candidates = reranked_candidates.sort_values('ultimate_score', ascending=False).reset_index(drop=True)
                
                print(f"    âœ… LLM ì¬ìˆœìœ„ ì™„ë£Œ")
                return reranked_candidates
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLM ì¬ìˆœìœ„ íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€")
                return candidates
                
        except Exception as e:
            print(f"    âš ï¸ LLM ì¬ìˆœìœ„ ì‹¤íŒ¨: {e}, ì›ë³¸ ìˆœì„œ ìœ ì§€")
            return candidates
    
    def _format_ultimate_recommendations(self, results: pd.DataFrame, final_count: int) -> List[Dict]:
        """ ì¶”ì²œ ê²°ê³¼ í¬ë§·íŒ… (nan ë¬¸ì œ í•´ê²°)"""
        recommendations = []
        
        for idx, row in results.head(final_count * 2).iterrows():
            # ğŸ”§ nan ë¬¸ì œ í•´ê²°: ì•ˆì „í•œ HS ì½”ë“œ ì¶”ì¶œ
            hs_key = row.get('HS_KEY', '')
            hs_code = row.get('HSë¶€í˜¸', '')
            
            # nan ê°’ ì²˜ë¦¬
            if pd.isna(hs_key) or hs_key == '' or str(hs_key) == 'nan':
                hs_key = hs_code
            
            if pd.isna(hs_code) or hs_code == '' or str(hs_code) == 'nan':
                hs_code = hs_key
                
            # ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ LLM ì •ë³´ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not hs_code or str(hs_code) == 'nan':
                # LLM í›„ë³´ì—ì„œ ì›ë˜ ì½”ë“œ ì°¾ê¸°
                if hasattr(row, 'name') and 'llm_confidence' in row:
                    # ì´ê±´ LLM ìƒì„± ë°ì´í„°ì¼ ê°€ëŠ¥ì„±
                    for col in row.index:
                        if 'hs' in col.lower() and pd.notna(row[col]) and str(row[col]) != 'nan':
                            hs_code = str(row[col])
                            hs_key = hs_code
                            break
            
            # ìµœì¢… ì•ˆì „ì¥ì¹˜
            if not hs_code or str(hs_code) == 'nan':
                hs_code = f"ì•Œ ìˆ˜ ì—†ìŒ_{idx}"
                hs_key = hs_code
            
            # ì´ë¦„ ì •ë³´
            name_kr = self._extract_best_name(row, ['í•œê¸€í’ˆëª©ëª…', 'ì„¸ë²ˆ10ë‹¨ìœ„í’ˆëª…', 'í‘œì¤€í’ˆëª…'])
            name_en = self._extract_best_name(row, ['ì˜ë¬¸í’ˆëª©ëª…', 'í‘œì¤€í’ˆëª…ì˜ë¬¸'])
            
            # ì„¤ëª… ì •ë³´
            description = ''
            if 'final_combined_text' in row and pd.notna(row['final_combined_text']):
                description = str(row['final_combined_text'])
            
            # ì ìˆ˜ ì •ë³´ (nan ì²˜ë¦¬)
            ultimate_score = row.get('ultimate_score', 0)
            hybrid_score = row.get('hybrid_score', 0)
            
            # nan ê°’ë“¤ì„ 0ìœ¼ë¡œ ëŒ€ì²´
            if pd.isna(ultimate_score):
                ultimate_score = 0
            if pd.isna(hybrid_score):
                hybrid_score = 0
                
            confidence = min(max(ultimate_score, 0), 1.0)  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
            
            # ì¥/í˜¸ ì •ë³´ (nan ì²˜ë¦¬)
            chapter = str(row.get('chapter', ''))
            if chapter == 'nan' or pd.isna(row.get('chapter')):
                chapter = hs_key[:2] if len(str(hs_key)) >= 2 else ''
                
            heading = str(row.get('heading', ''))
            if heading == 'nan' or pd.isna(row.get('heading')):
                heading = hs_key[:4] if len(str(hs_key)) >= 4 else ''
            
            recommendation = {
                'hs_code': str(hs_code),
                'hs_key': str(hs_key),
                'name_kr': name_kr,
                'name_en': name_en,
                'description': description[:500] if description else '',
                'chapter': chapter,
                'heading': heading,
                'confidence': round(confidence, 4),
                'scores': {
                    'ultimate': round(ultimate_score, 4),
                    'hybrid': round(hybrid_score, 4),
                    'keyword': round(row.get('keyword_score', 0) if pd.notna(row.get('keyword_score', 0)) else 0, 4),
                    'semantic': round(row.get('semantic_score', 0) if pd.notna(row.get('semantic_score', 0)) else 0, 4)
                },
                'data_source': str(row.get('data_source', '')),
                'match_type': str(row.get('match_type', '')),
                'is_standard_match': bool(row.get('is_standard_match', False))
            }
            
            # LLM ì •ë³´ ì¶”ê°€ (nan ì²˜ë¦¬)
            llm_info = {}
            llm_confidence = row.get('llm_confidence', 0)
            if pd.notna(llm_confidence) and llm_confidence > 0:
                llm_info['llm_direct'] = {
                    'confidence': int(llm_confidence),
                    'reason': str(row.get('llm_reason', ''))
                }
            
            llm_rerank_score = row.get('llm_rerank_score', 0)
            if pd.notna(llm_rerank_score) and llm_rerank_score > 0:
                llm_info['llm_rerank'] = {
                    'score': float(llm_rerank_score),
                    'rank': int(row.get('llm_rank', 999)) if pd.notna(row.get('llm_rank')) else 999,
                    'reason': str(row.get('llm_rerank_reason', ''))
                }
            
            if llm_info:
                recommendation['llm_analysis'] = llm_info
            
            recommendations.append(recommendation)
            
            if len(recommendations) >= final_count:
                break
        
        return recommendations
    
    def print_results(self, results: Dict, query: str):
            """ì¶”ì²œ ê²°ê³¼ ì¶œë ¥ (LLM í†µí•© ì •ë³´ í¬í•¨)"""
            print(f"\n{'='*80}")
            print(f"'{query}' ê²€ìƒ‰ ê²°ê³¼")
            print(f"{'='*80}")
            
            recommendations = results.get('recommendations', [])
            
            if not recommendations:
                print("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            for i, rec in enumerate(recommendations, 1):
                print(f"\n{i}. HS ì½”ë“œ: {rec['hs_code']}")
                print(f"   í•œê¸€ëª…: {rec['name_kr']}")
                if rec['name_en']:
                    print(f"   ì˜ë¬¸ëª…: {rec['name_en']}")
                print(f"   ì‹ ë¢°ë„: {rec['confidence']:.3f}")
                print(f"   ì¥: {rec['chapter']}, í˜¸: {rec['heading']}")
                
                if rec.get('description'):
                    desc = rec['description']
                    if len(desc) > 150:
                        desc = desc[:150] + "..."
                    print(f"   ì„¤ëª…: {desc}")
                
                # LLM ë¶„ì„ ê²°ê³¼ 
                if rec.get('llm_analysis'):
                    llm = rec['llm_analysis']
                    
                    # LLM ì§ì ‘ ì œì•ˆ ì •ë³´
                    if 'llm_direct' in llm:
                        direct = llm['llm_direct']
                        print(f"   ğŸ§  LLM ë¶„ì„:")
                        print(f"     âœ¨ LLM ì§ì ‘ ì œì•ˆ (í™•ì‹ ë„: {direct['confidence']}/10)")
                        print(f"     ğŸ“ ì œì•ˆ ì´ìœ : {direct['reason']}")
                    
                    # LLM ì¬ìˆœìœ„ ì •ë³´
                    if 'llm_rerank' in llm:
                        rerank = llm['llm_rerank']
                        print(f"     ğŸ”„ ì¬ìˆœìœ„ ì ìˆ˜: {rerank['score']}/10")
                        print(f"     ğŸ“Š ì¬ìˆœìœ„ ê·¼ê±°: {rerank['reason']}")
                        print(f"     ğŸ“ LLM ìˆœìœ„: {rerank['rank']}ìœ„")
                
                # ê¸°ì¡´ LLM ë¶„ì„ (í˜¸í™˜ì„±)
                elif rec.get('llm_analysis'):
                    llm = rec['llm_analysis']
                    if llm.get('reason'):
                        print(f"   AI ë¶„ì„: {llm['reason']}")
                    if llm.get('caution'):
                        print(f"   ì£¼ì˜ì‚¬í•­: {llm['caution']}")
                
                # ë§¤ì¹­ íƒ€ì… í‘œì‹œ
                match_type = rec.get('match_type', '')
                if match_type == 'llm_search_match':
                    print(f"   ğŸ¯ ë§¤ì¹­: LLM + ê²€ìƒ‰ì—”ì§„ ì¼ì¹˜")
                elif match_type == 'llm_only_with_data':
                    print(f"   ğŸ” ë§¤ì¹­: LLM ì „ìš© (ë°ì´í„° ìˆìŒ)")
                elif match_type == 'llm_only_no_data':
                    print(f"   ğŸ†• ë§¤ì¹­: LLM ì „ìš© (ì‹ ê·œ ì œì•ˆ)")
                elif match_type == 'search_only':
                    print(f"   ğŸ” ë§¤ì¹­: ê²€ìƒ‰ì—”ì§„ ì „ìš©")
                
                # ì ìˆ˜ ì •ë³´ (ultimate ì ìˆ˜ í¬í•¨)
                scores = rec['scores']
                if 'ultimate' in scores:
                    print(f"   ì ìˆ˜: U={scores['ultimate']:.3f}, H={scores['hybrid']:.3f}, "
                        f"K={scores['keyword']:.3f}, S={scores['semantic']:.3f}")
                else:
                    print(f"   ì ìˆ˜: H={scores['hybrid']:.3f}, K={scores['keyword']:.3f}, S={scores['semantic']:.3f}")
                
                print(f"   ë°ì´í„° ì†ŒìŠ¤: {rec.get('data_source', '')}")
            
            # ê²€ìƒ‰ ì •ë³´
            search_info = results.get('search_info', {})
            print(f"\nê²€ìƒ‰ ì •ë³´:")
            print(f"  ì´ í›„ë³´: {search_info.get('total_candidates', 0)}ê°œ")
            
            method = search_info.get('method', '')
            if method == 'ultimate_llm_hybrid':
                print(f"  ğŸ§  ë°©ë²•: LLM í†µí•© ì¶”ì²œ")
                print(f"  LLM ëª¨ë¸: {search_info.get('llm_model', '')}")
            else:
                print(f"  ì˜ë¯¸ ëª¨ë¸: {search_info.get('semantic_model', '')}")
            
            if search_info.get('llm_analysis') and search_info['llm_analysis'].get('recommendation'):
                print(f"\nì „ì²´ AI ì¶”ì²œ ì˜ê²¬:")
                print(f"  {search_info['llm_analysis']['recommendation']}")
